{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook is to train and test the CNN-driven advection scheme.**\n",
    "\n",
    "This notebook perform loading the velocity field, implementing the machine-learned scheme, training the scheme, wallclock time measurement, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages ##\n",
    "\n",
    "#import Pkg\n",
    "#Pkg.add(\"Plots\")\n",
    "using Plots\n",
    "#Pkg.add(\"BenchmarkTools\")\n",
    "using BenchmarkTools\n",
    "#Pkg.add(\"Flux\")\n",
    "using Flux\n",
    "#Pkg.add(\"CSV\")\n",
    "using CSV\n",
    "#Pkg.add(\"DelimitedFiles\")\n",
    "using DelimitedFiles\n",
    "#Pkg.add(\"Statistics\")\n",
    "using Statistics\n",
    "#Pkg.add(\"BSON\")\n",
    "using BSON: @save\n",
    "using BSON: @load\n",
    "#Pkg.add(\"Random\")\n",
    "using Random\n",
    "#Pkg.add(\"CUDA\")\n",
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we trained the model with a single dataset, we used this code.\n",
    "vel1_GEOS_Array = readdlm( \"Training_dataset/Vel_GEOS_Jan_2019_NASA_GMAO_10_U_1x_1x.csv\", ',', Float32);\n",
    "scalar1_GEOS_Array = readdlm(\"Training_dataset/VL_GEOS_Jan_2019_NASA_GMAO_10_U_1x_1x.csv\", ',', Float32);\n",
    "\n",
    "xdim = size(vel1_GEOS_Array, 1);\n",
    "nstep = size(vel1_GEOS_Array, 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes, we wanted to check the maximum CFL number in the velocity field we used.\n",
    "maximum(vel1_GEOS_Array*300/27034.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For training with the multiple datasets, we used this code.\n",
    "## For 2-D demonstration, we trained the solver for (16dx, 64dt) resolution again with 7 different datasets, \n",
    "## including the orignial trainign dataset and the generalization testing sets.\n",
    "\n",
    "vel1_GEOS_Array = readdlm( \"Training_dataset/Vel_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "scalar1_GEOS_Array = readdlm(\"Training_dataset/VL_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "vel2_GEOS_Array = readdlm(\"Generalization_tests/Season_April/Vel_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "scalar2_GEOS_Array = readdlm(\"Generalization_tests/Season_April/VL_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "vel3_GEOS_Array = readdlm(\"Generalization_tests/Season_Jul/Vel_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "scalar3_GEOS_Array = readdlm(\"Generalization_tests/Season_Jul/VL_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "vel4_GEOS_Array = readdlm(\"Generalization_tests/Season_Oct/Vel_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "scalar4_GEOS_Array = readdlm(\"Generalization_tests/Season_Oct/VL_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "vel5_GEOS_Array = readdlm(\"Generalization_tests/Lat_29N_corrected/Vel_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "scalar5_GEOS_Array = readdlm(\"Generalization_tests/Lat_29N_corrected/VL_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "vel6_GEOS_Array = readdlm(\"Generalization_tests/Lat_45N_corrected/Vel_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "scalar6_GEOS_Array = readdlm(\"Generalization_tests/Lat_45N_corrected/VL_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "vel7_GEOS_Array = readdlm(\"Generalization_tests/Longitudinal/Vel_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "scalar7_GEOS_Array = readdlm(\"Generalization_tests/Longitudinal/VL_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "xdim = size(vel1_GEOS_Array, 1);\n",
    "ydim = size(vel7_GEOS_Array, 1);\n",
    "nstep = size(vel1_GEOS_Array, 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise and normalize the input scale.\n",
    "\n",
    "Random.seed!(1)\n",
    "history = (scalar1_GEOS_Array*Float32(1e7), vel1_GEOS_Array/15);\n",
    "#history = (scalar1_GEOS_Array*Float32(1e7) + 4e-5*rand(Float32, xdim, nstep), vel1_GEOS_Array/15);\n",
    "#history2 = (scalar2_GEOS_Array*Float32(1e7) + 4e-5*rand(Float32, xdim, nstep), vel2_GEOS_Array/15);\n",
    "#history3 = (scalar3_GEOS_Array*Float32(1e7) + 4e-5*rand(Float32, xdim, nstep), vel3_GEOS_Array/15);\n",
    "#history4 = (scalar4_GEOS_Array*Float32(1e7) + 4e-5*rand(Float32, xdim, nstep), vel4_GEOS_Array/15);\n",
    "#history5 = (scalar5_GEOS_Array*Float32(1e7) + 4e-5*rand(Float32, xdim, nstep), vel5_GEOS_Array/15);\n",
    "#history6 = (scalar6_GEOS_Array*Float32(1e7) + 4e-5*rand(Float32, xdim, nstep), vel6_GEOS_Array/15);\n",
    "#history7 = (scalar7_GEOS_Array*Float32(1e7) + 4e-5*rand(Float32, ydim, nstep), vel7_GEOS_Array/15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the resolutions, we could use the solver code with different kernel sizes. pgm_ml() is the code to integrate throughout the whole timesteps, while one_step_integrate() is the code for a single step integration, which is used in model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pgm_ml (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Integrating through the whole timesteps using ML based advection solver ##\n",
    "## Programming advection scheme with 3 stencil (default) ##\n",
    "\n",
    "function pgm_ml(x, u1, model, xdim_total, nstep_total, dx, dt)\n",
    "    \n",
    "    ## Initialize\n",
    "    history_2x_learned = zeros(Float32, xdim, 1, nstep) #|> gpu\n",
    "    history_2x_learned[:,:,1] = x[:,1,1,1] #|> gpu\n",
    "    s1_input = x[:,:,:,1] #|> gpu\n",
    "    s1_scale = zeros(Float32, xdim_total, 3) #|> gpu\n",
    "    \n",
    "    ## Integrate\n",
    "    for n in 1:nstep_total-1\n",
    "        # learned solver\n",
    "        coeff_estimated = reshape(model(hcat(s1_input, u1[:,:,:,n])), (xdim_total, 2, 3)) #|> gpu\n",
    "        \n",
    "        s1_scale[1,1] = s1_input[1] #|> gpu\n",
    "        s1_scale[2:xdim_total,1] = s1_input[1:xdim_total-1] #|> gpu\n",
    "        \n",
    "        s1_scale[:,2] = s1_input[1:xdim_total] #|> gpu\n",
    "        \n",
    "        s1_scale[1:xdim_total-1,3] = s1_input[2:xdim_total] #|> gpu\n",
    "        s1_scale[xdim_total,3] = s1_input[xdim_total] #|> gpu\n",
    "        \n",
    "        s2_2x = reshape(s1_input, xdim_total) + 100*dt/dx*sum(coeff_estimated[:,1,:].*s1_scale, dims=2) \n",
    "                + 10000*(dt*dt)/(dx*dx)*sum(coeff_estimated[:,2,:].*s1_scale, dims=2) #|> gpu\n",
    "        \n",
    "        history_2x_learned[:,1,n+1] = s2_2x #|> gpu\n",
    "        s1_input = reshape(s2_2x, (xdim_total,1,1)) #|> gpu\n",
    "    end\n",
    "    \n",
    "    return history_2x_learned\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Integrating a single timestep using ML based advection solver ##\n",
    "## This code is called for model training later ##\n",
    "## Integrate function with 3 stencil (default) ##\n",
    "\n",
    "function one_step_integrate(x, u1, model, xdim_total, dx, dt)\n",
    "    ## Initialize\n",
    "    s1_input = reshape(x, (xdim_total, 1, 1))\n",
    "    #s1_scale = s1_input\n",
    "    \n",
    "    # learned solver\n",
    "    coeff_estimated = reshape(model(hcat(s1_input, u1)), (xdim_total, 2, 3))\n",
    "    su = s1_input\n",
    "    s1_bc = vcat( [su[1]], [su[i] for i in 1:xdim_total], [su[xdim_total]])\n",
    "    s1_scale = reshape(hcat([s1_bc[i] for i in 1:xdim_total],\n",
    "                [s1_bc[i] for i in 2:xdim_total+1],\n",
    "                [s1_bc[i] for i in 3:xdim_total+2]), xdim_total, 3)\n",
    "    s2_2x = reshape(s1_input, xdim_total) + 100*dt/dx*sum(coeff_estimated[:,1,:].*s1_scale, dims=2) \n",
    "                + 10000*(dt*dt)/(dx*dx)*sum(coeff_estimated[:,2,:].*s1_scale, dims=2)\n",
    "    \n",
    "    s1_input = reshape(s2_2x, (xdim_total,1,1))\n",
    "    return s2_2x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pgm_ml (generic function with 1 method)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Integrating through the whole timesteps using ML based advection solver ##\n",
    "## Programming advection scheme with 5 stencil ##\n",
    "\n",
    "function pgm_ml(x, u1, model, xdim_total, nstep_total, dx, dt)\n",
    "    \n",
    "    ## Initialize\n",
    "    history_2x_learned = zeros(Float32, xdim, 1, nstep) #|> gpu\n",
    "    history_2x_learned[:,:,1] = x[:,1,1,1] #|> gpu\n",
    "    s1_input = x[:,:,:,1] #|> gpu\n",
    "    s1_scale = zeros(Float32, xdim_total, 5) #|> gpu\n",
    "    \n",
    "    ## Integrate\n",
    "    for n in 1:nstep_total-1\n",
    "        # learned solver\n",
    "        coeff_estimated = reshape(model(hcat(s1_input, u1[:,:,:,n])), (xdim_total, 2, 5)) #|> gpu\n",
    "        \n",
    "        s1_scale[1:2,1] .= s1_input[1] #|> gpu\n",
    "        s1_scale[3:xdim_total,1] = s1_input[1:xdim_total-2] #|> gpu\n",
    "        \n",
    "        s1_scale[1,2] = s1_input[1] #|> gpu\n",
    "        s1_scale[2:xdim_total,1] = s1_input[1:xdim_total-1] #|> gpu\n",
    "        \n",
    "        s1_scale[:,3] = s1_input[1:xdim_total] #|> gpu\n",
    "        \n",
    "        s1_scale[1:xdim_total-1,4] = s1_input[2:xdim_total] #|> gpu\n",
    "        s1_scale[xdim_total,4] = s1_input[xdim_total] #|> gpu\n",
    "        \n",
    "        s1_scale[1:xdim_total-2,5] = s1_input[3:xdim_total] #|> gpu\n",
    "        s1_scale[xdim_total-1:xdim_total,5] .= s1_input[xdim_total] #|> gpu\n",
    "        \n",
    "        s2_2x = reshape(s1_input, xdim_total) + 100*dt/dx*sum(coeff_estimated[:,1,:].*s1_scale, dims=2) \n",
    "                + 10000*(dt*dt)/(dx*dx)*sum(coeff_estimated[:,2,:].*s1_scale, dims=2) #|> gpu\n",
    "        \n",
    "        history_2x_learned[:,1,n+1] = s2_2x #|> gpu\n",
    "        s1_input = reshape(s2_2x, (xdim_total,1,1)) #|> gpu\n",
    "    end\n",
    "    \n",
    "    return history_2x_learned\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Integrating a single timestep using ML based advection solver ##\n",
    "## This code is called for model training later ##\n",
    "## Integrate function with 5 stencil ##\n",
    "\n",
    "function one_step_integrate(x, u1, model, xdim_total, dx, dt)\n",
    "    ## Initialize\n",
    "    s1_input = reshape(x, (xdim, 1, 1))\n",
    "    #s1_scale = s1_input\n",
    "    \n",
    "    # learned solver\n",
    "    coeff_estimated = reshape(model(hcat(s1_input, u1)), (xdim_total, 2, 5))\n",
    "    su = s1_input\n",
    "    s1_bc = vcat( [su[1]], [su[1]], [su[i] for i in 1:xdim_total], [su[xdim_total]], [su[xdim_total]])\n",
    "    s1_scale = reshape(hcat([s1_bc[i] for i in 1:xdim_total],\n",
    "                [s1_bc[i] for i in 2:xdim_total+1],\n",
    "                [s1_bc[i] for i in 3:xdim_total+2],\n",
    "                [s1_bc[i] for i in 4:xdim_total+3],\n",
    "                [s1_bc[i] for i in 5:xdim_total+4]), xdim_total, 5)\n",
    "    s2_2x = reshape(s1_input, xdim_total) + 100*dt/dx*sum(coeff_estimated[:,1,:].*s1_scale, dims=2) \n",
    "                + 10000*(dt*dt)/(dx*dx)*sum(coeff_estimated[:,2,:].*s1_scale, dims=2)\n",
    "    \n",
    "    s1_input = reshape(s2_2x, (xdim_total,1,1))\n",
    "    return s2_2x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pgm_ml (generic function with 1 method)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Integrating through the whole timesteps using ML based advection solver ##\n",
    "## Programming advection scheme with 9 stencil ##\n",
    "\n",
    "function pgm_ml(x, u1, model, xdim_total, nstep_total, dx, dt)\n",
    "    \n",
    "    ## Initialize\n",
    "    history_2x_learned = zeros(Float32, xdim, 1, nstep) #|> gpu\n",
    "    history_2x_learned[:,:,1] = x[:,1,1,1] #|> gpu\n",
    "    s1_input = x[:,:,:,1] #|> gpu\n",
    "    s1_scale = zeros(Float32, xdim_total, 9) #|> gpu\n",
    "    \n",
    "    ## Integrate\n",
    "    for n in 1:nstep_total-1\n",
    "        # learned solver\n",
    "        coeff_estimated = reshape(model(hcat(s1_input, u1[:,:,:,n])), (xdim_total, 2, 9)) #|> gpu\n",
    "        s1_scale[1:4,1] .= s1_input[1] #|> gpu\n",
    "        s1_scale[5:xdim_total,1] = s1_input[1:xdim_total-4] #|> gpu\n",
    "        \n",
    "        s1_scale[1:3,2] .= s1_input[1] #|> gpu\n",
    "        s1_scale[4:xdim_total,2] = s1_input[1:xdim_total-3] #|> gpu\n",
    "        \n",
    "        s1_scale[1:2,3] .= s1_input[1] #|> gpu\n",
    "        s1_scale[3:xdim_total,3] = s1_input[1:xdim_total-2] #|> gpu\n",
    "        \n",
    "        s1_scale[1,4] = s1_input[1] #|> gpu\n",
    "        s1_scale[2:xdim_total,4] = s1_input[1:xdim_total-1] #|> gpu\n",
    "        \n",
    "        \n",
    "        s1_scale[:,5] = s1_input[1:xdim_total] #|> gpu\n",
    "        \n",
    "        \n",
    "        s1_scale[1:xdim_total-1,6] = s1_input[2:xdim_total] #|> gpu\n",
    "        s1_scale[xdim_total,6] = s1_input[xdim_total] #|> gpu\n",
    "        \n",
    "        s1_scale[1:xdim_total-2,7] = s1_input[3:xdim_total] #|> gpu\n",
    "        s1_scale[xdim_total-1:xdim_total,7] .= s1_input[xdim_total] #|> gpu\n",
    "        \n",
    "        s1_scale[1:xdim_total-3,8] = s1_input[4:xdim_total] #|> gpu\n",
    "        s1_scale[xdim_total-2:xdim_total,8] .= s1_input[xdim_total] #|> gpu\n",
    "        \n",
    "        s1_scale[1:xdim_total-4,9] = s1_input[5:xdim_total] #|> gpu\n",
    "        s1_scale[xdim_total-3:xdim_total,9] .= s1_input[xdim_total] #|> gpu\n",
    "        \n",
    "        s2_2x = reshape(s1_input, xdim_total) + 100*dt/dx*sum(coeff_estimated[:,1,:].*s1_scale, dims=2) \n",
    "                + 10000*(dt*dt)/(dx*dx)*sum(coeff_estimated[:,2,:].*s1_scale, dims=2) #|> gpu\n",
    "        \n",
    "        history_2x_learned[:,1,n+1] = s2_2x #|> gpu\n",
    "        s1_input = reshape(s2_2x, (xdim_total,1,1)) #|> gpu\n",
    "    end\n",
    "    \n",
    "    return history_2x_learned\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Integrating a single timestep using ML based advection solver ##\n",
    "## This code is called for model training later ##\n",
    "## Integrate function with 9 stencil ##\n",
    "\n",
    "function one_step_integrate(x, u1, model, xdim_total, dx, dt)\n",
    "    ## Initialize\n",
    "    s1_input = reshape(x, (xdim, 1, 1))\n",
    "    #s1_scale = s1_input\n",
    "    \n",
    "    # learned solver\n",
    "    coeff_estimated = reshape(model(hcat(s1_input, u1)), (xdim_total, 2, 9))\n",
    "    su = s1_input\n",
    "    s1_bc = vcat( [su[1]], [su[1]], [su[1]], [su[1]], [su[i] for i in 1:xdim_total], \n",
    "            [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]])\n",
    "    s1_scale = reshape(hcat([s1_bc[i] for i in 1:xdim_total],\n",
    "                [s1_bc[i] for i in 2:xdim_total+1],\n",
    "                [s1_bc[i] for i in 3:xdim_total+2],\n",
    "                [s1_bc[i] for i in 4:xdim_total+3],\n",
    "                [s1_bc[i] for i in 5:xdim_total+4],\n",
    "                [s1_bc[i] for i in 6:xdim_total+5],\n",
    "                [s1_bc[i] for i in 7:xdim_total+6],\n",
    "                [s1_bc[i] for i in 8:xdim_total+7],\n",
    "                [s1_bc[i] for i in 9:xdim_total+8]), xdim_total, 9)\n",
    "    s2_2x = reshape(s1_input, xdim_total) + 100*dt/dx*sum(coeff_estimated[:,1,:].*s1_scale, dims=2) \n",
    "                + 10000*(dt*dt)/(dx*dx)*sum(coeff_estimated[:,2,:].*s1_scale, dims=2)\n",
    "    \n",
    "    s1_input = reshape(s2_2x, (xdim_total,1,1))\n",
    "    return s2_2x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pgm_ml (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Integrating through the whole timesteps using ML based advection solver ##\n",
    "## Programming advection scheme with 17 stencil ##\n",
    "\n",
    "function pgm_ml(x, u1, model, xdim_total, nstep_total, dx, dt)\n",
    "    \n",
    "    ## Initialize\n",
    "    history_2x_learned = zeros(Float32, xdim, 1, nstep) |> gpu\n",
    "    history_2x_learned[:,:,1] = x[:,1,1,1] |> gpu\n",
    "    s1_input = x[:,:,:,1] |> gpu\n",
    "    s1_scale = zeros(Float32, xdim_total, 17) |> gpu\n",
    "    \n",
    "    ## Integrate\n",
    "    for n in 1:nstep_total-1\n",
    "        # learned solver\n",
    "        coeff_estimated = reshape(model(hcat(s1_input, u1[:,:,:,n])), (xdim_total, 2, 17)) |> gpu\n",
    "        \n",
    "        s1_scale[1:8,1] .= s1_input[1] |> gpu\n",
    "        s1_scale[9:xdim_total,1] = s1_input[1:xdim_total-8] |> gpu\n",
    "        \n",
    "        s1_scale[1:7,2] .= s1_input[1] |> gpu\n",
    "        s1_scale[8:xdim_total,2] = s1_input[1:xdim_total-7] |> gpu\n",
    "        \n",
    "        s1_scale[1:6,3] .= s1_input[1] |> gpu\n",
    "        s1_scale[7:xdim_total,3] = s1_input[1:xdim_total-6] |> gpu\n",
    "        \n",
    "        s1_scale[1:5,4] .= s1_input[1] |> gpu\n",
    "        s1_scale[6:xdim_total,4] = s1_input[1:xdim_total-5] |> gpu\n",
    "        \n",
    "        s1_scale[1:4,5] .= s1_input[1] |> gpu\n",
    "        s1_scale[5:xdim_total,5] = s1_input[1:xdim_total-4] |> gpu\n",
    "        \n",
    "        s1_scale[1:3,6] .= s1_input[1] |> gpu\n",
    "        s1_scale[4:xdim_total,6] = s1_input[1:xdim_total-3] |> gpu\n",
    "        \n",
    "        s1_scale[1:2,7] .= s1_input[1] |> gpu\n",
    "        s1_scale[3:xdim_total,7] = s1_input[1:xdim_total-2] |> gpu\n",
    "        \n",
    "        s1_scale[1,8] = s1_input[1] |> gpu\n",
    "        s1_scale[2:xdim_total,8] = s1_input[1:xdim_total-1] |> gpu\n",
    "        \n",
    "        \n",
    "        \n",
    "        s1_scale[:,9] = s1_input[1:xdim_total] |> gpu\n",
    "        \n",
    "        \n",
    "        \n",
    "        s1_scale[1:xdim_total-1,10] = s1_input[2:xdim_total] |> gpu\n",
    "        s1_scale[xdim_total,10] = s1_input[xdim_total] |> gpu\n",
    "        \n",
    "        s1_scale[1:xdim_total-2,11] = s1_input[3:xdim_total] |> gpu\n",
    "        s1_scale[xdim_total-1:xdim_total,11] .= s1_input[xdim_total] |> gpu\n",
    "        \n",
    "        s1_scale[1:xdim_total-3,12] = s1_input[4:xdim_total] |> gpu\n",
    "        s1_scale[xdim_total-2:xdim_total,12] .= s1_input[xdim_total] |> gpu\n",
    "        \n",
    "        s1_scale[1:xdim_total-4,13] = s1_input[5:xdim_total] |> gpu\n",
    "        s1_scale[xdim_total-3:xdim_total,13] .= s1_input[xdim_total] |> gpu\n",
    "        \n",
    "        s1_scale[1:xdim_total-5,14] = s1_input[6:xdim_total] |> gpu\n",
    "        s1_scale[xdim_total-4:xdim_total,14] .= s1_input[xdim_total] |> gpu\n",
    "        \n",
    "        s1_scale[1:xdim_total-6,15] = s1_input[7:xdim_total] |> gpu\n",
    "        s1_scale[xdim_total-5:xdim_total,15] .= s1_input[xdim_total] |> gpu\n",
    "        \n",
    "        s1_scale[1:xdim_total-7,16] = s1_input[8:xdim_total] |> gpu\n",
    "        s1_scale[xdim_total-6:xdim_total,16] .= s1_input[xdim_total] |> gpu\n",
    "        \n",
    "        s1_scale[1:xdim_total-8,17] = s1_input[9:xdim_total] |> gpu\n",
    "        s1_scale[xdim_total-7:xdim_total,17] .= s1_input[xdim_total] |> gpu\n",
    "        \n",
    "        s2_2x = reshape(s1_input, xdim_total) + 100*dt/dx*sum(coeff_estimated[:,1,:].*s1_scale, dims=2) \n",
    "                + 10000*(dt*dt)/(dx*dx)*sum(coeff_estimated[:,2,:].*s1_scale, dims=2) |> gpu\n",
    "        \n",
    "        history_2x_learned[:,1,n+1] = s2_2x |> gpu\n",
    "        s1_input = reshape(s2_2x, (xdim_total,1,1)) |> gpu\n",
    "    end\n",
    "    \n",
    "    return history_2x_learned\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Integrating a single timestep using ML based advection solver ##\n",
    "## This code is called for model training later ##\n",
    "## Integrate function with 17 stencil ##\n",
    "\n",
    "function one_step_integrate(x, u1, model, xdim_total, dx, dt)\n",
    "    ## Initialize\n",
    "    s1_input = reshape(x, (xdim, 1, 1))\n",
    "    #s1_scale = s1_input\n",
    "    \n",
    "    # learned solver\n",
    "    coeff_estimated = reshape(model(hcat(s1_input, u1)), (xdim_total, 2, 17))\n",
    "    su = s1_input\n",
    "    s1_bc = vcat( [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[i] for i in 1:xdim_total], \n",
    "            [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], \n",
    "            [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]])\n",
    "        s1_scale = reshape(hcat([s1_bc[i] for i in 1:xdim_total],\n",
    "                [s1_bc[i] for i in 2:xdim_total+1],\n",
    "                [s1_bc[i] for i in 3:xdim_total+2],\n",
    "                [s1_bc[i] for i in 4:xdim_total+3],\n",
    "                [s1_bc[i] for i in 5:xdim_total+4],\n",
    "                [s1_bc[i] for i in 6:xdim_total+5],\n",
    "                [s1_bc[i] for i in 7:xdim_total+6],\n",
    "                [s1_bc[i] for i in 8:xdim_total+7],\n",
    "                [s1_bc[i] for i in 9:xdim_total+8],\n",
    "                [s1_bc[i] for i in 10:xdim_total+9],\n",
    "                [s1_bc[i] for i in 11:xdim_total+10],\n",
    "                [s1_bc[i] for i in 12:xdim_total+11],\n",
    "                [s1_bc[i] for i in 13:xdim_total+12],\n",
    "                [s1_bc[i] for i in 14:xdim_total+13],\n",
    "                [s1_bc[i] for i in 15:xdim_total+14],\n",
    "                [s1_bc[i] for i in 16:xdim_total+15],\n",
    "                [s1_bc[i] for i in 17:xdim_total+16]), xdim_total, 17)\n",
    "    s2_2x = reshape(s1_input, xdim_total) + 100*dt/dx*sum(coeff_estimated[:,1,:].*s1_scale, dims=2) \n",
    "                + 10000*(dt*dt)/(dx*dx)*sum(coeff_estimated[:,2,:].*s1_scale, dims=2)\n",
    "    \n",
    "    s1_input = reshape(s2_2x, (xdim_total,1,1))\n",
    "    return s2_2x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Integrating through the whole timesteps using ML based advection solver ##\n",
    "## Programming advection scheme with 31 stencil ##\n",
    "\n",
    "function pgm_ml(x, u1, model, xdim_total, nstep_total, dx, dt)\n",
    "    \n",
    "    ## Initialize\n",
    "    history_2x_learned = zeros(Float32, xdim, 1, nstep)\n",
    "    history_2x_learned[:,:,1] = x[:,1,1,1]\n",
    "    s1_input = x[:,:,:,1]\n",
    "    #s1_scale = s1_input\n",
    "    #s1_bc = zeros(xdim_total+4)\n",
    "    \n",
    "    ## Integrate\n",
    "    for n in 1:nstep_total-1\n",
    "        # learned solver\n",
    "        #coeff_estimated = model(hcat(s1_input, u1[:,:,:,n]))\n",
    "        coeff_estimated = reshape(model(hcat(s1_input, u1[:,:,:,n])), (xdim_total, 2, 31))\n",
    "        su = s1_input\n",
    "        s1_bc = vcat( [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], \n",
    "                [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[i] for i in 1:xdim_total], \n",
    "                [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]],\n",
    "                [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]],\n",
    "                [su[xdim_total]], [su[xdim_total]], [su[xdim_total]])\n",
    "        s1_scale = reshape(hcat([s1_bc[i] for i in 1:xdim_total],\n",
    "                [s1_bc[i] for i in 2:xdim_total+1],\n",
    "                [s1_bc[i] for i in 3:xdim_total+2],\n",
    "                [s1_bc[i] for i in 4:xdim_total+3],\n",
    "                [s1_bc[i] for i in 5:xdim_total+4],\n",
    "                [s1_bc[i] for i in 6:xdim_total+5],\n",
    "                [s1_bc[i] for i in 7:xdim_total+6],\n",
    "                [s1_bc[i] for i in 8:xdim_total+7],\n",
    "                [s1_bc[i] for i in 9:xdim_total+8],\n",
    "                [s1_bc[i] for i in 10:xdim_total+9],\n",
    "                [s1_bc[i] for i in 11:xdim_total+10],\n",
    "                [s1_bc[i] for i in 12:xdim_total+11],\n",
    "                [s1_bc[i] for i in 13:xdim_total+12],\n",
    "                [s1_bc[i] for i in 14:xdim_total+13],\n",
    "                [s1_bc[i] for i in 15:xdim_total+14],\n",
    "                [s1_bc[i] for i in 16:xdim_total+15],\n",
    "                [s1_bc[i] for i in 17:xdim_total+16],\n",
    "                [s1_bc[i] for i in 18:xdim_total+17],\n",
    "                [s1_bc[i] for i in 19:xdim_total+18],\n",
    "                [s1_bc[i] for i in 20:xdim_total+19],\n",
    "                [s1_bc[i] for i in 21:xdim_total+20],\n",
    "                [s1_bc[i] for i in 22:xdim_total+21],\n",
    "                [s1_bc[i] for i in 23:xdim_total+22],\n",
    "                [s1_bc[i] for i in 24:xdim_total+23],\n",
    "                [s1_bc[i] for i in 25:xdim_total+24],\n",
    "                [s1_bc[i] for i in 26:xdim_total+25],\n",
    "                [s1_bc[i] for i in 27:xdim_total+26],\n",
    "                [s1_bc[i] for i in 28:xdim_total+27],\n",
    "                [s1_bc[i] for i in 29:xdim_total+28],\n",
    "                [s1_bc[i] for i in 30:xdim_total+29],\n",
    "                [s1_bc[i] for i in 31:xdim_total+30]), xdim_total, 31)\n",
    "        s2_2x = reshape(s1_input, xdim_total) + 100*dt/dx*sum(coeff_estimated[:,1,:].*s1_scale, dims=2) \n",
    "                + 10000*(dt*dt)/(dx*dx)*sum(coeff_estimated[:,2,:].*s1_scale, dims=2)\n",
    "        \n",
    "        history_2x_learned[:,1,n+1] = s2_2x\n",
    "        s1_input = reshape(s2_2x, (xdim_total,1,1))\n",
    "    end\n",
    "    \n",
    "    return history_2x_learned\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Integrating a single timestep using ML based advection solver ##\n",
    "## This code is called for model training later ##\n",
    "## Integrate function with 31 stencil ##\n",
    "\n",
    "function one_step_integrate(x, u1, model, xdim_total, dx, dt)\n",
    "    ## Initialize\n",
    "    s1_input = reshape(x, (xdim, 1, 1))\n",
    "    #s1_scale = s1_input\n",
    "    \n",
    "    # learned solver\n",
    "    coeff_estimated = reshape(model(hcat(s1_input, u1)), (xdim_total, 2, 31))\n",
    "    su = s1_input\n",
    "    s1_bc = vcat( [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]],\n",
    "                [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]],\n",
    "                [su[1]], [su[1]], [su[1]], [su[i] for i in 1:xdim_total], \n",
    "                [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]],\n",
    "                [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]],\n",
    "                [su[xdim_total]], [su[xdim_total]], [su[xdim_total]])\n",
    "    s1_scale = reshape(hcat([s1_bc[i] for i in 1:xdim_total],\n",
    "                [s1_bc[i] for i in 2:xdim_total+1],\n",
    "                [s1_bc[i] for i in 3:xdim_total+2],\n",
    "                [s1_bc[i] for i in 4:xdim_total+3],\n",
    "                [s1_bc[i] for i in 5:xdim_total+4],\n",
    "                [s1_bc[i] for i in 6:xdim_total+5],\n",
    "                [s1_bc[i] for i in 7:xdim_total+6],\n",
    "                [s1_bc[i] for i in 8:xdim_total+7],\n",
    "                [s1_bc[i] for i in 9:xdim_total+8],\n",
    "                [s1_bc[i] for i in 10:xdim_total+9],\n",
    "                [s1_bc[i] for i in 11:xdim_total+10],\n",
    "                [s1_bc[i] for i in 12:xdim_total+11],\n",
    "                [s1_bc[i] for i in 13:xdim_total+12],\n",
    "                [s1_bc[i] for i in 14:xdim_total+13],\n",
    "                [s1_bc[i] for i in 15:xdim_total+14],\n",
    "                [s1_bc[i] for i in 16:xdim_total+15],\n",
    "                [s1_bc[i] for i in 17:xdim_total+16],\n",
    "                [s1_bc[i] for i in 18:xdim_total+17],\n",
    "                [s1_bc[i] for i in 19:xdim_total+18],\n",
    "                [s1_bc[i] for i in 20:xdim_total+19],\n",
    "                [s1_bc[i] for i in 21:xdim_total+20],\n",
    "                [s1_bc[i] for i in 22:xdim_total+21],\n",
    "                [s1_bc[i] for i in 23:xdim_total+22],\n",
    "                [s1_bc[i] for i in 24:xdim_total+23],\n",
    "                [s1_bc[i] for i in 25:xdim_total+24],\n",
    "                [s1_bc[i] for i in 26:xdim_total+25],\n",
    "                [s1_bc[i] for i in 27:xdim_total+26],\n",
    "                [s1_bc[i] for i in 28:xdim_total+27],\n",
    "                [s1_bc[i] for i in 29:xdim_total+28],\n",
    "                [s1_bc[i] for i in 30:xdim_total+29],\n",
    "                [s1_bc[i] for i in 31:xdim_total+30]), xdim_total, 31)\n",
    "    s2_2x = reshape(s1_input, xdim_total) + 100*dt/dx*sum(coeff_estimated[:,1,:].*s1_scale, dims=2) \n",
    "                + 10000*(dt*dt)/(dx*dx)*sum(coeff_estimated[:,2,:].*s1_scale, dims=2)\n",
    "    \n",
    "    s1_input = reshape(s2_2x, (xdim_total,1,1))\n",
    "    return s2_2x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate arrays for storing training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good\n"
     ]
    }
   ],
   "source": [
    "## Generate arrays to store the input (scalar and velocity) and target (next time scalar) data ##\n",
    "input_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1) #conentration\n",
    "u_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1) #velocity\n",
    "target_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1) #next time concentration\n",
    "\n",
    "## Below are used when we are feeding multiple training datasets ##\n",
    "\n",
    "#input2_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "#u2_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "#target2_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "\n",
    "#input3_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "#u3_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "#target3_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "\n",
    "#input4_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "#u4_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "#target4_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "\n",
    "#input5_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "#u5_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "#target5_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "\n",
    "#input6_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "#u6_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "#target6_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "\n",
    "#input7_NN_integrate = zeros(Float32, ydim, 1, 1, nstep-1)\n",
    "#u7_NN_integrate = zeros(Float32, ydim, 1, 1, nstep-1)\n",
    "#target7_NN_integrate = zeros(Float32, ydim, 1, 1, nstep-1)\n",
    "\n",
    "## When we use GPU to train ##\n",
    "\n",
    "#input_NN_integrate |> gpu\n",
    "#u_NN_integrate |> gpu\n",
    "#target_NN_integrate |> gpu\n",
    "\n",
    "## I just wanted to see if it is good ##\n",
    "println(\"Good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224×2879 Matrix{Float32}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.00182714  0.00183716  0.00184696\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.00533854  0.0053668   0.00539445\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.00862063  0.00866309  0.00870459\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0118458   0.0118979   0.0119486\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0150919   0.0151482   0.0152029\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0180649   0.0181185   0.0181702\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0204709   0.0205141   0.0205555\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.022319    0.0223457   0.0223708\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.023829    0.0238354   0.0238406\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0250864   0.025071    0.0250547\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0258404   0.025803    0.0257652\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0260894   0.0260333   0.0259774\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0260722   0.0260019   0.0259325\n",
       " ⋮                        ⋮         ⋱                          \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.600619    0.596108    0.591644\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.63833     0.633751    0.629213\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.684285    0.679757    0.67526\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.745188    0.740861    0.736552\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.82261     0.818727    0.814844\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.914835    0.911769    0.908678\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.01471     1.01295     1.01113\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.1001      1.10018     1.10015\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  1.17232     1.17465     1.17683\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.24254     1.2473      1.25186\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.29808     1.30499     1.31165\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.31154     1.31967     1.32753"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now we store the scalar, velocity at the generated array ##\n",
    "\n",
    "input_NN_integrate[:,1,1,:] = history[1][:,1:nstep-1]\n",
    "u_NN_integrate[:,1,1,:] = history[2][:,1:nstep-1]\n",
    "target_NN_integrate[:,1,1,:] = history[1][:,2:nstep]\n",
    "\n",
    "#input2_NN_integrate[:,1,1,:] = history2[1][:,1:nstep-1]\n",
    "#u2_NN_integrate[:,1,1,:] = history2[2][:,1:nstep-1]\n",
    "#target2_NN_integrate[:,1,1,:] = history2[1][:,2:nstep]\n",
    "\n",
    "#input3_NN_integrate[:,1,1,:] = history3[1][:,1:nstep-1]\n",
    "#u3_NN_integrate[:,1,1,:] = history3[2][:,1:nstep-1]\n",
    "#target3_NN_integrate[:,1,1,:] = history3[1][:,2:nstep]\n",
    "\n",
    "#input4_NN_integrate[:,1,1,:] = history4[1][:,1:nstep-1]\n",
    "#u4_NN_integrate[:,1,1,:] = history4[2][:,1:nstep-1]\n",
    "#target4_NN_integrate[:,1,1,:] = history4[1][:,2:nstep]\n",
    "\n",
    "#input5_NN_integrate[:,1,1,:] = history5[1][:,1:nstep-1]\n",
    "#u5_NN_integrate[:,1,1,:] = history5[2][:,1:nstep-1]\n",
    "#target5_NN_integrate[:,1,1,:] = history5[1][:,2:nstep]\n",
    "\n",
    "#input6_NN_integrate[:,1,1,:] = history6[1][:,1:nstep-1]\n",
    "#u6_NN_integrate[:,1,1,:] = history6[2][:,1:nstep-1]\n",
    "#target6_NN_integrate[:,1,1,:] = history6[1][:,2:nstep]\n",
    "\n",
    "#input7_NN_integrate[:,1,1,:] = history7[1][:,1:nstep-1]\n",
    "#u7_NN_integrate[:,1,1,:] = history7[2][:,1:nstep-1]\n",
    "#target7_NN_integrate[:,1,1,:] = history7[1][:,2:nstep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the convolutional neural networks. Depending on the problem complexity we could use different size of network. We could change the kernel size at the begining, or the number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([[-0.23724414 -0.1847739; -0.2186838 -0.13725726; -0.18596105 -0.051771183;;; 0.107935704 0.09306445; -0.23084818 -0.05097895; -0.02000927 0.12976532;;; 0.027414659 0.17254841; -0.20021169 0.16721494; 0.17437194 0.16511054;;; … ;;; 0.05513018 -0.035334747; 0.15314297 -0.061656483; 0.13123395 0.13320887;;; 0.16111721 -0.03930819; 0.03960307 -0.044986144; -0.21798879 0.1565445;;; 0.063831136 0.11841493; -0.16270801 0.17739886; -0.062633 -0.096048824], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07363952 -0.047109388 … -0.12786153 0.003514183; -0.17596187 -0.08074215 … 0.064750224 -0.13086778; -0.09424503 0.15709022 … -0.15967849 0.1111644;;; -0.17100489 0.16413192 … 0.12996615 -0.081389844; -0.033714738 -0.032853067 … -0.14265299 -0.055032805; 0.02673326 -0.015684191 … -0.12769084 0.011887202;;; -0.03100634 -0.12751737 … 0.16694982 0.09528412; 0.14384148 -0.15594721 … -0.17209105 0.16045; 0.010559387 -0.027374335 … -0.12399238 0.045416664;;; … ;;; 0.1283581 -0.14920998 … -0.0926711 -0.11721641; 0.00027932823 -0.073874384 … 0.14511956 -0.08530611; -0.14782575 -0.14287381 … 0.13870715 0.012605427;;; -0.11735579 -0.1765631 … 0.09848797 0.009204345; -0.08498684 0.17157038 … 0.015785828 0.14241292; -0.0044220896 0.15291809 … -0.012105586 0.15746945;;; 0.113224454 -0.00064788136 … -0.03921644 -0.09644602; -0.16662727 0.10528382 … 0.16773668 0.13854404; 0.103757724 -0.0544661 … -0.088971786 0.00031597493], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [-0.040679753 0.060144078 … -0.08698044 0.03747257; 0.06255646 -0.025641257 … 0.019072425 0.07752642; -0.08147927 0.18824448 … -0.19724697 -0.1900684;;; 0.02047882 -0.013012054 … 0.15016791 -0.0952495; -0.17377803 0.13815413 … 0.19524921 0.044143576; -0.17008097 0.089598015 … -0.14150126 -0.22746786;;; -0.1520864 0.09855306 … -0.0717647 0.15120678; -0.042682536 0.012270993 … -0.174672 -0.08879561; 0.09237033 -0.1656183 … 0.10022206 -0.0038136917;;; 0.17941701 -0.09827235 … 0.13365725 0.07337079; -0.17625019 -0.13227019 … -0.228546 -0.014580353; 0.1851027 -0.04876971 … 0.15791248 -0.037369873;;; 0.13556713 0.1282958 … 0.11703287 0.08840579; 0.22086895 -0.14683837 … -0.12513532 0.18617323; -0.19544688 0.086719945 … 0.16624855 -0.07290302;;; 0.07018535 -0.13869245 … -0.05463076 0.11432589; -0.054591436 -0.15231818 … -0.10465303 -0.20729236; -0.07527365 0.021156488 … 0.16446634 -0.12686181], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Three layer model\n",
    "model = Chain( Conv((3,), 2 => 32, pad = SamePad(), gelu),\n",
    "    Conv((3,), 32 => 32, pad = SamePad(), gelu),\n",
    "    Conv((3,), 32 => 6, pad = SamePad(), tanh))\n",
    "\n",
    "loss(x, y) = Flux.Losses.mae(model(x), y)\n",
    "\n",
    "ps = Flux.params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([[-0.009612143 -0.009659199; 0.044706717 0.089672185; … ; 0.056957096 -0.05944776; -0.012639276 -0.015488572;;; 0.06191108 0.11270316; -0.07864362 -0.09245483; … ; 0.10297047 -0.03236092; -0.026258748 -0.048672862;;; -0.050101694 -0.05139888; -0.08993016 0.016895887; … ; 0.050840553 0.11115066; 0.093233466 0.0017739873;;; … ;;; -0.09429093 -0.036797322; -0.032266762 0.080782376; … ; 0.09572192 0.062228806; 0.100657836 -0.11418092;;; 0.02613919 -0.0856348; -0.04975573 0.004815787; … ; -0.093202025 -0.074404754; -0.0044896705 0.035685305;;; -0.08045359 -0.009624699; 0.105840065 -0.008336247; … ; 0.055261154 -0.09312024; 0.033189096 -0.008104035], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.044070456 -0.008149819 … 0.07529017 0.037085187; 0.056104492 0.030698419 … -0.07855728 -0.049991682; … ; 0.06667938 -0.057261925 … 0.04981746 0.066125266; 0.047441065 -0.06792799 … 0.07857337 0.0020298928;;; 0.079462625 0.067108445 … 0.023142008 -0.03793023; -0.06993242 -0.052318677 … 0.0151252 -0.03236628; … ; -0.042573195 0.073036015 … -0.07587328 -0.037602976; 0.07056333 0.07641899 … -0.03200844 0.0048272363;;; 0.03817298 0.028725915 … 0.024890574 0.049634542; 0.060560748 0.03993987 … -0.077023216 0.038841143; … ; -0.005049606 0.017041175 … -0.07786582 0.02917204; -0.0066550267 0.0408536 … -0.010359533 -0.03426286;;; … ;;; 0.038002383 0.06865268 … 0.011668968 -0.07561047; -0.048063986 0.029019766 … -0.08452528 0.028672747; … ; -0.048473503 0.014619528 … 0.030802658 0.084054604; -0.03826562 -0.0824784 … -0.051456634 0.0433261;;; 0.04135987 -0.029952085 … 0.07942487 -0.003881483; -0.05043884 -0.020048952 … 0.01181849 0.07752223; … ; -0.07182024 -0.0135339815 … -0.08165476 0.038612373; 0.0014057785 0.0073188045 … 0.07665295 0.082890786;;; -0.02948964 0.03254515 … 0.05649557 0.012540183; -0.056241967 0.008925065 … -0.036299292 0.052404765; … ; -0.0017823871 0.07876548 … 0.006502539 0.046963163; 0.038486198 0.06978018 … -0.061388314 0.037923988], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0026947935 -0.060880583 … 0.06872894 0.06555398; -0.012212659 -0.0038379363 … 0.0791495 -0.037243057; … ; 0.07678309 -0.06543455 … 0.04746454 0.003685167; 0.026463427 -0.06606187 … -0.029713755 -0.04861805;;; 0.05540654 0.005754347 … 0.047829404 -0.029535282; -0.015383009 -0.057032872 … -0.07816187 -0.040292274; … ; -0.012620242 0.02176482 … 0.024359578 -0.0675898; 0.0178495 0.0762771 … 0.0050403 -0.03668748;;; 0.042080395 0.061883736 … 0.004307516 -0.07450575; 0.029509697 -0.087940514 … -0.062921666 -0.05328673; … ; -0.057046507 -0.065747134 … 0.03734206 0.022700131; -0.08655313 -0.029087864 … -0.013353462 -0.080165684;;; … ;;; -0.02174621 -0.029308723 … -0.067868896 0.05169332; 0.037991293 -0.00049948285 … -0.029023804 -0.08874796; … ; 0.06310565 -0.07252207 … 0.07750751 -0.019209525; -0.06990405 0.032851182 … 0.017112834 0.02220942;;; -0.088430345 -0.000488349 … -0.06677012 0.07626925; -0.0012498131 -0.045671836 … -0.026717484 0.0195271; … ; -0.06294026 0.07796324 … 0.073121645 0.08884587; 0.08642463 0.081659265 … -0.026464436 0.043258525;;; 0.039113775 0.070645936 … -0.0026082748 0.061921597; -0.03665391 -0.06200939 … 0.034605242 -0.038864467; … ; 0.08481456 0.043954153 … 0.02095395 -0.030555964; -0.007099484 -0.018679459 … -0.016684026 0.01125153], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Three layer model - 13-stencil\n",
    "model = Chain( Conv((13,), 2 => 32, pad = SamePad(), gelu),\n",
    "    Conv((13,), 32 => 32, pad = SamePad(), gelu),\n",
    "    Conv((13,), 32 => 26, pad = SamePad(), tanh))\n",
    "\n",
    "loss(x, y) = Flux.Losses.mae(model(x), y)\n",
    "\n",
    "ps = Flux.params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In case we need to use the larger size model - 4-layer here\n",
    "\n",
    "model = Chain( Conv((31,), 2 => 64, pad = SamePad(), gelu),\n",
    "    Conv((3,), 64 => 64, pad = SamePad(), gelu),\n",
    "    Conv((3,), 64 => 64, pad = SamePad(), gelu),\n",
    "    Conv((3,), 64 => 62, pad = SamePad(), tanh))\n",
    "\n",
    "loss(x, y) = Flux.Losses.mae(model(x), y)\n",
    "\n",
    "ps = Flux.params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In case we need to use the larger size model - 4-layer with larger filters here\n",
    "\n",
    "model = Chain( Conv((31,), 2 => 64, pad = SamePad(), gelu),\n",
    "    Conv((3,), 64 => 128, pad = SamePad(), gelu),\n",
    "    Conv((3,), 128 => 128, pad = SamePad(), gelu),\n",
    "    Conv((3,), 128 => 62, pad = SamePad(), tanh))\n",
    "\n",
    "loss(x, y) = Flux.Losses.mae(model(x), y)\n",
    "\n",
    "ps = Flux.params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In case we need to use the larger size model -5-layer here\n",
    "\n",
    "model = Chain( Conv((31,), 2 => 64, pad = SamePad(), gelu),\n",
    "    Conv((5,), 64 => 128, pad = SamePad(), gelu),\n",
    "    Conv((5,), 128 => 128, pad = SamePad(), gelu),\n",
    "    Conv((5,), 128 => 128, pad = SamePad(), gelu),\n",
    "    Conv((5,), 128 => 62, pad = SamePad(), tanh))\n",
    "\n",
    "loss(x, y) = Flux.Losses.mae(model(x), y)\n",
    "\n",
    "ps = Flux.params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We could also load the trained model parameters.\n",
    "#@load \"spacing_phys_norm_outputs/CNN_DLR_60EPOCHS_16X1X_VL_GEOS_JAN_2019_NASA_GMAO.bson_MODEL\" model\n",
    "@load \"Train_outputs/01x01t/CNN_DLR_95EPOCHS_1X1X_VL_GEOS_JAN_2019_NASA_GMAO.bson_MODEL\" model\n",
    "#model |> gpu\n",
    "#ps = Flux.params(model) |> gpu;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Push the data in the format neural net could understand. \n",
    "Data = []\n",
    "#Data2 = []\n",
    "#Data3 = []\n",
    "#Data4 = []\n",
    "#Data5 = []\n",
    "#Data6 = []\n",
    "#Data7 = []\n",
    "\n",
    "for i in 1:nstep-1\n",
    "    push!(Data, (hcat(input_NN_integrate[:,:,:,i], u_NN_integrate[:,:,:,i]), target_NN_integrate[:,:,:,i]))\n",
    "    #push!(Data2, (hcat(input2_NN_integrate[:,:,:,i], u2_NN_integrate[:,:,:,i]), target2_NN_integrate[:,:,:,i]))\n",
    "    #push!(Data3, (hcat(input3_NN_integrate[:,:,:,i], u3_NN_integrate[:,:,:,i]), target3_NN_integrate[:,:,:,i]))\n",
    "    #push!(Data4, (hcat(input4_NN_integrate[:,:,:,i], u4_NN_integrate[:,:,:,i]), target4_NN_integrate[:,:,:,i]))\n",
    "    #push!(Data5, (hcat(input5_NN_integrate[:,:,:,i], u5_NN_integrate[:,:,:,i]), target5_NN_integrate[:,:,:,i]))\n",
    "    #push!(Data6, (hcat(input6_NN_integrate[:,:,:,i], u6_NN_integrate[:,:,:,i]), target6_NN_integrate[:,:,:,i]))\n",
    "    #push!(Data7, (hcat(input7_NN_integrate[:,:,:,i], u7_NN_integrate[:,:,:,i]), target7_NN_integrate[:,:,:,i]))\n",
    "end\n",
    "\n",
    "#Data |> gpu;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Training function for 10 time steps training\n",
    "\n",
    "function my_custom_train!(ps, data, opt, dx, dt)\n",
    "    \n",
    "    local len = length(data)\n",
    "    \n",
    "    xdim = size(data[1][1],1);\n",
    "    \n",
    "    for i in 1:len\n",
    "        gs = gradient(ps) do\n",
    "            if  0 < i < len-8\n",
    "                A = hcat([one_step_integrate(data[i][1][:,1,:], data[i][1][:,2,:], model, xdim, dx, dt)])\n",
    "                for j in 1:9\n",
    "                    x = A[j]\n",
    "                    y = data[i+j][1][:,2]\n",
    "                    A = hcat(A, [one_step_integrate(hcat(x, y)[:,1,:], hcat(x, y)[:,2,:], model, xdim, dx, dt)])\n",
    "                end\n",
    "                B = hcat([data[i+k-1][2] for k in 1:10])\n",
    "                training_loss = mean(Flux.Losses.mae.(A, B))\n",
    "            elseif len-8 ≤ i < len\n",
    "                n = len - i\n",
    "                A = hcat([one_step_integrate(data[i][1][:,1,:], data[i][1][:,2,:], model, xdim, dx, dt)])\n",
    "                for j in 1:n\n",
    "                    x = A[j]\n",
    "                    y = data[i+j][1][:,2]\n",
    "                    A = hcat(A, [one_step_integrate(hcat(x, y)[:,1,:], hcat(x, y)[:,2,:], model, xdim, dx, dt)])\n",
    "                end\n",
    "                B = hcat([data[i+k][2] for k in 0:n])\n",
    "                training_loss = mean(Flux.Losses.mae.(A, B))\n",
    "            elseif i == len\n",
    "                A = hcat([one_step_integrate(data[i][1][:,1,:], data[i][1][:,2,:], model, xdim, dx, dt)])\n",
    "                B = data[i][2]\n",
    "                training_loss = mean(Flux.Losses.mae.(A, B))\n",
    "            end\n",
    "            return training_loss\n",
    "        end\n",
    "        Flux.update!(opt, ps, gs)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Training loop. To see the progress, it printed out the number of epoch every time it's done.\n",
    "answer = scalar1_GEOS_Array\n",
    "model |> gpu\n",
    "@time CUDA.@sync for i in 1:100\n",
    "#@time for i in 1:100\n",
    "    α = (1/(1+1*i))*0.01\n",
    "    #α = 1e-3\n",
    "    dx = Float32(27034.3*1)\n",
    "    dt = 300*4\n",
    "    my_custom_train!(ps, Data, ADAM(α, (0.9, 0.999)), dx, dt)\n",
    "    #if i % 1 == 0\n",
    "    #model |> cpu\n",
    "    trial = 1e-7*pgm_ml(input_NN_integrate, u_NN_integrate, model, xdim, nstep, dx, dt);\n",
    "    dim = size(trial, 1);\n",
    "    #writedlm( \"Train_outputs/01x04t_13stencil/CNN_DLR_\"*string(i)*\"_01x04x_VL_GEOS_JAN_NASA_GMAO.csv\", A, ',')\n",
    "    #@save \"Train_outputs/01x04t_13stencil/CNN_DLR_\"*string(i)*\"EPOCHS_01X04X_VL_GEOS_JAN_2019_NASA_GMAO.bson_MODEL\" model    \n",
    "    mae = StatsBase.L1dist(trial, reshape(answer,dim))/(dim)\n",
    "    rmse = StatsBase.L2dist(trial, reshape(answer,dim))/sqrt(dim)\n",
    "    r2 = Statistics.cor(trial, reshape(answer, dim))^2\n",
    "    \n",
    "    println(i, \"   \", mae, \"   \", rmse, \"   \", r2)\n",
    "    #model |> gpu\n",
    "    #end\n",
    "end\n",
    "model |> cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for mulitple datasets\n",
    "#@time CUDA.@sync for i in 1:200\n",
    "@time for i in 1:500\n",
    "    α = (1/(1+1*i))*0.01\n",
    "    #α = 1e-3\n",
    "    dx = Float32(27034.3*16)\n",
    "    dx_29 = Float32(30276.7*16)\n",
    "    dx_45 = Float32(24597.8*16)\n",
    "    dy = Float32(27829.3*16)\n",
    "    dt = 300*64\n",
    "    my_custom_train!(ps, Data, ADAM(α, (0.9, 0.999)), dx, dt)\n",
    "    my_custom_train!(ps, Data2, ADAM(α, (0.9, 0.999)), dx, dt)\n",
    "    my_custom_train!(ps, Data3, ADAM(α, (0.9, 0.999)), dx, dt)\n",
    "    my_custom_train!(ps, Data4, ADAM(α, (0.9, 0.999)), dx, dt)\n",
    "    my_custom_train!(ps, Data5, ADAM(α, (0.9, 0.999)), dx_29, dt)\n",
    "    my_custom_train!(ps, Data6, ADAM(α, (0.9, 0.999)), dx_45, dt)\n",
    "    my_custom_train!(ps, Data7, ADAM(α, (0.9, 0.999)), dy, dt)\n",
    "    #model |> cpu\n",
    "    A1 = 1e-7*pgm_ml(input_NN_integrate, u_NN_integrate, model, xdim, nstep, dx, dt);\n",
    "    A2 = 1e-7*pgm_ml(input2_NN_integrate, u2_NN_integrate, model, xdim, nstep, dx, dt);\n",
    "    A3 = 1e-7*pgm_ml(input3_NN_integrate, u3_NN_integrate, model, xdim, nstep, dx, dt);\n",
    "    A4 = 1e-7*pgm_ml(input4_NN_integrate, u4_NN_integrate, model, xdim, nstep, dx, dt);\n",
    "    A5 = 1e-7*pgm_ml(input5_NN_integrate, u5_NN_integrate, model, xdim, nstep, dx_29, dt);\n",
    "    A6 = 1e-7*pgm_ml(input6_NN_integrate, u6_NN_integrate, model, xdim, nstep, dx_45, dt);\n",
    "    A7 = 1e-7*pgm_ml(input7_NN_integrate, u7_NN_integrate, model, ydim, nstep, dy, dt);\n",
    "    writedlm( \"Train_outputs/16x64t_for2D_7data/CNN_DLR_\"*string(i)*\"_16x64x_VL_GEOS_JAN_NASA_GMAO_U.csv\", A1, ',')\n",
    "    writedlm( \"Train_outputs/16x64t_for2D_7data/CNN_DLR_\"*string(i)*\"_16x64x_VL_GEOS_JAN_NASA_GMAO_U2.csv\", A2, ',')\n",
    "    writedlm( \"Train_outputs/16x64t_for2D_7data/CNN_DLR_\"*string(i)*\"_16x64x_VL_GEOS_JAN_NASA_GMAO_U3.csv\", A3, ',')\n",
    "    writedlm( \"Train_outputs/16x64t_for2D_7data/CNN_DLR_\"*string(i)*\"_16x64x_VL_GEOS_JAN_NASA_GMAO_U4.csv\", A4, ',')\n",
    "    writedlm( \"Train_outputs/16x64t_for2D_7data/CNN_DLR_\"*string(i)*\"_16x64x_VL_GEOS_JAN_NASA_GMAO_U5.csv\", A5, ',')\n",
    "    writedlm( \"Train_outputs/16x64t_for2D_7data/CNN_DLR_\"*string(i)*\"_16x64x_VL_GEOS_JAN_NASA_GMAO_U6.csv\", A6, ',')\n",
    "    writedlm( \"Train_outputs/16x64t_for2D_7data/CNN_DLR_\"*string(i)*\"_16x64x_VL_GEOS_JAN_NASA_GMAO_V.csv\", A7, ',')\n",
    "    @save \"Train_outputs/16x64t_for2D_7data/CNN_DLR_\"*string(i)*\"EPOCHS_16X64X_VL_GEOS_JAN_2019_NASA_GMAO.bson_MODEL\" model\n",
    "    println(i)\n",
    "    #model |> gpu\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## The way we are using the trained model to return and save the output\n",
    "\n",
    "@time A_8x16x = 1e-7*pgm_ml(input_NN_integrate, u_NN_integrate, model, xdim, nstep, Float32(27034.3*8), 300*16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The way we are using the trained model to return and save the output\n",
    "\n",
    "@time A = 1e-7*pgm_ml(input7_NN_integrate, u7_NN_integrate, model, ydim, nstep, Float32(27829.3*16), 300*64);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Conv((17,), 2 => 32, gelu, pad=8),    \u001b[90m# 1_120 parameters\u001b[39m\n",
       "  Conv((3,), 32 => 32, gelu, pad=1),    \u001b[90m# 3_104 parameters\u001b[39m\n",
       "  Conv((3,), 32 => 34, tanh, pad=1),    \u001b[90m# 3_298 parameters\u001b[39m\n",
       ") \u001b[90m                  # Total: 6 arrays, \u001b[39m7_522 parameters, 1.211 KiB."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This cell is used to test the trained model again.\n",
    "vel_GEOS_Array = readdlm( \"Training_dataset/Vel_GEOS_Jan_2019_NASA_GMAO_10_U_2x_64x.csv\", ',', Float32) |> gpu;\n",
    "scalar_GEOS_Array = readdlm(\"Training_dataset/VL_GEOS_Jan_2019_NASA_GMAO_10_U_2x_64x.csv\", ',', Float32) |> gpu;\n",
    "xdim = size(vel_GEOS_Array, 1) |> gpu;\n",
    "nstep = size(vel_GEOS_Array, 2) |> gpu;\n",
    "\n",
    "Random.seed!(1)\n",
    "history = (scalar_GEOS_Array*Float32(1e7), vel_GEOS_Array/15) |> gpu;\n",
    "\n",
    "input_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1) |> gpu\n",
    "u_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1) |> gpu\n",
    "target_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1) |> gpu\n",
    "\n",
    "input_NN_integrate[:,1,1,:] = history[1][:,1:nstep-1] |> gpu\n",
    "u_NN_integrate[:,1,1,:] = history[2][:,1:nstep-1] |> gpu\n",
    "target_NN_integrate[:,1,1,:] = history[1][:,2:nstep] |> gpu\n",
    "\n",
    "@load \"Train_outputs/02x64t/CNN_DLR_221EPOCHS_02X64X_VL_GEOS_JAN_2019_NASA_GMAO.bson_MODEL\" model\n",
    "model = gpu(model)\n",
    "#ps = Flux.params(model) |> gpu;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.709195 seconds (512.49 k allocations: 935.674 MiB, 7.07% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time A = 1e-7*pgm_ml(input_NN_integrate, u_NN_integrate, model, xdim, nstep, Float32(27034.3*1), 300*1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 8 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m656.536 ms\u001b[22m\u001b[39m … \u001b[35m686.288 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m3.66% … 4.19%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m662.812 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m3.65%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m666.384 ms\u001b[22m\u001b[39m ± \u001b[32m 10.335 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m3.71% ± 0.20%\n",
       "\n",
       "  \u001b[39m█\u001b[39m \u001b[39m \u001b[39m█\u001b[39m█\u001b[39m \u001b[39m \u001b[39m \u001b[34m█\u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m█\u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m█\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m█\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m█\u001b[39m \u001b[39m \n",
       "  \u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[34m█\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[32m▁\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m \u001b[39m▁\n",
       "  657 ms\u001b[90m           Histogram: frequency by time\u001b[39m          686 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m935.67 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m512482\u001b[39m."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluating the computational time\n",
    "\n",
    "@benchmark A = 1e-7*pgm_ml(input_NN_integrate, u_NN_integrate, model, xdim, nstep, Float32(27034.3*1), 300*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Plot the r2 throughout the timesteps\n",
    "\n",
    "begin\n",
    "    r2 = zeros(Float32, nstep)\n",
    "    for i in 1:nstep\n",
    "        r2[i] = Statistics.cor(A[:,1,i], scalar1_GEOS_Array[:,i])^2\n",
    "    end\n",
    "    plot(Int(2880/nstep)*10/2880:Int(2880/nstep)*10/2880:10, r2, width=2, label=false, xlabel=\"time (days)\", ylabel=\"r²\", xlabelfontsize=14, ylabelfontsize=14,\n",
    "        xtickfontsize=12, ytickfontsize=12, xlims=(0,10), ylims=(0.0,1.0))\n",
    "end\n",
    "savefig(\"r2_8x16t.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating correlations in different time steps\n",
    "\n",
    "Statistics.cor(A[:,1,1], 1e-7*input_NN_integrate[:,1,1,1])^2, Statistics.cor(A[:,1,960], 1e-7*input_NN_integrate[:,1,1,960])^2, Statistics.cor(A[:,1,1920], 1e-7*input_NN_integrate[:,1,1,1920])^2, Statistics.cor(A[:,1,2879], 1e-7*input_NN_integrate[:,1,1,2879])^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example of 1-D visualization with 8dx, 16dt case (t = 0 day). \n",
    "## You need to change the dimension if the resolution changes.\n",
    "\n",
    "begin\n",
    "    plot(-130.0+0.3125*4:0.3125*8:-60-0.3125*4, A[:,1,1]*1e9, label = \"Learned\", title = string(\"time: \", string(0), \" days\"), \n",
    "        xlabel=\"Longitude (°)\", ylabel=\"Mixing ratio (ppb)\", xlabelfontsize=18, ylabelfontsize=18, \n",
    "        xtickfontsize=14, ytickfontsize=14, titlefontsize=20, legendfontsize=14, width=4, ylims=(0.0, 1.2e2), legend = true,\n",
    "         margin=2.5Plots.mm)\n",
    "    plot!(-130.0+0.3125*4:0.3125*8:-60-0.3125*4, 1e2*input_NN_integrate[:,1,1,1], label = \"Reference\", width=4)\n",
    "end\n",
    "\n",
    "#savefig(\"8x16t_0day.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example of 1-D visualization with 8dx, 16dt case (t = 10 days). \n",
    "## You need to change the dimension if the resolution changes.\n",
    "\n",
    "begin\n",
    "    plot(-130.0+0.3125*4:0.3125*8:-60-0.3125*4, 1e9*A[:,1,179], label = \"Learned\", title = string(\"time: \", string(10), \" days\"), \n",
    "        xtickfontsize=14, ytickfontsize=14, titlefontsize=20, width=4, ylims=(0.0, 1.2e2), legend = false, margin=2.5Plots.mm)\n",
    "    plot!(-130.0+0.3125*4:0.3125*8:-60-0.3125*4, 1e2*input_NN_integrate[:,1,1,179], label = \"Reference\", width=4)\n",
    "end\n",
    "\n",
    "#savefig(\"8x16t_10day.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Example of 1-D visualization with 1dx, 1dt case (4 panels within 10 days - 0, 3.3, 6.6, and 10 days). \n",
    "## You need to change the dimension if the resolution changes.\n",
    "\n",
    "begin\n",
    "    xtf=12\n",
    "    ytf=12\n",
    "    p1=plot(-130.0+0.3125/2:0.3125*1:-60, 1e9*A[:,1,1], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=3,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.4e2), legend = false)\n",
    "    plot!(-130.0+0.3125/2:0.3125*1:-60, 1e2*input_NN_integrate[:,1,1,1], label = \"Reference\", width=4)\n",
    "    p2=plot(-130.0+0.3125/2:0.3125*1:-60, 1e9*A[:,1,960], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=3,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.4e2), legend = false)\n",
    "    plot!(-130.0+0.3125/2:0.3125*1:-60, 1e2*input_NN_integrate[:,1,1,960], label = \"Reference\", width=4)\n",
    "    p3=plot(-130.0+0.3125/2:0.3125*1:-60, 1e9*A[:,1,1920], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=3,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.4e2), legend = false)\n",
    "    plot!(-130.0+0.3125/2:0.3125*1:-60, 1e2*input_NN_integrate[:,1,1,1920], label = \"Reference\", width=4)\n",
    "    p4=plot(-130.0+0.3125/2:0.3125*1:-60, 1e9*A[:,1,2879], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=3,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.4e2), legend = false)\n",
    "    plot!(-130.0+0.3125/2:0.3125*1:-60, 1e2*input_NN_integrate[:,1,1,2879], label = \"Reference\", width=4)\n",
    "    \n",
    "    plot(p1,p2,p3,p4, layout = (1,4), size=(1200, 200), bottommargin=5Plots.mm, rightmargin=2.5Plots.mm)\n",
    "end\n",
    "\n",
    "#savefig(\"1x1t_time_series.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example of 1-D visualization with 8dx, 16dt case (4 panels within 10 days - 0, 3.3, 6.6, and 10 days). \n",
    "## You need to change the dimension if the resolution changes.\n",
    "\n",
    "begin\n",
    "    xtf=12\n",
    "    ytf=12\n",
    "    p1=plot(-130.0+0.3125*4:0.3125*8:-60, 1e9*A_8x16x[:,1,1], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=4,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.1e2), legend = false)\n",
    "    plot!(-130.0+0.3125*4:0.3125*8:-60, 1e2*input_NN_integrate[:,1,1,1], label = \"Reference\", width=4)\n",
    "    p2=plot(-130.0+0.3125*4:0.3125*8:-60, 1e9*A_8x16x[:,1,60], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=4,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.1e2), legend = false)\n",
    "    plot!(-130.0+0.3125*4:0.3125*8:-60, 1e2*input_NN_integrate[:,1,1,60], label = \"Reference\", width=4)\n",
    "    p3=plot(-130.0+0.3125*4:0.3125*8:-60, 1e9*A_8x16x[:,1,120], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=4,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.1e2), legend = false)\n",
    "    plot!(-130.0+0.3125*4:0.3125*8:-60, 1e2*input_NN_integrate[:,1,1,120], label = \"Reference\", width=4)\n",
    "    p4=plot(-130.0+0.3125*4:0.3125*8:-60, 1e9*A_8x16x[:,1,179], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=4,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.1e2), legend = false)\n",
    "    plot!(-130.0+0.3125*4:0.3125*8:-60, 1e2*input_NN_integrate[:,1,1,179], label = \"Reference\", width=4)\n",
    "    \n",
    "    plot(p1,p2,p3,p4, layout = (1,4), size=(1200, 200), bottommargin=5Plots.mm, rightmargin=2.5Plots.mm)\n",
    "end\n",
    "\n",
    "savefig(\"8x16t_time_series.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example of 1-D visualization with 16dx, 64dt case (4 panels within 10 days - 0, 3.3, 6.6, and 10 days). \n",
    "## You need to change the dimension if the resolution changes.\n",
    "\n",
    "begin\n",
    "    xtf=12\n",
    "    ytf=12\n",
    "    p1=plot(-130.0+0.3125*8:0.3125*16:-60, 1e9*A_16x64x[:,1,1], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=4,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.1e2), legend = false)\n",
    "    plot!(-130.0+0.3125*8:0.3125*16:-60, 1e2*input_NN_integrate[:,1,1,1], label = \"Reference\", width=4)\n",
    "    p2=plot(-130.0+0.3125*8:0.3125*16:-60, 1e9*A_16x64x[:,1,15], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=4,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.1e2), legend = false)\n",
    "    plot!(-130.0+0.3125*8:0.3125*16:-60, 1e2*input_NN_integrate[:,1,1,15], label = \"Reference\", width=4)\n",
    "    p3=plot(-130.0+0.3125*8:0.3125*16:-60, 1e9*A_16x64x[:,1,30], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=4,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.1e2), legend = false)\n",
    "    plot!(-130.0+0.3125*8:0.3125*16:-60, 1e2*input_NN_integrate[:,1,1,30], label = \"Reference\", width=4)\n",
    "    p4=plot(-130.0+0.3125*8:0.3125*16:-60, 1e9*A_16x64x[:,1,44], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=4,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.1e2), legend = false)\n",
    "    plot!(-130.0+0.3125*8:0.3125*16:-60, 1e2*input_NN_integrate[:,1,1,44], label = \"Reference\", width=4)\n",
    "    \n",
    "    plot(p1,p2,p3,p4, layout = (1,4), size=(1200, 200), bottommargin=5Plots.mm, rightmargin=2.5Plots.mm)\n",
    "end\n",
    "\n",
    "savefig(\"16x64t_time_series.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
