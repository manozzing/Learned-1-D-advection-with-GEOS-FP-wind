{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook is to train and test the CNN-driven advection scheme.**\n",
    "\n",
    "This notebook perform loading the velocity field, implementing the machine-learned scheme, training the scheme, wallclock time measurement, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages ##\n",
    "\n",
    "#import Pkg\n",
    "#Pkg.add(\"Plots\")\n",
    "using Plots\n",
    "#Pkg.add(\"BenchmarkTools\")\n",
    "using BenchmarkTools\n",
    "#Pkg.add(\"Flux\")\n",
    "using Flux\n",
    "#Pkg.add(\"CSV\")\n",
    "using CSV\n",
    "#Pkg.add(\"DelimitedFiles\")\n",
    "using DelimitedFiles\n",
    "#Pkg.add(\"Statistics\")\n",
    "using Statistics\n",
    "#Pkg.add(\"BSON\")\n",
    "using BSON: @save\n",
    "using BSON: @load\n",
    "#Pkg.add(\"Random\")\n",
    "using Random\n",
    "#Pkg.add(\"CUDA\")\n",
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we trained the model with a single dataset, we used this code.\n",
    "vel1_GEOS_Array = readdlm( \"Training_dataset/Vel_GEOS_Jan_2019_NASA_GMAO_10_U_1x_4x.csv\", ',', Float32);\n",
    "scalar1_GEOS_Array = readdlm(\"Training_dataset/VL_GEOS_Jan_2019_NASA_GMAO_10_U_1x_4x.csv\", ',', Float32);\n",
    "\n",
    "xdim = size(vel1_GEOS_Array, 1);\n",
    "nstep = size(vel1_GEOS_Array, 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22384399893145004"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sometimes, we wanted to check the maximum CFL number in the velocity field we used.\n",
    "maximum(vel1_GEOS_Array*300/27034.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For training with the multiple datasets, we used this code.\n",
    "## For 2-D demonstration, we trained the solver for (16dx, 64dt) resolution again with 7 different datasets, \n",
    "## including the orignial trainign dataset and the generalization testing sets.\n",
    "\n",
    "vel1_GEOS_Array = readdlm( \"Training_dataset/Vel_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "scalar1_GEOS_Array = readdlm(\"Training_dataset/VL_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "vel2_GEOS_Array = readdlm(\"Generalization_tests/Season_April/Vel_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "scalar2_GEOS_Array = readdlm(\"Generalization_tests/Season_April/VL_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "vel3_GEOS_Array = readdlm(\"Generalization_tests/Season_Jul/Vel_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "scalar3_GEOS_Array = readdlm(\"Generalization_tests/Season_Jul/VL_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "vel4_GEOS_Array = readdlm(\"Generalization_tests/Season_Oct/Vel_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "scalar4_GEOS_Array = readdlm(\"Generalization_tests/Season_Oct/VL_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "vel5_GEOS_Array = readdlm(\"Generalization_tests/Lat_29N_corrected/Vel_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "scalar5_GEOS_Array = readdlm(\"Generalization_tests/Lat_29N_corrected/VL_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "vel6_GEOS_Array = readdlm(\"Generalization_tests/Lat_45N_corrected/Vel_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "scalar6_GEOS_Array = readdlm(\"Generalization_tests/Lat_45N_corrected/VL_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "vel7_GEOS_Array = readdlm(\"Generalization_tests/Longitudinal/Vel_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "scalar7_GEOS_Array = readdlm(\"Generalization_tests/Longitudinal/VL_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "xdim = size(vel1_GEOS_Array, 1);\n",
    "ydim = size(vel7_GEOS_Array, 1);\n",
    "nstep = size(vel1_GEOS_Array, 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise and normalize the input scale.\n",
    "\n",
    "Random.seed!(1)\n",
    "history = (scalar1_GEOS_Array*Float32(1e7), vel1_GEOS_Array/15);\n",
    "#history = (scalar1_GEOS_Array*Float32(1e7) + 4e-5*rand(Float32, xdim, nstep), vel1_GEOS_Array/15);\n",
    "#history2 = (scalar2_GEOS_Array*Float32(1e7) + 4e-5*rand(Float32, xdim, nstep), vel2_GEOS_Array/15);\n",
    "#history3 = (scalar3_GEOS_Array*Float32(1e7) + 4e-5*rand(Float32, xdim, nstep), vel3_GEOS_Array/15);\n",
    "#history4 = (scalar4_GEOS_Array*Float32(1e7) + 4e-5*rand(Float32, xdim, nstep), vel4_GEOS_Array/15);\n",
    "#history5 = (scalar5_GEOS_Array*Float32(1e7) + 4e-5*rand(Float32, xdim, nstep), vel5_GEOS_Array/15);\n",
    "#history6 = (scalar6_GEOS_Array*Float32(1e7) + 4e-5*rand(Float32, xdim, nstep), vel6_GEOS_Array/15);\n",
    "#history7 = (scalar7_GEOS_Array*Float32(1e7) + 4e-5*rand(Float32, ydim, nstep), vel7_GEOS_Array/15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the resolutions, we could use the solver code with different kernel sizes. pgm_ml() is the code to integrate throughout the whole timesteps, while one_step_integrate() is the code for a single step integration, which is used in model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pgm_ml (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Integrating through the whole timesteps using ML based advection solver ##\n",
    "## Programming advection scheme with 3 stencil (default) ##\n",
    "\n",
    "function pgm_ml(x, u1, model, xdim_total, nstep_total, dx, dt)\n",
    "    \n",
    "    ## Initialize\n",
    "    history_2x_learned = zeros(Float32, xdim_total, 1, nstep)\n",
    "    history_2x_learned[:,:,1] = x[:,1,1,1]\n",
    "    s1_input = x[:,:,:,1]\n",
    "    #s1_scale = s1_input\n",
    "    s1_bc = zeros(xdim_total+2)\n",
    "    \n",
    "    ## Integrate\n",
    "    for n in 1:nstep_total-1\n",
    "        # learned solver\n",
    "        #coeff_estimated = model(hcat(s1_input, u1[:,:,:,n]))\n",
    "        coeff_estimated = reshape(model(hcat(s1_input, u1[:,:,:,n])), (xdim_total, 2, 3))\n",
    "        su = s1_input\n",
    "        s1_bc = vcat( [su[1]], [su[i] for i in 1:xdim_total], [su[xdim_total]])\n",
    "        s1_scale = reshape(hcat([s1_bc[i] for i in 1:xdim_total],\n",
    "                [s1_bc[i] for i in 2:xdim_total+1],\n",
    "                [s1_bc[i] for i in 3:xdim_total+2]), xdim_total, 3)\n",
    "        s2_2x = reshape(s1_input, xdim_total) + 100*dt/dx*sum(coeff_estimated[:,1,:].*s1_scale, dims=2) \n",
    "                + 10000*(dt*dt)/(dx*dx)*sum(coeff_estimated[:,2,:].*s1_scale, dims=2)\n",
    "        \n",
    "        history_2x_learned[:,1,n+1] = s2_2x\n",
    "        s1_input = reshape(s2_2x, (xdim_total,1,1))\n",
    "    end\n",
    "    \n",
    "    return history_2x_learned\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one_step_integrate (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Integrating a single timestep using ML based advection solver ##\n",
    "## This code is called for model training later ##\n",
    "## Integrate function with 3 stencil (default) ##\n",
    "\n",
    "function one_step_integrate(x, u1, model, xdim_total, dx, dt)\n",
    "    ## Initialize\n",
    "    s1_input = reshape(x, (xdim_total, 1, 1))\n",
    "    #s1_scale = s1_input\n",
    "    \n",
    "    # learned solver\n",
    "    coeff_estimated = reshape(model(hcat(s1_input, u1)), (xdim_total, 2, 3))\n",
    "    su = s1_input\n",
    "    s1_bc = vcat( [su[1]], [su[i] for i in 1:xdim_total], [su[xdim_total]])\n",
    "    s1_scale = reshape(hcat([s1_bc[i] for i in 1:xdim_total],\n",
    "                [s1_bc[i] for i in 2:xdim_total+1],\n",
    "                [s1_bc[i] for i in 3:xdim_total+2]), xdim_total, 3)\n",
    "    s2_2x = reshape(s1_input, xdim_total) + 100*dt/dx*sum(coeff_estimated[:,1,:].*s1_scale, dims=2) \n",
    "                + 10000*(dt*dt)/(dx*dx)*sum(coeff_estimated[:,2,:].*s1_scale, dims=2)\n",
    "    \n",
    "    s1_input = reshape(s2_2x, (xdim_total,1,1))\n",
    "    return s2_2x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pgm_ml (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Integrating through the whole timesteps using ML based advection solver ##\n",
    "## Programming advection scheme with 5 stencil ##\n",
    "\n",
    "function pgm_ml(x, u1, model, xdim_total, nstep_total, dx, dt)\n",
    "    \n",
    "    ## Initialize\n",
    "    history_2x_learned = zeros(Float32, xdim, 1, nstep)\n",
    "    history_2x_learned[:,:,1] = x[:,1,1,1]\n",
    "    s1_input = x[:,:,:,1]\n",
    "    #s1_scale = s1_input\n",
    "    s1_bc = zeros(xdim_total+4)\n",
    "    \n",
    "    ## Integrate\n",
    "    for n in 1:nstep_total-1\n",
    "        # learned solver\n",
    "        #coeff_estimated = model(hcat(s1_input, u1[:,:,:,n]))\n",
    "        coeff_estimated = reshape(model(hcat(s1_input, u1[:,:,:,n])), (xdim_total, 2, 5))\n",
    "        su = s1_input\n",
    "        s1_bc = vcat( [su[1]], [su[1]], [su[i] for i in 1:xdim_total], [su[xdim_total]], [su[xdim_total]])\n",
    "        s1_scale = reshape(hcat([s1_bc[i] for i in 1:xdim_total],\n",
    "                [s1_bc[i] for i in 2:xdim_total+1],\n",
    "                [s1_bc[i] for i in 3:xdim_total+2],\n",
    "                [s1_bc[i] for i in 4:xdim_total+3],\n",
    "                [s1_bc[i] for i in 5:xdim_total+4]), xdim_total, 5)\n",
    "        s2_2x = reshape(s1_input, xdim_total) + 100*dt/dx*sum(coeff_estimated[:,1,:].*s1_scale, dims=2) \n",
    "                + 10000*(dt*dt)/(dx*dx)*sum(coeff_estimated[:,2,:].*s1_scale, dims=2)\n",
    "        \n",
    "        history_2x_learned[:,1,n+1] = s2_2x\n",
    "        s1_input = reshape(s2_2x, (xdim_total,1,1))\n",
    "    end\n",
    "    \n",
    "    return history_2x_learned\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one_step_integrate (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Integrating a single timestep using ML based advection solver ##\n",
    "## This code is called for model training later ##\n",
    "## Integrate function with 5 stencil ##\n",
    "\n",
    "function one_step_integrate(x, u1, model, xdim_total, dx, dt)\n",
    "    ## Initialize\n",
    "    s1_input = reshape(x, (xdim, 1, 1))\n",
    "    #s1_scale = s1_input\n",
    "    \n",
    "    # learned solver\n",
    "    coeff_estimated = reshape(model(hcat(s1_input, u1)), (xdim_total, 2, 5))\n",
    "    su = s1_input\n",
    "    s1_bc = vcat( [su[1]], [su[1]], [su[i] for i in 1:xdim_total], [su[xdim_total]], [su[xdim_total]])\n",
    "    s1_scale = reshape(hcat([s1_bc[i] for i in 1:xdim_total],\n",
    "                [s1_bc[i] for i in 2:xdim_total+1],\n",
    "                [s1_bc[i] for i in 3:xdim_total+2],\n",
    "                [s1_bc[i] for i in 4:xdim_total+3],\n",
    "                [s1_bc[i] for i in 5:xdim_total+4]), xdim_total, 5)\n",
    "    s2_2x = reshape(s1_input, xdim_total) + 100*dt/dx*sum(coeff_estimated[:,1,:].*s1_scale, dims=2) \n",
    "                + 10000*(dt*dt)/(dx*dx)*sum(coeff_estimated[:,2,:].*s1_scale, dims=2)\n",
    "    \n",
    "    s1_input = reshape(s2_2x, (xdim_total,1,1))\n",
    "    return s2_2x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pgm_ml (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Integrating through the whole timesteps using ML based advection solver ##\n",
    "## Programming advection scheme with 9 stencil ##\n",
    "\n",
    "function pgm_ml(x, u1, model, xdim_total, nstep_total, dx, dt)\n",
    "    \n",
    "    ## Initialize\n",
    "    history_2x_learned = zeros(Float32, xdim, 1, nstep)\n",
    "    history_2x_learned[:,:,1] = x[:,1,1,1]\n",
    "    s1_input = x[:,:,:,1]\n",
    "    #s1_scale = s1_input\n",
    "    #s1_bc = zeros(xdim_total+4)\n",
    "    \n",
    "    ## Integrate\n",
    "    for n in 1:nstep_total-1\n",
    "        # learned solver\n",
    "        #coeff_estimated = model(hcat(s1_input, u1[:,:,:,n]))\n",
    "        coeff_estimated = reshape(model(hcat(s1_input, u1[:,:,:,n])), (xdim_total, 2, 9))\n",
    "        su = s1_input\n",
    "        s1_bc = vcat( [su[1]], [su[1]], [su[1]], [su[1]], [su[i] for i in 1:xdim_total], \n",
    "            [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]])\n",
    "        s1_scale = reshape(hcat([s1_bc[i] for i in 1:xdim_total],\n",
    "                [s1_bc[i] for i in 2:xdim_total+1],\n",
    "                [s1_bc[i] for i in 3:xdim_total+2],\n",
    "                [s1_bc[i] for i in 4:xdim_total+3],\n",
    "                [s1_bc[i] for i in 5:xdim_total+4],\n",
    "                [s1_bc[i] for i in 6:xdim_total+5],\n",
    "                [s1_bc[i] for i in 7:xdim_total+6],\n",
    "                [s1_bc[i] for i in 8:xdim_total+7],\n",
    "                [s1_bc[i] for i in 9:xdim_total+8]), xdim_total, 9)\n",
    "        s2_2x = reshape(s1_input, xdim_total) + 100*dt/dx*sum(coeff_estimated[:,1,:].*s1_scale, dims=2) \n",
    "                + 10000*(dt*dt)/(dx*dx)*sum(coeff_estimated[:,2,:].*s1_scale, dims=2)\n",
    "        \n",
    "        history_2x_learned[:,1,n+1] = s2_2x\n",
    "        s1_input = reshape(s2_2x, (xdim_total,1,1))\n",
    "    end\n",
    "    \n",
    "    return history_2x_learned\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one_step_integrate (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Integrating a single timestep using ML based advection solver ##\n",
    "## This code is called for model training later ##\n",
    "## Integrate function with 9 stencil ##\n",
    "\n",
    "function one_step_integrate(x, u1, model, xdim_total, dx, dt)\n",
    "    ## Initialize\n",
    "    s1_input = reshape(x, (xdim, 1, 1))\n",
    "    #s1_scale = s1_input\n",
    "    \n",
    "    # learned solver\n",
    "    coeff_estimated = reshape(model(hcat(s1_input, u1)), (xdim_total, 2, 9))\n",
    "    su = s1_input\n",
    "    s1_bc = vcat( [su[1]], [su[1]], [su[1]], [su[1]], [su[i] for i in 1:xdim_total], \n",
    "            [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]])\n",
    "    s1_scale = reshape(hcat([s1_bc[i] for i in 1:xdim_total],\n",
    "                [s1_bc[i] for i in 2:xdim_total+1],\n",
    "                [s1_bc[i] for i in 3:xdim_total+2],\n",
    "                [s1_bc[i] for i in 4:xdim_total+3],\n",
    "                [s1_bc[i] for i in 5:xdim_total+4],\n",
    "                [s1_bc[i] for i in 6:xdim_total+5],\n",
    "                [s1_bc[i] for i in 7:xdim_total+6],\n",
    "                [s1_bc[i] for i in 8:xdim_total+7],\n",
    "                [s1_bc[i] for i in 9:xdim_total+8]), xdim_total, 9)\n",
    "    s2_2x = reshape(s1_input, xdim_total) + 100*dt/dx*sum(coeff_estimated[:,1,:].*s1_scale, dims=2) \n",
    "                + 10000*(dt*dt)/(dx*dx)*sum(coeff_estimated[:,2,:].*s1_scale, dims=2)\n",
    "    \n",
    "    s1_input = reshape(s2_2x, (xdim_total,1,1))\n",
    "    return s2_2x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pgm_ml (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Integrating through the whole timesteps using ML based advection solver ##\n",
    "## Programming advection scheme with 17 stencil ##\n",
    "\n",
    "function pgm_ml(x, u1, model, xdim_total, nstep_total, dx, dt)\n",
    "    \n",
    "    ## Initialize\n",
    "    history_2x_learned = zeros(Float32, xdim, 1, nstep)\n",
    "    history_2x_learned[:,:,1] = x[:,1,1,1]\n",
    "    s1_input = x[:,:,:,1]\n",
    "    #s1_scale = s1_input\n",
    "    #s1_bc = zeros(xdim_total+4)\n",
    "    \n",
    "    ## Integrate\n",
    "    for n in 1:nstep_total-1\n",
    "        # learned solver\n",
    "        #coeff_estimated = model(hcat(s1_input, u1[:,:,:,n]))\n",
    "        coeff_estimated = reshape(model(hcat(s1_input, u1[:,:,:,n])), (xdim_total, 2, 17))\n",
    "        su = s1_input\n",
    "        s1_bc = vcat( [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[i] for i in 1:xdim_total], \n",
    "            [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], \n",
    "            [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]])\n",
    "        s1_scale = reshape(hcat([s1_bc[i] for i in 1:xdim_total],\n",
    "                [s1_bc[i] for i in 2:xdim_total+1],\n",
    "                [s1_bc[i] for i in 3:xdim_total+2],\n",
    "                [s1_bc[i] for i in 4:xdim_total+3],\n",
    "                [s1_bc[i] for i in 5:xdim_total+4],\n",
    "                [s1_bc[i] for i in 6:xdim_total+5],\n",
    "                [s1_bc[i] for i in 7:xdim_total+6],\n",
    "                [s1_bc[i] for i in 8:xdim_total+7],\n",
    "                [s1_bc[i] for i in 9:xdim_total+8],\n",
    "                [s1_bc[i] for i in 10:xdim_total+9],\n",
    "                [s1_bc[i] for i in 11:xdim_total+10],\n",
    "                [s1_bc[i] for i in 12:xdim_total+11],\n",
    "                [s1_bc[i] for i in 13:xdim_total+12],\n",
    "                [s1_bc[i] for i in 14:xdim_total+13],\n",
    "                [s1_bc[i] for i in 15:xdim_total+14],\n",
    "                [s1_bc[i] for i in 16:xdim_total+15],\n",
    "                [s1_bc[i] for i in 17:xdim_total+16]), xdim_total, 17)\n",
    "        s2_2x = reshape(s1_input, xdim_total) + 100*dt/dx*sum(coeff_estimated[:,1,:].*s1_scale, dims=2) \n",
    "                + 10000*(dt*dt)/(dx*dx)*sum(coeff_estimated[:,2,:].*s1_scale, dims=2)\n",
    "        \n",
    "        history_2x_learned[:,1,n+1] = s2_2x\n",
    "        s1_input = reshape(s2_2x, (xdim_total,1,1))\n",
    "    end\n",
    "    \n",
    "    return history_2x_learned\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one_step_integrate (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Integrating a single timestep using ML based advection solver ##\n",
    "## This code is called for model training later ##\n",
    "## Integrate function with 17 stencil ##\n",
    "\n",
    "function one_step_integrate(x, u1, model, xdim_total, dx, dt)\n",
    "    ## Initialize\n",
    "    s1_input = reshape(x, (xdim, 1, 1))\n",
    "    #s1_scale = s1_input\n",
    "    \n",
    "    # learned solver\n",
    "    coeff_estimated = reshape(model(hcat(s1_input, u1)), (xdim_total, 2, 17))\n",
    "    su = s1_input\n",
    "    s1_bc = vcat( [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[i] for i in 1:xdim_total], \n",
    "            [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], \n",
    "            [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]])\n",
    "        s1_scale = reshape(hcat([s1_bc[i] for i in 1:xdim_total],\n",
    "                [s1_bc[i] for i in 2:xdim_total+1],\n",
    "                [s1_bc[i] for i in 3:xdim_total+2],\n",
    "                [s1_bc[i] for i in 4:xdim_total+3],\n",
    "                [s1_bc[i] for i in 5:xdim_total+4],\n",
    "                [s1_bc[i] for i in 6:xdim_total+5],\n",
    "                [s1_bc[i] for i in 7:xdim_total+6],\n",
    "                [s1_bc[i] for i in 8:xdim_total+7],\n",
    "                [s1_bc[i] for i in 9:xdim_total+8],\n",
    "                [s1_bc[i] for i in 10:xdim_total+9],\n",
    "                [s1_bc[i] for i in 11:xdim_total+10],\n",
    "                [s1_bc[i] for i in 12:xdim_total+11],\n",
    "                [s1_bc[i] for i in 13:xdim_total+12],\n",
    "                [s1_bc[i] for i in 14:xdim_total+13],\n",
    "                [s1_bc[i] for i in 15:xdim_total+14],\n",
    "                [s1_bc[i] for i in 16:xdim_total+15],\n",
    "                [s1_bc[i] for i in 17:xdim_total+16]), xdim_total, 17)\n",
    "    s2_2x = reshape(s1_input, xdim_total) + 100*dt/dx*sum(coeff_estimated[:,1,:].*s1_scale, dims=2) \n",
    "                + 10000*(dt*dt)/(dx*dx)*sum(coeff_estimated[:,2,:].*s1_scale, dims=2)\n",
    "    \n",
    "    s1_input = reshape(s2_2x, (xdim_total,1,1))\n",
    "    return s2_2x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pgm_ml (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Integrating through the whole timesteps using ML based advection solver ##\n",
    "## Programming advection scheme with 31 stencil ##\n",
    "\n",
    "function pgm_ml(x, u1, model, xdim_total, nstep_total, dx, dt)\n",
    "    \n",
    "    ## Initialize\n",
    "    history_2x_learned = zeros(Float32, xdim, 1, nstep)\n",
    "    history_2x_learned[:,:,1] = x[:,1,1,1]\n",
    "    s1_input = x[:,:,:,1]\n",
    "    #s1_scale = s1_input\n",
    "    #s1_bc = zeros(xdim_total+4)\n",
    "    \n",
    "    ## Integrate\n",
    "    for n in 1:nstep_total-1\n",
    "        # learned solver\n",
    "        #coeff_estimated = model(hcat(s1_input, u1[:,:,:,n]))\n",
    "        coeff_estimated = reshape(model(hcat(s1_input, u1[:,:,:,n])), (xdim_total, 2, 31))\n",
    "        su = s1_input\n",
    "        s1_bc = vcat( [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], \n",
    "                [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[i] for i in 1:xdim_total], \n",
    "                [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]],\n",
    "                [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]],\n",
    "                [su[xdim_total]], [su[xdim_total]], [su[xdim_total]])\n",
    "        s1_scale = reshape(hcat([s1_bc[i] for i in 1:xdim_total],\n",
    "                [s1_bc[i] for i in 2:xdim_total+1],\n",
    "                [s1_bc[i] for i in 3:xdim_total+2],\n",
    "                [s1_bc[i] for i in 4:xdim_total+3],\n",
    "                [s1_bc[i] for i in 5:xdim_total+4],\n",
    "                [s1_bc[i] for i in 6:xdim_total+5],\n",
    "                [s1_bc[i] for i in 7:xdim_total+6],\n",
    "                [s1_bc[i] for i in 8:xdim_total+7],\n",
    "                [s1_bc[i] for i in 9:xdim_total+8],\n",
    "                [s1_bc[i] for i in 10:xdim_total+9],\n",
    "                [s1_bc[i] for i in 11:xdim_total+10],\n",
    "                [s1_bc[i] for i in 12:xdim_total+11],\n",
    "                [s1_bc[i] for i in 13:xdim_total+12],\n",
    "                [s1_bc[i] for i in 14:xdim_total+13],\n",
    "                [s1_bc[i] for i in 15:xdim_total+14],\n",
    "                [s1_bc[i] for i in 16:xdim_total+15],\n",
    "                [s1_bc[i] for i in 17:xdim_total+16],\n",
    "                [s1_bc[i] for i in 18:xdim_total+17],\n",
    "                [s1_bc[i] for i in 19:xdim_total+18],\n",
    "                [s1_bc[i] for i in 20:xdim_total+19],\n",
    "                [s1_bc[i] for i in 21:xdim_total+20],\n",
    "                [s1_bc[i] for i in 22:xdim_total+21],\n",
    "                [s1_bc[i] for i in 23:xdim_total+22],\n",
    "                [s1_bc[i] for i in 24:xdim_total+23],\n",
    "                [s1_bc[i] for i in 25:xdim_total+24],\n",
    "                [s1_bc[i] for i in 26:xdim_total+25],\n",
    "                [s1_bc[i] for i in 27:xdim_total+26],\n",
    "                [s1_bc[i] for i in 28:xdim_total+27],\n",
    "                [s1_bc[i] for i in 29:xdim_total+28],\n",
    "                [s1_bc[i] for i in 30:xdim_total+29],\n",
    "                [s1_bc[i] for i in 31:xdim_total+30]), xdim_total, 31)\n",
    "        s2_2x = reshape(s1_input, xdim_total) + 100*dt/dx*sum(coeff_estimated[:,1,:].*s1_scale, dims=2) \n",
    "                + 10000*(dt*dt)/(dx*dx)*sum(coeff_estimated[:,2,:].*s1_scale, dims=2)\n",
    "        \n",
    "        history_2x_learned[:,1,n+1] = s2_2x\n",
    "        s1_input = reshape(s2_2x, (xdim_total,1,1))\n",
    "    end\n",
    "    \n",
    "    return history_2x_learned\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one_step_integrate (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Integrating a single timestep using ML based advection solver ##\n",
    "## This code is called for model training later ##\n",
    "## Integrate function with 31 stencil ##\n",
    "\n",
    "function one_step_integrate(x, u1, model, xdim_total, dx, dt)\n",
    "    ## Initialize\n",
    "    s1_input = reshape(x, (xdim, 1, 1))\n",
    "    #s1_scale = s1_input\n",
    "    \n",
    "    # learned solver\n",
    "    coeff_estimated = reshape(model(hcat(s1_input, u1)), (xdim_total, 2, 31))\n",
    "    su = s1_input\n",
    "    s1_bc = vcat( [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]],\n",
    "                [su[1]], [su[1]], [su[1]], [su[1]], [su[1]], [su[1]],\n",
    "                [su[1]], [su[1]], [su[1]], [su[i] for i in 1:xdim_total], \n",
    "                [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]],\n",
    "                [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]], [su[xdim_total]],\n",
    "                [su[xdim_total]], [su[xdim_total]], [su[xdim_total]])\n",
    "    s1_scale = reshape(hcat([s1_bc[i] for i in 1:xdim_total],\n",
    "                [s1_bc[i] for i in 2:xdim_total+1],\n",
    "                [s1_bc[i] for i in 3:xdim_total+2],\n",
    "                [s1_bc[i] for i in 4:xdim_total+3],\n",
    "                [s1_bc[i] for i in 5:xdim_total+4],\n",
    "                [s1_bc[i] for i in 6:xdim_total+5],\n",
    "                [s1_bc[i] for i in 7:xdim_total+6],\n",
    "                [s1_bc[i] for i in 8:xdim_total+7],\n",
    "                [s1_bc[i] for i in 9:xdim_total+8],\n",
    "                [s1_bc[i] for i in 10:xdim_total+9],\n",
    "                [s1_bc[i] for i in 11:xdim_total+10],\n",
    "                [s1_bc[i] for i in 12:xdim_total+11],\n",
    "                [s1_bc[i] for i in 13:xdim_total+12],\n",
    "                [s1_bc[i] for i in 14:xdim_total+13],\n",
    "                [s1_bc[i] for i in 15:xdim_total+14],\n",
    "                [s1_bc[i] for i in 16:xdim_total+15],\n",
    "                [s1_bc[i] for i in 17:xdim_total+16],\n",
    "                [s1_bc[i] for i in 18:xdim_total+17],\n",
    "                [s1_bc[i] for i in 19:xdim_total+18],\n",
    "                [s1_bc[i] for i in 20:xdim_total+19],\n",
    "                [s1_bc[i] for i in 21:xdim_total+20],\n",
    "                [s1_bc[i] for i in 22:xdim_total+21],\n",
    "                [s1_bc[i] for i in 23:xdim_total+22],\n",
    "                [s1_bc[i] for i in 24:xdim_total+23],\n",
    "                [s1_bc[i] for i in 25:xdim_total+24],\n",
    "                [s1_bc[i] for i in 26:xdim_total+25],\n",
    "                [s1_bc[i] for i in 27:xdim_total+26],\n",
    "                [s1_bc[i] for i in 28:xdim_total+27],\n",
    "                [s1_bc[i] for i in 29:xdim_total+28],\n",
    "                [s1_bc[i] for i in 30:xdim_total+29],\n",
    "                [s1_bc[i] for i in 31:xdim_total+30]), xdim_total, 31)\n",
    "    s2_2x = reshape(s1_input, xdim_total) + 100*dt/dx*sum(coeff_estimated[:,1,:].*s1_scale, dims=2) \n",
    "                + 10000*(dt*dt)/(dx*dx)*sum(coeff_estimated[:,2,:].*s1_scale, dims=2)\n",
    "    \n",
    "    s1_input = reshape(s2_2x, (xdim_total,1,1))\n",
    "    return s2_2x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate arrays for storing training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good\n"
     ]
    }
   ],
   "source": [
    "## Generate arrays to store the input (scalar and velocity) and target (next time scalar) data ##\n",
    "input_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1) #conentration\n",
    "u_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1) #velocity\n",
    "target_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1) #next time concentration\n",
    "\n",
    "## Below are used when we are feeding multiple training datasets ##\n",
    "\n",
    "#input2_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "#u2_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "#target2_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "\n",
    "#input3_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "#u3_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "#target3_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "\n",
    "#input4_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "#u4_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "#target4_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "\n",
    "#input5_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "#u5_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "#target5_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "\n",
    "#input6_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "#u6_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "#target6_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "\n",
    "#input7_NN_integrate = zeros(Float32, ydim, 1, 1, nstep-1)\n",
    "#u7_NN_integrate = zeros(Float32, ydim, 1, 1, nstep-1)\n",
    "#target7_NN_integrate = zeros(Float32, ydim, 1, 1, nstep-1)\n",
    "\n",
    "## When we use GPU to train ##\n",
    "\n",
    "#input_NN_integrate |> gpu\n",
    "#u_NN_integrate |> gpu\n",
    "#target_NN_integrate |> gpu\n",
    "\n",
    "## I just wanted to see if it is good ##\n",
    "println(\"Good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224×719 Matrix{Float32}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.00174551  0.00179036  0.00183205\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.00510731  0.00523452  0.00535237\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.00827073  0.00846375  0.00864138\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0114123   0.0116524   0.0118712\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0146162   0.014881    0.0151192\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0176034   0.0178622   0.0180908\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0200861   0.0203042   0.0204916\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0220622   0.0222113   0.0223316\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0237343   0.023795    0.0238316\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0251714   0.0251331   0.0250783\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0261078   0.0259686   0.0258215\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0265167   0.0262874   0.0260614\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0266282   0.0263254   0.0260375\n",
       " ⋮                        ⋮         ⋱                          \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.635978    0.616812    0.598387\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.673908    0.654688    0.636061\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.719137    0.700375    0.682036\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.778115    0.760464    0.743033\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.851636    0.836182    0.820668\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.936899    0.925331    0.913289\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.02581     1.02033     1.0138\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.09594     1.09892     1.10009\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  1.1495      1.16279     1.17341\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.19971     1.22406     1.24482\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.2375      1.27165     1.30141\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.24097     1.28062     1.31546"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now we store the scalar, velocity at the generated array ##\n",
    "\n",
    "input_NN_integrate[:,1,1,:] = history[1][:,1:nstep-1]\n",
    "u_NN_integrate[:,1,1,:] = history[2][:,1:nstep-1]\n",
    "target_NN_integrate[:,1,1,:] = history[1][:,2:nstep]\n",
    "\n",
    "#input2_NN_integrate[:,1,1,:] = history2[1][:,1:nstep-1]\n",
    "#u2_NN_integrate[:,1,1,:] = history2[2][:,1:nstep-1]\n",
    "#target2_NN_integrate[:,1,1,:] = history2[1][:,2:nstep]\n",
    "\n",
    "#input3_NN_integrate[:,1,1,:] = history3[1][:,1:nstep-1]\n",
    "#u3_NN_integrate[:,1,1,:] = history3[2][:,1:nstep-1]\n",
    "#target3_NN_integrate[:,1,1,:] = history3[1][:,2:nstep]\n",
    "\n",
    "#input4_NN_integrate[:,1,1,:] = history4[1][:,1:nstep-1]\n",
    "#u4_NN_integrate[:,1,1,:] = history4[2][:,1:nstep-1]\n",
    "#target4_NN_integrate[:,1,1,:] = history4[1][:,2:nstep]\n",
    "\n",
    "#input5_NN_integrate[:,1,1,:] = history5[1][:,1:nstep-1]\n",
    "#u5_NN_integrate[:,1,1,:] = history5[2][:,1:nstep-1]\n",
    "#target5_NN_integrate[:,1,1,:] = history5[1][:,2:nstep]\n",
    "\n",
    "#input6_NN_integrate[:,1,1,:] = history6[1][:,1:nstep-1]\n",
    "#u6_NN_integrate[:,1,1,:] = history6[2][:,1:nstep-1]\n",
    "#target6_NN_integrate[:,1,1,:] = history6[1][:,2:nstep]\n",
    "\n",
    "#input7_NN_integrate[:,1,1,:] = history7[1][:,1:nstep-1]\n",
    "#u7_NN_integrate[:,1,1,:] = history7[2][:,1:nstep-1]\n",
    "#target7_NN_integrate[:,1,1,:] = history7[1][:,2:nstep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the convolutional neural networks. Depending on the problem complexity we could use different size of network. We could change the kernel size at the begining, or the number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([[-0.23724414 -0.1847739; -0.2186838 -0.13725726; -0.18596105 -0.051771183;;; 0.107935704 0.09306445; -0.23084818 -0.05097895; -0.02000927 0.12976532;;; 0.027414659 0.17254841; -0.20021169 0.16721494; 0.17437194 0.16511054;;; … ;;; 0.05513018 -0.035334747; 0.15314297 -0.061656483; 0.13123395 0.13320887;;; 0.16111721 -0.03930819; 0.03960307 -0.044986144; -0.21798879 0.1565445;;; 0.063831136 0.11841493; -0.16270801 0.17739886; -0.062633 -0.096048824], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07363952 -0.047109388 … -0.12786153 0.003514183; -0.17596187 -0.08074215 … 0.064750224 -0.13086778; -0.09424503 0.15709022 … -0.15967849 0.1111644;;; -0.17100489 0.16413192 … 0.12996615 -0.081389844; -0.033714738 -0.032853067 … -0.14265299 -0.055032805; 0.02673326 -0.015684191 … -0.12769084 0.011887202;;; -0.03100634 -0.12751737 … 0.16694982 0.09528412; 0.14384148 -0.15594721 … -0.17209105 0.16045; 0.010559387 -0.027374335 … -0.12399238 0.045416664;;; … ;;; 0.1283581 -0.14920998 … -0.0926711 -0.11721641; 0.00027932823 -0.073874384 … 0.14511956 -0.08530611; -0.14782575 -0.14287381 … 0.13870715 0.012605427;;; -0.11735579 -0.1765631 … 0.09848797 0.009204345; -0.08498684 0.17157038 … 0.015785828 0.14241292; -0.0044220896 0.15291809 … -0.012105586 0.15746945;;; 0.113224454 -0.00064788136 … -0.03921644 -0.09644602; -0.16662727 0.10528382 … 0.16773668 0.13854404; 0.103757724 -0.0544661 … -0.088971786 0.00031597493], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [-0.040679753 0.060144078 … -0.08698044 0.03747257; 0.06255646 -0.025641257 … 0.019072425 0.07752642; -0.08147927 0.18824448 … -0.19724697 -0.1900684;;; 0.02047882 -0.013012054 … 0.15016791 -0.0952495; -0.17377803 0.13815413 … 0.19524921 0.044143576; -0.17008097 0.089598015 … -0.14150126 -0.22746786;;; -0.1520864 0.09855306 … -0.0717647 0.15120678; -0.042682536 0.012270993 … -0.174672 -0.08879561; 0.09237033 -0.1656183 … 0.10022206 -0.0038136917;;; 0.17941701 -0.09827235 … 0.13365725 0.07337079; -0.17625019 -0.13227019 … -0.228546 -0.014580353; 0.1851027 -0.04876971 … 0.15791248 -0.037369873;;; 0.13556713 0.1282958 … 0.11703287 0.08840579; 0.22086895 -0.14683837 … -0.12513532 0.18617323; -0.19544688 0.086719945 … 0.16624855 -0.07290302;;; 0.07018535 -0.13869245 … -0.05463076 0.11432589; -0.054591436 -0.15231818 … -0.10465303 -0.20729236; -0.07527365 0.021156488 … 0.16446634 -0.12686181], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Three layer model\n",
    "model = Chain( Conv((3,), 2 => 32, pad = SamePad(), gelu),\n",
    "    Conv((3,), 32 => 32, pad = SamePad(), gelu),\n",
    "    Conv((3,), 32 => 6, pad = SamePad(), tanh))\n",
    "\n",
    "loss(x, y) = Flux.Losses.mae(model(x), y)\n",
    "\n",
    "ps = Flux.params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([[-0.013975545 -0.003936288; 0.042066474 -0.026185589; … ; 0.03067407 0.035301037; -0.009458232 0.017693035;;; -0.050075695 0.017394336; 0.038545646 0.0072103273; … ; -0.010611166 -0.009392463; -0.012942548 0.028999677;;; -0.04133677 0.0354597; 4.398163f-5 0.040843237; … ; -0.009089743 0.02957335; -0.049952175 0.031220622;;; … ;;; 0.047856543 -0.020572983; -0.0066741556 0.020885864; … ; -0.04208318 -0.050180115; 0.050199617 -0.045900553;;; -0.034758545 -0.04091833; -0.028321825 0.009372709; … ; 0.016248433 0.008062511; 0.022425653 0.040175967;;; 0.030734986 0.051149562; -0.014032257 0.034139052; … ; 0.04936472 0.017351452; 0.00069669535 -0.053708147], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.038770914 -0.08905837 … 0.049702153 -0.11076902; 0.030447751 0.058050454 … 0.049610406 0.06725255; 0.11006053 -0.064584866 … 0.049740598 -0.118652225;;; 0.08074947 0.117760435 … 0.014490351 0.015219733; 0.036376163 -0.056980923 … -0.072974384 0.102674395; 0.11357291 0.0024172366 … 0.10449226 -0.025415301;;; -0.034056336 0.05063562 … 0.0700507 0.050410464; -0.09263387 -0.018611267 … 0.06983121 -0.008905247; -0.00969705 0.075088456 … -0.017455384 -0.073092535;;; … ;;; -0.092837185 -0.018051356 … 0.108147115 0.040316984; -0.062325582 -0.0041136444 … 0.09562506 -0.058744907; 0.06496684 0.07741544 … -0.06436461 0.030789271;;; 0.03146726 -0.11079918 … 0.030904561 0.0028549582; 0.062496275 -0.023018494 … 0.004933089 0.071897626; 0.03246899 -0.06865798 … 0.019298956 -0.06871441;;; 0.09325206 0.10474315 … -0.086678386 0.100657165; 0.007701084 -0.057369813 … 0.09123184 -0.015963301; 0.0345999 0.0948745 … 0.03538978 -0.066999644], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11492348 -0.06585708 … 0.0011636317 0.021221012; 0.034134805 -0.013887823 … 0.10785085 0.05812095; -0.09320374 -0.11938052 … 0.10663773 0.1070167;;; -0.07933578 0.07035348 … 0.091533795 -0.06748888; -0.057560056 0.07190174 … 0.020238727 0.0090601295; -0.11062619 0.055085123 … 0.008853257 0.09433669;;; -0.109923646 0.06420761 … 0.011342168 0.06593956; -0.12134814 -0.057058766 … -0.01741679 -0.04359892; -0.06712738 -0.046376392 … -0.12040557 -0.07047014;;; … ;;; -0.004737079 -0.10062075 … 0.08703978 0.1006497; 0.02119866 0.008867621 … 0.09043099 -0.04249777; 0.059048846 0.01468654 … 0.018424407 -0.06464884;;; 0.06526974 0.08218673 … -0.006751105 0.025585383; -0.09523404 0.059761927 … 0.01148358 -0.055184513; 0.04279588 0.028989598 … -0.01928237 0.09178653;;; 0.03580612 -0.016940191 … 0.07013388 -0.063176334; 0.06983517 0.002145633 … 0.10255548 -0.070399806; 0.016159654 -0.1071372 … -0.026336327 0.053585052], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [-0.0857634 0.042577185 … -0.020853464 0.016681137; -0.0037798362 -0.029424572 … 0.025713433 0.044344228; -0.11722099 -0.008686891 … 0.096465856 -0.03428184;;; 0.124377385 0.06068009 … -0.081135266 0.025664607; -0.110557854 0.08859367 … -0.032544054 0.08715359; 0.027340302 -0.02801291 … -0.02226541 0.07205761;;; -0.119074136 0.07796666 … -0.07471065 0.043197002; -0.025446681 -0.10715653 … 0.017022789 0.005770209; -0.05359784 0.049148835 … -0.0599323 -0.065081306;;; … ;;; -0.053136803 0.060231596 … -0.06780907 0.01861989; 0.1092566 -0.09426851 … -0.004651702 0.061696153; 0.08785981 -0.053388134 … 0.054687634 -0.09219999;;; -0.114419505 0.0347618 … -0.117158584 -0.10534641; -0.12060467 0.08509392 … 0.08474889 0.08884989; 0.07157725 0.059040923 … 0.10718277 -0.031821493;;; -0.11713308 0.024573179 … 0.044559687 -0.10554484; -0.09881339 -0.07947697 … -0.1210504 0.07832337; 0.059659343 -0.008173212 … 0.0784307 0.109654434], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## In case we need to use the larger size model - 4-layer here\n",
    "\n",
    "model = Chain( Conv((31,), 2 => 64, pad = SamePad(), gelu),\n",
    "    Conv((3,), 64 => 64, pad = SamePad(), gelu),\n",
    "    Conv((3,), 64 => 64, pad = SamePad(), gelu),\n",
    "    Conv((3,), 64 => 62, pad = SamePad(), tanh))\n",
    "\n",
    "loss(x, y) = Flux.Losses.mae(model(x), y)\n",
    "\n",
    "ps = Flux.params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([[0.022558425 0.015880609; -0.053903427 0.042891253; … ; 0.037100986 0.00016292505; -0.005801295 -0.03196833;;; 0.036214955 0.0010765202; -0.052104957 -0.040089488; … ; 0.019835314 -0.014859896; -0.048915245 0.01938555;;; -0.039000865 -0.0076553146; 0.00027360537 0.04754961; … ; -0.0440438 0.0022800984; -0.0288473 -0.00339368;;; … ;;; -0.02689298 -0.036051326; -0.0070830756 0.009582179; … ; -0.0015065111 0.03250881; 0.0007742006 0.05202653;;; 0.05397415 -0.019132925; -0.04125702 0.044648807; … ; -0.031683777 -0.035321068; -0.014946439 0.018531816;;; -0.0023965242 -0.0115464525; 0.053014304 -0.014093655; … ; -0.0029072613 0.0049230373; -0.005792154 0.007547894], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [-0.018097539 0.026756795 … 0.06680644 -0.042374432; 0.027830012 -0.011407238 … 0.08686213 0.019638518; -0.036248356 0.083745874 … -0.062950835 -0.101195514;;; -0.06765994 0.043844115 … 0.059461206 0.03264107; -0.018988533 0.005459098 … -0.10167515 -0.0064864825; 0.041093554 -0.07367998 … 0.07025184 -0.016625045;;; 0.06031087 0.057076015 … -0.02430404 0.050861105; 0.0982598 -0.0653252 … -0.04655786 -0.092219874; -0.08695008 0.03857982 … 0.073167495 -0.056438062;;; … ;;; 0.043382388 0.09781149 … 0.10166726 0.060129747; 0.059422176 0.05708302 … -0.026517803 -0.076331936; 0.05193993 0.061365645 … -0.07385664 -0.06297057;;; -0.094907455 0.02345541 … -0.09638005 0.04276747; 0.051744435 0.07786834 … 0.05575041 -0.005706594; -0.044566736 0.05924582 … -0.022284165 0.073240094;;; -0.03397843 0.012674917 … 0.07220982 0.0998548; 0.008275396 0.0131953005 … -0.05536733 0.03970966; 0.022160806 0.054766703 … 0.0505908 0.07595559], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.054514386 -0.004573871 … -0.06680527 -0.08556433; 0.03835277 -0.06562834 … -0.024036516 -0.009060087; 0.019943172 0.08192927 … -0.085593306 -0.062130995;;; -0.003931226 0.048532538 … 0.05393525 -0.019845665; 0.046805877 -0.0880997 … -0.041733526 -0.037818104; 0.040566266 0.038540754 … 0.059776947 0.0479051;;; -0.04247928 -0.06894571 … -0.05145997 -0.016834104; 0.019185383 -0.08234194 … 0.05092163 0.008875125; -0.07353576 0.074589066 … 0.081603125 0.031398084;;; … ;;; 0.034794867 -0.07010961 … 0.066057324 -0.019757748; -0.05016908 -0.032356273 … -0.059473343 -0.07108714; 0.023719719 0.05327607 … -0.014091904 0.047204755;;; -0.0049255756 -0.07411226 … 0.062171206 -0.015166712; -0.054878306 0.025944304 … 0.052855194 0.053747956; 0.0010440511 -0.027511029 … -0.02738561 0.025212614;;; 0.08325391 -0.0512524 … 0.001163569 0.051447067; -0.061991658 0.069194525 … -0.078750595 0.044564106; 0.033405498 -0.021613779 … 0.010794935 0.0105769625], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.039154835 0.047788378 … 0.09329715 0.062258065; 0.08952969 0.075281456 … 0.041532308 0.045253094; -0.06499063 0.07213179 … 0.01698879 -0.06238722;;; -0.07636849 -0.05979279 … -0.05049116 0.08190359; 0.004423557 -0.059727564 … 0.09893769 0.0039881347; -0.098908976 0.04737985 … 0.020662598 -0.03147746;;; -0.09108444 0.00068989233 … 0.001745847 -0.07951402; -0.05485425 -0.085327476 … -0.0062820967 -0.05189224; 0.026254838 0.06816823 … 0.06264803 0.07471204;;; … ;;; -0.029890915 -0.08773231 … 0.016313966 0.08570423; 0.08161341 -0.023311345 … -0.020794824 0.08136777; 0.0677168 0.078495346 … -0.0635514 0.06695481;;; -0.081627786 -0.008050558 … 0.00766977 -0.086053476; 0.03377805 0.040175725 … -0.020618213 -0.08018274; 0.07114784 0.039119013 … -0.034055 -0.07646325;;; -0.059262004 -0.0895608 … 0.0060496414 0.051006462; -0.09848066 0.08133571 … -0.08892728 0.059280682; 0.022479517 0.048336618 … 0.04788034 -0.03185804], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## In case we need to use the larger size model - 4-layer with larger filters here\n",
    "\n",
    "model = Chain( Conv((31,), 2 => 64, pad = SamePad(), gelu),\n",
    "    Conv((3,), 64 => 128, pad = SamePad(), gelu),\n",
    "    Conv((3,), 128 => 128, pad = SamePad(), gelu),\n",
    "    Conv((3,), 128 => 62, pad = SamePad(), tanh))\n",
    "\n",
    "loss(x, y) = Flux.Losses.mae(model(x), y)\n",
    "\n",
    "ps = Flux.params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([[0.017529065 -0.03482061; -0.052596804 -0.02668342; … ; -0.0058259615 -0.022827208; -0.022524849 -0.016757356;;; 0.045387432 0.038000338; -0.027362332 -0.012140433; … ; -0.0057884227 -0.011639316; -0.008852256 -0.045160946;;; -0.020098468 -0.009958924; -0.046563406 -0.003004514; … ; 0.053179406 0.040546853; 0.018227626 0.018764617;;; … ;;; 0.04019753 0.0134972725; 0.014637561 0.0072620683; … ; 0.04171735 0.040326983; -0.018736787 -0.013933706;;; -0.02448177 -0.04148482; 0.00949836 -0.0022310622; … ; 0.030867737 -0.021814145; 0.021707615 0.042193636;;; -0.016755057 0.0015019148; -0.026856758 0.014124545; … ; -0.05395899 0.012642752; -0.028076036 -0.044413473], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [-0.059229374 0.005837472 … -0.004093699 0.0516604; 0.07491155 0.06331759 … 0.05054852 -0.01829766; … ; 0.07001731 -0.018477956 … 0.06945655 0.051875934; 0.034045853 -0.05173132 … -0.03779645 -0.017917512;;; -0.06706634 -0.060561087 … 0.026126934 0.030858453; 0.058742967 -0.07486858 … -0.07633792 0.032345656; … ; 0.046513364 -0.04762627 … 0.07161517 0.05867142; -0.02578977 0.049934525 … 0.049107276 -0.03029617;;; 0.042951327 0.013659904 … -0.049390864 0.05889594; -0.063613415 0.0594554 … 0.05818958 -0.07471891; … ; -0.008648879 -0.053849235 … -0.05828602 0.06441478; 0.026428852 -0.017761765 … -0.029326897 -0.00858977;;; … ;;; -0.03567428 0.0095193 … -0.07100416 -0.0548283; -0.07904866 0.049404368 … -0.07225616 0.0632443; … ; -0.07774435 0.051150102 … 0.03286685 -0.055027086; 0.062228393 0.0019475926 … 0.037461236 0.014641541;;; -0.031520624 -0.032492526 … 0.020328073 0.013668396; 0.008325323 -0.053013034 … -0.066338696 0.049036067; … ; 0.07673362 -0.01478071 … 0.04923673 -0.021947606; 0.031040842 0.0121775335 … -0.032773446 -0.03502797;;; 0.042597935 0.027145308 … -0.018496646 0.03192848; -0.050578017 -0.074660875 … -0.007716126 -0.035228632; … ; 0.044207968 0.031214578 … -0.004529178 -0.017230714; -0.04259682 -0.077498674 … 0.04967151 0.028633973], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [-0.029834377 -0.050171714 … -0.053045098 0.00551283; -0.0035675291 -0.020883389 … 0.05399238 -0.060289655; … ; -0.05840852 -0.046175916 … -0.043557324 0.01096226; 0.0666047 0.032571845 … 0.03507236 -0.06645241;;; 0.02027624 0.032862425 … 0.05345849 -0.01573111; -0.0047874916 0.0130962925 … 0.066376075 -0.0658105; … ; -0.009982211 0.040532768 … 0.054228164 -0.058109656; -0.020488486 0.059529066 … -0.037831612 0.032948703;;; 0.018392045 0.044639654 … -0.057685845 -0.06788321; -0.052026927 -0.05053496 … 0.010963745 0.043521225; … ; 0.06582188 0.054214355 … -0.04890999 0.04973676; 0.05419953 0.010252632 … -0.055026002 0.011533865;;; … ;;; -0.040038064 -0.039403416 … -0.010819969 -0.04060646; 0.04414376 -0.009228755 … 0.035821892 -0.017729854; … ; -0.0015815665 0.060049724 … -0.05107641 -0.04551943; 0.038880333 0.04992626 … -0.042902544 -0.035097256;;; -0.0350597 -0.051261973 … -0.06657939 -0.05075784; 0.061180376 0.014553695 … 0.03730622 -0.06540054; … ; 0.03621419 0.02047675 … 0.030094752 0.04601815; -0.04192908 -0.012609782 … 0.06137348 0.025207965;;; 0.03895997 -0.06608288 … 0.048644107 0.048520472; 0.0038864475 8.516736f-5 … 0.02114004 0.023315813; … ; 0.043248683 -0.044790704 … -0.027412776 0.055457357; -0.05336091 0.06521141 … -0.035090446 0.06528981], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0048057246 0.011641608 … -0.0383581 0.054197125; 0.055467468 -0.02765667 … 0.017689258 -0.004507406; … ; -0.016081491 0.046927437 … 0.024197709 0.03829847; -0.041361198 -0.067308515 … -0.009868004 -0.046993736;;; 0.034876652 0.037931986 … 0.001810339 0.015655851; 0.05429193 0.06577925 … 0.027803035 0.039815895; … ; -0.01668323 0.042057406 … -0.045042764 0.020275269; 0.060386542 -0.06778582 … -0.038614556 -0.05720889;;; 0.018315773 -0.037426867 … -0.013608529 -0.05960408; -0.035691924 -0.05008138 … 0.029200131 -0.059212327; … ; 0.013178791 -0.051926237 … -0.054173127 -0.011721356; -0.0132542215 0.0544372 … -0.023257334 -0.04902629;;; … ;;; 0.067819074 0.027637752 … -0.0017466696 0.014462128; -0.02290484 -0.06031747 … -0.064880826 -0.026197424; … ; 0.0066532646 -0.020072328 … -0.027818011 0.008967042; 0.062068205 -0.007536606 … 0.025521489 -0.03380254;;; -0.062897824 -0.0439053 … 0.04655787 -0.044997286; -0.031504065 -0.03404459 … -0.034150373 0.06760553; … ; -0.004912072 0.029218683 … 0.021414682 -0.0029813964; 0.04996862 -0.044215336 … 0.020105619 0.05738128;;; -0.0152401915 -0.023133162 … 0.054509424 -0.040736437; 0.050007634 -0.03311988 … 0.061245546 0.023403559; … ; -0.04368592 -0.06491336 … -0.009852538 0.052976206; -0.040698525 -0.068232976 … 0.042193152 0.0344386], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [-0.038832523 0.07239394 … 0.0388358 -0.050706808; 0.013228846 0.022322012 … 0.06511653 -0.057374075; … ; -0.058506638 0.047323156 … -0.071348585 -0.034659885; -0.005593072 0.05561675 … 0.0038259444 0.00897696;;; 0.03774856 0.03125253 … 0.02816054 0.06797801; -0.004594743 -0.0018835798 … -0.0686918 0.07472011; … ; 0.018653825 -0.048926633 … -0.05527991 -0.027384125; 0.03403092 0.022683853 … -0.040857974 -0.071984254;;; 0.00549319 -0.070441276 … 0.0403932 0.06193633; 0.06885837 -0.017993558 … 0.0143225575 -0.015032922; … ; -0.050196577 -0.01346535 … -0.07404795 -0.039619483; 0.0760952 -0.056458328 … 0.024957327 0.030382108;;; … ;;; 0.002284795 0.064468466 … 0.0012472913 0.0146233225; -0.027632736 0.0603104 … 0.03464553 -0.05384969; … ; 0.031110566 -0.06612276 … -0.024719175 -0.07147938; 0.051399227 -0.039562706 … 0.0640093 -0.0572245;;; 0.07074028 0.025026647 … -0.009346599 0.021445183; -0.05126165 -0.038888875 … -0.039897237 -0.015250108; … ; 0.01881125 0.013586074 … -0.010908904 0.0051393914; 0.05990729 -0.0006240862 … -0.04545124 0.011524245;;; -0.023371207 -0.030900465 … 0.04470744 0.035763185; 0.026387358 -0.059456587 … 0.01036037 -0.042593714; … ; -0.053610098 0.031089004 … 0.070391595 0.073475495; -0.0059925443 0.022772329 … 0.03265918 0.074257635], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## In case we need to use the larger size model -5-layer here\n",
    "\n",
    "model = Chain( Conv((31,), 2 => 64, pad = SamePad(), gelu),\n",
    "    Conv((5,), 64 => 128, pad = SamePad(), gelu),\n",
    "    Conv((5,), 128 => 128, pad = SamePad(), gelu),\n",
    "    Conv((5,), 128 => 128, pad = SamePad(), gelu),\n",
    "    Conv((5,), 128 => 62, pad = SamePad(), tanh))\n",
    "\n",
    "loss(x, y) = Flux.Losses.mae(model(x), y)\n",
    "\n",
    "ps = Flux.params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We could also load the trained model parameters.\n",
    "#@load \"spacing_phys_norm_outputs/CNN_DLR_60EPOCHS_16X1X_VL_GEOS_JAN_2019_NASA_GMAO.bson_MODEL\" model\n",
    "@load \"Train_outputs/08x16t/CNN_DLR_43EPOCHS_08X16X_VL_GEOS_JAN_2019_NASA_GMAO.bson_MODEL\" model\n",
    "#model |> gpu\n",
    "ps = Flux.params(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Push the data in the format neural net could understand. \n",
    "Data = []\n",
    "#Data2 = []\n",
    "#Data3 = []\n",
    "#Data4 = []\n",
    "#Data5 = []\n",
    "#Data6 = []\n",
    "#Data7 = []\n",
    "\n",
    "for i in 1:nstep-1\n",
    "    push!(Data, (hcat(input_NN_integrate[:,:,:,i], u_NN_integrate[:,:,:,i]), target_NN_integrate[:,:,:,i]))\n",
    "    #push!(Data2, (hcat(input2_NN_integrate[:,:,:,i], u2_NN_integrate[:,:,:,i]), target2_NN_integrate[:,:,:,i]))\n",
    "    #push!(Data3, (hcat(input3_NN_integrate[:,:,:,i], u3_NN_integrate[:,:,:,i]), target3_NN_integrate[:,:,:,i]))\n",
    "    #push!(Data4, (hcat(input4_NN_integrate[:,:,:,i], u4_NN_integrate[:,:,:,i]), target4_NN_integrate[:,:,:,i]))\n",
    "    #push!(Data5, (hcat(input5_NN_integrate[:,:,:,i], u5_NN_integrate[:,:,:,i]), target5_NN_integrate[:,:,:,i]))\n",
    "    #push!(Data6, (hcat(input6_NN_integrate[:,:,:,i], u6_NN_integrate[:,:,:,i]), target6_NN_integrate[:,:,:,i]))\n",
    "    #push!(Data7, (hcat(input7_NN_integrate[:,:,:,i], u7_NN_integrate[:,:,:,i]), target7_NN_integrate[:,:,:,i]))\n",
    "end\n",
    "\n",
    "#Data |> gpu;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_custom_train! (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training function for 10 time steps training\n",
    "\n",
    "function my_custom_train!(ps, data, opt, dx, dt)\n",
    "    \n",
    "    local len = length(data)\n",
    "    \n",
    "    xdim = size(data[1][1],1);\n",
    "    \n",
    "    for i in 1:len\n",
    "        gs = gradient(ps) do\n",
    "            if  0 < i < len-8\n",
    "                A = hcat([one_step_integrate(data[i][1][:,1,:], data[i][1][:,2,:], model, xdim, dx, dt)])\n",
    "                for j in 1:9\n",
    "                    x = A[j]\n",
    "                    y = data[i+j][1][:,2]\n",
    "                    A = hcat(A, [one_step_integrate(hcat(x, y)[:,1,:], hcat(x, y)[:,2,:], model, xdim, dx, dt)])\n",
    "                end\n",
    "                B = hcat([data[i+k-1][2] for k in 1:10])\n",
    "                training_loss = mean(Flux.Losses.mae.(A, B))\n",
    "            elseif len-8 ≤ i < len\n",
    "                n = len - i\n",
    "                A = hcat([one_step_integrate(data[i][1][:,1,:], data[i][1][:,2,:], model, xdim, dx, dt)])\n",
    "                for j in 1:n\n",
    "                    x = A[j]\n",
    "                    y = data[i+j][1][:,2]\n",
    "                    A = hcat(A, [one_step_integrate(hcat(x, y)[:,1,:], hcat(x, y)[:,2,:], model, xdim, dx, dt)])\n",
    "                end\n",
    "                B = hcat([data[i+k][2] for k in 0:n])\n",
    "                training_loss = mean(Flux.Losses.mae.(A, B))\n",
    "            elseif i == len\n",
    "                A = hcat([one_step_integrate(data[i][1][:,1,:], data[i][1][:,2,:], model, xdim, dx, dt)])\n",
    "                B = data[i][2]\n",
    "                training_loss = mean(Flux.Losses.mae.(A, B))\n",
    "            end\n",
    "            return training_loss\n",
    "        end\n",
    "        Flux.update!(opt, ps, gs)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "TaskFailedException\n\n\u001b[91m    nested task error: \u001b[39mInterruptException:",
     "output_type": "error",
     "traceback": [
      "TaskFailedException\n\n\u001b[91m    nested task error: \u001b[39mInterruptException:",
      "",
      "Stacktrace:",
      "  [1] sync_end(c::Channel{Any})",
      "    @ Base ./task.jl:436",
      "  [2] macro expansion",
      "    @ ./task.jl:455 [inlined]",
      "  [3] ∇conv_filter!(out::Array{Float32, 5}, in1::Array{Float32, 5}, in2::Array{Float32, 5}, cdims::DenseConvDims{3, 3, 3, 6, 3}; kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ NNlib /projects/ctessum/manhop2/software_installs/julia/bin/.julia/packages/NNlib/c0XLe/src/conv.jl:245",
      "  [4] ∇conv_filter!(out::Array{Float32, 5}, in1::Array{Float32, 5}, in2::Array{Float32, 5}, cdims::DenseConvDims{3, 3, 3, 6, 3})",
      "    @ NNlib /projects/ctessum/manhop2/software_installs/julia/bin/.julia/packages/NNlib/c0XLe/src/conv.jl:232",
      "  [5] ∇conv_filter!(y::Array{Float32, 3}, x::Array{Float32, 3}, w::Array{Float32, 3}, cdims::DenseConvDims{1, 1, 1, 2, 1}; kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ NNlib /projects/ctessum/manhop2/software_installs/julia/bin/.julia/packages/NNlib/c0XLe/src/conv.jl:145",
      "  [6] ∇conv_filter!",
      "    @ /projects/ctessum/manhop2/software_installs/julia/bin/.julia/packages/NNlib/c0XLe/src/conv.jl:140 [inlined]",
      "  [7] #∇conv_filter#200",
      "    @ /projects/ctessum/manhop2/software_installs/julia/bin/.julia/packages/NNlib/c0XLe/src/conv.jl:112 [inlined]",
      "  [8] ∇conv_filter",
      "    @ /projects/ctessum/manhop2/software_installs/julia/bin/.julia/packages/NNlib/c0XLe/src/conv.jl:107 [inlined]",
      "  [9] #314",
      "    @ /projects/ctessum/manhop2/software_installs/julia/bin/.julia/packages/NNlib/c0XLe/src/conv.jl:309 [inlined]",
      " [10] unthunk",
      "    @ /projects/ctessum/manhop2/software_installs/julia/bin/.julia/packages/ChainRulesCore/C73ay/src/tangent_types/thunks.jl:204 [inlined]",
      " [11] wrap_chainrules_output",
      "    @ /projects/ctessum/manhop2/software_installs/julia/bin/.julia/packages/Zygote/SmJK6/src/compiler/chainrules.jl:105 [inlined]",
      " [12] map",
      "    @ ./tuple.jl:223 [inlined]",
      " [13] map",
      "    @ ./tuple.jl:224 [inlined]",
      " [14] wrap_chainrules_output",
      "    @ /projects/ctessum/manhop2/software_installs/julia/bin/.julia/packages/Zygote/SmJK6/src/compiler/chainrules.jl:106 [inlined]",
      " [15] ZBack",
      "    @ /projects/ctessum/manhop2/software_installs/julia/bin/.julia/packages/Zygote/SmJK6/src/compiler/chainrules.jl:206 [inlined]",
      " [16] Pullback",
      "    @ /projects/ctessum/manhop2/software_installs/julia/bin/.julia/packages/Flux/FKl3M/src/layers/conv.jl:200 [inlined]",
      " [17] macro expansion",
      "    @ /projects/ctessum/manhop2/software_installs/julia/bin/.julia/packages/Flux/FKl3M/src/layers/basic.jl:53 [inlined]",
      " [18] Pullback",
      "    @ /projects/ctessum/manhop2/software_installs/julia/bin/.julia/packages/Flux/FKl3M/src/layers/basic.jl:53 [inlined]",
      " [19] (::typeof(∂(_applychain)))(Δ::Array{Float32, 3})",
      "    @ Zygote /projects/ctessum/manhop2/software_installs/julia/bin/.julia/packages/Zygote/SmJK6/src/compiler/interface2.jl:0",
      " [20] Pullback",
      "    @ /projects/ctessum/manhop2/software_installs/julia/bin/.julia/packages/Flux/FKl3M/src/layers/basic.jl:51 [inlined]",
      " [21] (::typeof(∂(λ)))(Δ::Array{Float32, 3})",
      "    @ Zygote /projects/ctessum/manhop2/software_installs/julia/bin/.julia/packages/Zygote/SmJK6/src/compiler/interface2.jl:0",
      " [22] Pullback",
      "    @ ./In[5]:8 [inlined]",
      " [23] (::typeof(∂(one_step_integrate)))(Δ::Matrix{Float32})",
      "    @ Zygote /projects/ctessum/manhop2/software_installs/julia/bin/.julia/packages/Zygote/SmJK6/src/compiler/interface2.jl:0",
      " [24] Pullback",
      "    @ ./In[9]:14 [inlined]",
      " [25] (::typeof(∂(λ)))(Δ::Float32)",
      "    @ Zygote /projects/ctessum/manhop2/software_installs/julia/bin/.julia/packages/Zygote/SmJK6/src/compiler/interface2.jl:0",
      " [26] (::Zygote.var\"#99#100\"{Zygote.Params{Zygote.Buffer{Any, Vector{Any}}}, typeof(∂(λ)), Zygote.Context{true}})(Δ::Float32)",
      "    @ Zygote /projects/ctessum/manhop2/software_installs/julia/bin/.julia/packages/Zygote/SmJK6/src/compiler/interface.jl:389",
      " [27] gradient(f::Function, args::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}})",
      "    @ Zygote /projects/ctessum/manhop2/software_installs/julia/bin/.julia/packages/Zygote/SmJK6/src/compiler/interface.jl:97",
      " [28] my_custom_train!(ps::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}}, data::Vector{Any}, opt::Adam, dx::Float32, dt::Int64)",
      "    @ Main ./In[9]:8",
      " [29] macro expansion",
      "    @ ./In[15]:7 [inlined]",
      " [30] top-level scope",
      "    @ ./timing.jl:262 [inlined]",
      " [31] top-level scope",
      "    @ ./In[15]:0",
      " [32] eval",
      "    @ ./boot.jl:368 [inlined]",
      " [33] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "## Training loop. To see the progress, it printed out the number of epoch every time it's done.\n",
    "\n",
    "#@time CUDA.@sync for i in 1:200\n",
    "@time for i in 1:500\n",
    "    α = (1/(1+1*i))*0.01\n",
    "    #α = 1e-3\n",
    "    dx = Float32(27034.3*1)\n",
    "    dt = 300*64\n",
    "    my_custom_train!(ps, Data, ADAM(α, (0.9, 0.999)), dx, dt)\n",
    "    #if i % 1 == 0\n",
    "    #model |> cpu\n",
    "    A = 1e-7*pgm_ml(input_NN_integrate, u_NN_integrate, model, xdim, nstep, dx, dt);\n",
    "    writedlm( \"Train_outputs/01x64t_5layer/CNN_DLR_\"*string(i)*\"_01x64x_VL_GEOS_JAN_NASA_GMAO.csv\", A, ',')\n",
    "    @save \"Train_outputs/01x64t_5layer/CNN_DLR_\"*string(i)*\"EPOCHS_01X64X_VL_GEOS_JAN_2019_NASA_GMAO.bson_MODEL\" model\n",
    "    println(i)\n",
    "    #model |> gpu\n",
    "    #end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "680.396128 seconds (2.95 G allocations: 314.670 GiB, 9.10% gc time, 3.26% compilation time)\n"
     ]
    }
   ],
   "source": [
    "# Training loop for mulitple datasets\n",
    "#@time CUDA.@sync for i in 1:200\n",
    "@time for i in 1:500\n",
    "    α = (1/(1+1*i))*0.01\n",
    "    #α = 1e-3\n",
    "    dx = Float32(27034.3*16)\n",
    "    dx_29 = Float32(30276.7*16)\n",
    "    dx_45 = Float32(24597.8*16)\n",
    "    dy = Float32(27829.3*16)\n",
    "    dt = 300*64\n",
    "    my_custom_train!(ps, Data, ADAM(α, (0.9, 0.999)), dx, dt)\n",
    "    my_custom_train!(ps, Data2, ADAM(α, (0.9, 0.999)), dx, dt)\n",
    "    my_custom_train!(ps, Data3, ADAM(α, (0.9, 0.999)), dx, dt)\n",
    "    my_custom_train!(ps, Data4, ADAM(α, (0.9, 0.999)), dx, dt)\n",
    "    my_custom_train!(ps, Data5, ADAM(α, (0.9, 0.999)), dx_29, dt)\n",
    "    my_custom_train!(ps, Data6, ADAM(α, (0.9, 0.999)), dx_45, dt)\n",
    "    my_custom_train!(ps, Data7, ADAM(α, (0.9, 0.999)), dy, dt)\n",
    "    #model |> cpu\n",
    "    A1 = 1e-7*pgm_ml(input_NN_integrate, u_NN_integrate, model, xdim, nstep, dx, dt);\n",
    "    A2 = 1e-7*pgm_ml(input2_NN_integrate, u2_NN_integrate, model, xdim, nstep, dx, dt);\n",
    "    A3 = 1e-7*pgm_ml(input3_NN_integrate, u3_NN_integrate, model, xdim, nstep, dx, dt);\n",
    "    A4 = 1e-7*pgm_ml(input4_NN_integrate, u4_NN_integrate, model, xdim, nstep, dx, dt);\n",
    "    A5 = 1e-7*pgm_ml(input5_NN_integrate, u5_NN_integrate, model, xdim, nstep, dx_29, dt);\n",
    "    A6 = 1e-7*pgm_ml(input6_NN_integrate, u6_NN_integrate, model, xdim, nstep, dx_45, dt);\n",
    "    A7 = 1e-7*pgm_ml(input7_NN_integrate, u7_NN_integrate, model, ydim, nstep, dy, dt);\n",
    "    writedlm( \"Train_outputs/16x64t_for2D_7data/CNN_DLR_\"*string(i)*\"_16x64x_VL_GEOS_JAN_NASA_GMAO_U.csv\", A1, ',')\n",
    "    writedlm( \"Train_outputs/16x64t_for2D_7data/CNN_DLR_\"*string(i)*\"_16x64x_VL_GEOS_JAN_NASA_GMAO_U2.csv\", A2, ',')\n",
    "    writedlm( \"Train_outputs/16x64t_for2D_7data/CNN_DLR_\"*string(i)*\"_16x64x_VL_GEOS_JAN_NASA_GMAO_U3.csv\", A3, ',')\n",
    "    writedlm( \"Train_outputs/16x64t_for2D_7data/CNN_DLR_\"*string(i)*\"_16x64x_VL_GEOS_JAN_NASA_GMAO_U4.csv\", A4, ',')\n",
    "    writedlm( \"Train_outputs/16x64t_for2D_7data/CNN_DLR_\"*string(i)*\"_16x64x_VL_GEOS_JAN_NASA_GMAO_U5.csv\", A5, ',')\n",
    "    writedlm( \"Train_outputs/16x64t_for2D_7data/CNN_DLR_\"*string(i)*\"_16x64x_VL_GEOS_JAN_NASA_GMAO_U6.csv\", A6, ',')\n",
    "    writedlm( \"Train_outputs/16x64t_for2D_7data/CNN_DLR_\"*string(i)*\"_16x64x_VL_GEOS_JAN_NASA_GMAO_V.csv\", A7, ',')\n",
    "    @save \"Train_outputs/16x64t_for2D_7data/CNN_DLR_\"*string(i)*\"EPOCHS_16X64X_VL_GEOS_JAN_2019_NASA_GMAO.bson_MODEL\" model\n",
    "    println(i)\n",
    "    #model |> gpu\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.176398 seconds (127.99 k allocations: 233.680 MiB, 4.88% gc time)\n"
     ]
    }
   ],
   "source": [
    "## The way we are using the trained model to return and save the output\n",
    "\n",
    "@time A = 1e-7*pgm_ml(input_NN_integrate, u_NN_integrate, model, xdim, nstep, Float32(27034.3*1), 300*4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 28 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m180.552 ms\u001b[22m\u001b[39m … \u001b[35m207.398 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m3.99% … 3.61%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m181.851 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m3.94%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m183.684 ms\u001b[22m\u001b[39m ± \u001b[32m  5.413 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m3.89% ± 0.31%\n",
       "\n",
       "  \u001b[39m█\u001b[39m▃\u001b[39m▃\u001b[34m \u001b[39m\u001b[39m▁\u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m▇\u001b[39m\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[32m▁\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m \u001b[39m▁\n",
       "  181 ms\u001b[90m           Histogram: frequency by time\u001b[39m          207 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m233.68 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m127992\u001b[39m."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark A = 1e-7*pgm_ml(input_NN_integrate, u_NN_integrate, model, xdim, nstep, Float32(27034.3*1), 300*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.002061 seconds (11.01 k allocations: 1.474 MiB)\n"
     ]
    }
   ],
   "source": [
    "## The way we are using the trained model to return and save the output\n",
    "\n",
    "@time A = 1e-7*pgm_ml(input7_NN_integrate, u7_NN_integrate, model, ydim, nstep, Float32(27829.3*16), 300*64);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.002232 seconds (11.14 k allocations: 1.551 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14×1×45 Array{Float64, 3}:\n",
       "[:, :, 1] =\n",
       " 7.629415604139115e-39\n",
       " 4.5181448781871174e-24\n",
       " 2.3972182461307057e-15\n",
       " 3.18078207783401e-10\n",
       " 4.648909270763397e-8\n",
       " 9.357329607009887e-8\n",
       " 9.195564985275269e-8\n",
       " 9.359100461006165e-8\n",
       " 9.563956260681152e-8\n",
       " 4.6869465708732604e-8\n",
       " 3.1377868726849554e-10\n",
       " 1.6442967876173497e-15\n",
       " 3.2109284019075693e-25\n",
       " 7.931349666810871e-43\n",
       "\n",
       "[:, :, 2] =\n",
       " -6.74120662969864e-26\n",
       "  2.1719630538541422e-16\n",
       "  2.713728754315525e-11\n",
       "  6.329558044672012e-9\n",
       "  3.6867305636405946e-8\n",
       "  8.134331107139587e-8\n",
       "  8.421504497528075e-8\n",
       "  8.854617476463317e-8\n",
       "  8.610190749168396e-8\n",
       "  5.477691292762756e-8\n",
       "  1.6819772124290466e-8\n",
       "  6.171717541292309e-11\n",
       "  1.5621733906456823e-16\n",
       " -1.0266172186608011e-26\n",
       "\n",
       "[:, :, 3] =\n",
       " -1.267780498637183e-18\n",
       "  1.63604872795986e-12\n",
       "  5.016792099922895e-10\n",
       "  9.936489164829254e-9\n",
       "  3.4045732021331783e-8\n",
       "  7.15028166770935e-8\n",
       "  8.0584979057312e-8\n",
       "  8.54066014289856e-8\n",
       "  8.852548003196716e-8\n",
       "  5.473588109016418e-8\n",
       "  2.9760262370109558e-8\n",
       "  4.200602322816849e-9\n",
       "  2.2765899484511464e-11\n",
       "  2.5043919715805885e-17\n",
       "\n",
       ";;; … \n",
       "\n",
       "[:, :, 43] =\n",
       " 8.613226935267448e-10\n",
       " 1.6051402315497397e-9\n",
       " 2.2268492728471754e-9\n",
       " 2.7759991586208342e-9\n",
       " 3.1984228640794754e-9\n",
       " 2.5464000180363654e-9\n",
       " 1.7369845882058143e-9\n",
       " 3.1227607280015943e-9\n",
       " 1.1030840873718262e-8\n",
       " 2.9484999179840085e-8\n",
       " 3.614958226680755e-8\n",
       " 3.279941380023956e-8\n",
       " 3.4858539700508116e-8\n",
       " 3.9450681209564204e-8\n",
       "\n",
       "[:, :, 44] =\n",
       " 8.63184779882431e-10\n",
       " 1.5828054398298262e-9\n",
       " 2.136352844536304e-9\n",
       " 2.7292123064398764e-9\n",
       " 3.1175550073385237e-9\n",
       " 2.589395269751549e-9\n",
       " 2.057756669819355e-9\n",
       " 3.3160358667373656e-9\n",
       " 1.021982580423355e-8\n",
       " 2.4953505396842957e-8\n",
       " 2.982706725597382e-8\n",
       " 3.049705922603607e-8\n",
       " 3.486353456974029e-8\n",
       " 4.192197024822235e-8\n",
       "\n",
       "[:, :, 45] =\n",
       " 8.596901781857014e-10\n",
       " 1.5654947608709336e-9\n",
       " 2.017369121313095e-9\n",
       " 2.647361159324646e-9\n",
       " 3.0600206926465035e-9\n",
       " 2.6988647878170013e-9\n",
       " 2.3402906954288484e-9\n",
       " 3.5796407610177994e-9\n",
       " 9.545636177062988e-9\n",
       " 2.1391309797763822e-8\n",
       " 2.411365807056427e-8\n",
       " 2.7416250109672545e-8\n",
       " 3.4440591931343076e-8\n",
       " 4.584182500839233e-8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This cell is used to test the trained model again.\n",
    "vel_GEOS_Array = readdlm( \"Training_dataset/Vel_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "scalar_GEOS_Array = readdlm(\"Training_dataset/VL_GEOS_Jan_2019_NASA_GMAO_10_U_16x_64x.csv\", ',', Float32);\n",
    "xdim = size(vel_GEOS_Array, 1);\n",
    "nstep = size(vel_GEOS_Array, 2);\n",
    "\n",
    "Random.seed!(1)\n",
    "history = (scalar_GEOS_Array*Float32(1e7), vel_GEOS_Array/15);\n",
    "#history = (scalar_GEOS_Array*Float32(1e7) + 4e-5*rand(Float32, xdim, nstep), vel_GEOS_Array/15);\n",
    "\n",
    "## Call variables\n",
    "coeff_estimated = zeros(Float32, xdim,2)\n",
    "#input = coarse_grained_s_2x[:,:]\n",
    "s2_2x = convert(Array{Float64}, zeros(xdim))\n",
    "history_2x_learned = zeros(xdim, 1, nstep);\n",
    "\n",
    "input_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "u_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "target_NN_integrate = zeros(Float32, xdim, 1, 1, nstep-1)\n",
    "\n",
    "input_NN_integrate[:,1,1,:] = history[1][:,1:nstep-1]\n",
    "u_NN_integrate[:,1,1,:] = history[2][:,1:nstep-1]\n",
    "target_NN_integrate[:,1,1,:] = history[1][:,2:nstep]\n",
    "\n",
    "@load \"Train_outputs/16x64t/CNN_DLR_56EPOCHS_16X64X_VL_GEOS_JAN_2019_NASA_GMAO.bson_MODEL\" model\n",
    "ps = Flux.params(model);\n",
    "\n",
    "@time A_16x64x = 1e-7*pgm_ml(input_NN_integrate, u_NN_integrate, model, xdim, nstep, Float32(27034.3*16), 300*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 25 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m170.200 ms\u001b[22m\u001b[39m … \u001b[35m316.915 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 29.12%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m188.875 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m204.140 ms\u001b[22m\u001b[39m ± \u001b[32m 37.792 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m5.33% ± 10.17%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m█\u001b[34m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▆\u001b[39m▁\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▁\u001b[39m▄\u001b[39m█\u001b[34m▆\u001b[39m\u001b[39m▄\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[32m▁\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▆\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▆\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m \u001b[39m▁\n",
       "  170 ms\u001b[90m           Histogram: frequency by time\u001b[39m          317 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m150.83 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m837288\u001b[39m."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluating the computational time\n",
    "\n",
    "@benchmark A = 1e-7*pgm_ml(input_NN_integrate, u_NN_integrate, model, xdim, nstep, Float32(27034.3), 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip620\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip620)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip621\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip620)\" d=\"\n",
       "M267.629 1392.94 L2352.76 1392.94 L2352.76 47.2441 L267.629 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip622\">\n",
       "    <rect x=\"267\" y=\"47\" width=\"2086\" height=\"1347\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  267.629,1392.94 267.629,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  684.655,1392.94 684.655,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1101.68,1392.94 1101.68,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1518.71,1392.94 1518.71,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1935.73,1392.94 1935.73,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2352.76,1392.94 2352.76,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  267.629,1392.94 2352.76,1392.94 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  267.629,1392.94 267.629,1374.04 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  684.655,1392.94 684.655,1374.04 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1101.68,1392.94 1101.68,1374.04 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1518.71,1392.94 1518.71,1374.04 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1935.73,1392.94 1935.73,1374.04 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2352.76,1392.94 2352.76,1374.04 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip620)\" d=\"M267.629 1429.72 Q262.213 1429.72 259.47 1435.07 Q256.761 1440.38 256.761 1451.07 Q256.761 1461.73 259.47 1467.08 Q262.213 1472.39 267.629 1472.39 Q273.081 1472.39 275.789 1467.08 Q278.532 1461.73 278.532 1451.07 Q278.532 1440.38 275.789 1435.07 Q273.081 1429.72 267.629 1429.72 M267.629 1424.16 Q276.345 1424.16 280.928 1431.07 Q285.546 1437.95 285.546 1451.07 Q285.546 1464.16 280.928 1471.07 Q276.345 1477.95 267.629 1477.95 Q258.914 1477.95 254.296 1471.07 Q249.713 1464.16 249.713 1451.07 Q249.713 1437.95 254.296 1431.07 Q258.914 1424.16 267.629 1424.16 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M676.634 1471.04 L701.113 1471.04 L701.113 1476.94 L668.196 1476.94 L668.196 1471.04 Q672.189 1466.91 679.064 1459.96 Q685.974 1452.98 687.745 1450.97 Q691.113 1447.18 692.432 1444.58 Q693.787 1441.94 693.787 1439.41 Q693.787 1435.27 690.87 1432.67 Q687.988 1430.07 683.335 1430.07 Q680.037 1430.07 676.356 1431.21 Q672.71 1432.36 668.544 1434.68 L668.544 1427.6 Q672.78 1425.9 676.46 1425.03 Q680.141 1424.16 683.196 1424.16 Q691.252 1424.16 696.043 1428.19 Q700.835 1432.22 700.835 1438.95 Q700.835 1442.15 699.62 1445.03 Q698.439 1447.88 695.28 1451.77 Q694.412 1452.77 689.759 1457.6 Q685.106 1462.39 676.634 1471.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1106.19 1431.21 L1088.49 1458.88 L1106.19 1458.88 L1106.19 1431.21 M1104.35 1425.1 L1113.17 1425.1 L1113.17 1458.88 L1120.57 1458.88 L1120.57 1464.72 L1113.17 1464.72 L1113.17 1476.94 L1106.19 1476.94 L1106.19 1464.72 L1082.79 1464.72 L1082.79 1457.95 L1104.35 1425.1 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1519.31 1448.22 Q1514.59 1448.22 1511.81 1451.45 Q1509.07 1454.68 1509.07 1460.31 Q1509.07 1465.9 1511.81 1469.16 Q1514.59 1472.39 1519.31 1472.39 Q1524.04 1472.39 1526.78 1469.16 Q1529.56 1465.9 1529.56 1460.31 Q1529.56 1454.68 1526.78 1451.45 Q1524.04 1448.22 1519.31 1448.22 M1533.24 1426.25 L1533.24 1432.63 Q1530.6 1431.38 1527.89 1430.72 Q1525.22 1430.07 1522.58 1430.07 Q1515.63 1430.07 1511.95 1434.75 Q1508.31 1439.44 1507.79 1448.92 Q1509.83 1445.9 1512.92 1444.3 Q1516.01 1442.67 1519.73 1442.67 Q1527.54 1442.67 1532.06 1447.43 Q1536.6 1452.15 1536.6 1460.31 Q1536.6 1468.29 1531.88 1473.12 Q1527.16 1477.95 1519.31 1477.95 Q1510.32 1477.95 1505.56 1471.07 Q1500.81 1464.16 1500.81 1451.07 Q1500.81 1438.78 1506.64 1431.49 Q1512.47 1424.16 1522.3 1424.16 Q1524.94 1424.16 1527.61 1424.68 Q1530.32 1425.2 1533.24 1426.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1935.73 1452.32 Q1930.73 1452.32 1927.85 1455 Q1925 1457.67 1925 1462.36 Q1925 1467.04 1927.85 1469.72 Q1930.73 1472.39 1935.73 1472.39 Q1940.73 1472.39 1943.61 1469.72 Q1946.49 1467.01 1946.49 1462.36 Q1946.49 1457.67 1943.61 1455 Q1940.77 1452.32 1935.73 1452.32 M1928.72 1449.34 Q1924.2 1448.22 1921.67 1445.13 Q1919.17 1442.04 1919.17 1437.6 Q1919.17 1431.38 1923.58 1427.77 Q1928.02 1424.16 1935.73 1424.16 Q1943.47 1424.16 1947.88 1427.77 Q1952.29 1431.38 1952.29 1437.6 Q1952.29 1442.04 1949.76 1445.13 Q1947.26 1448.22 1942.78 1449.34 Q1947.85 1450.52 1950.66 1453.95 Q1953.51 1457.39 1953.51 1462.36 Q1953.51 1469.89 1948.89 1473.92 Q1944.31 1477.95 1935.73 1477.95 Q1927.15 1477.95 1922.54 1473.92 Q1917.95 1469.89 1917.95 1462.36 Q1917.95 1457.39 1920.8 1453.95 Q1923.65 1450.52 1928.72 1449.34 M1926.15 1438.26 Q1926.15 1442.29 1928.65 1444.54 Q1931.18 1446.8 1935.73 1446.8 Q1940.24 1446.8 1942.78 1444.54 Q1945.35 1442.29 1945.35 1438.26 Q1945.35 1434.23 1942.78 1431.97 Q1940.24 1429.72 1935.73 1429.72 Q1931.18 1429.72 1928.65 1431.97 Q1926.15 1434.23 1926.15 1438.26 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M2314.79 1471.04 L2326.25 1471.04 L2326.25 1431.49 L2313.78 1433.99 L2313.78 1427.6 L2326.18 1425.1 L2333.19 1425.1 L2333.19 1471.04 L2344.65 1471.04 L2344.65 1476.94 L2314.79 1476.94 L2314.79 1471.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M2373.81 1429.72 Q2368.4 1429.72 2365.66 1435.07 Q2362.95 1440.38 2362.95 1451.07 Q2362.95 1461.73 2365.66 1467.08 Q2368.4 1472.39 2373.81 1472.39 Q2379.27 1472.39 2381.97 1467.08 Q2384.72 1461.73 2384.72 1451.07 Q2384.72 1440.38 2381.97 1435.07 Q2379.27 1429.72 2373.81 1429.72 M2373.81 1424.16 Q2382.53 1424.16 2387.11 1431.07 Q2391.73 1437.95 2391.73 1451.07 Q2391.73 1464.16 2387.11 1471.07 Q2382.53 1477.95 2373.81 1477.95 Q2365.1 1477.95 2360.48 1471.07 Q2355.9 1464.16 2355.9 1451.07 Q2355.9 1437.95 2360.48 1431.07 Q2365.1 1424.16 2373.81 1424.16 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1090.65 1521.02 L1090.65 1533.91 L1106.01 1533.91 L1106.01 1539.7 L1090.65 1539.7 L1090.65 1564.33 Q1090.65 1569.88 1092.15 1571.46 Q1093.69 1573.04 1098.35 1573.04 L1106.01 1573.04 L1106.01 1579.28 L1098.35 1579.28 Q1089.72 1579.28 1086.44 1576.08 Q1083.16 1572.83 1083.16 1564.33 L1083.16 1539.7 L1077.69 1539.7 L1077.69 1533.91 L1083.16 1533.91 L1083.16 1521.02 L1090.65 1521.02 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1115.81 1533.91 L1123.26 1533.91 L1123.26 1579.28 L1115.81 1579.28 L1115.81 1533.91 M1115.81 1516.24 L1123.26 1516.24 L1123.26 1525.68 L1115.81 1525.68 L1115.81 1516.24 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1174.18 1542.62 Q1176.98 1537.59 1180.87 1535.2 Q1184.76 1532.81 1190.02 1532.81 Q1197.11 1532.81 1200.96 1537.79 Q1204.81 1542.74 1204.81 1551.89 L1204.81 1579.28 L1197.31 1579.28 L1197.31 1552.13 Q1197.31 1545.61 1195.01 1542.45 Q1192.7 1539.29 1187.96 1539.29 Q1182.16 1539.29 1178.8 1543.14 Q1175.44 1546.99 1175.44 1553.63 L1175.44 1579.28 L1167.95 1579.28 L1167.95 1552.13 Q1167.95 1545.57 1165.64 1542.45 Q1163.33 1539.29 1158.51 1539.29 Q1152.79 1539.29 1149.43 1543.18 Q1146.07 1547.03 1146.07 1553.63 L1146.07 1579.28 L1138.58 1579.28 L1138.58 1533.91 L1146.07 1533.91 L1146.07 1540.95 Q1148.62 1536.78 1152.19 1534.8 Q1155.75 1532.81 1160.65 1532.81 Q1165.6 1532.81 1169.04 1535.32 Q1172.52 1537.84 1174.18 1542.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1258.48 1554.73 L1258.48 1558.37 L1224.21 1558.37 Q1224.7 1566.07 1228.83 1570.12 Q1233 1574.13 1240.42 1574.13 Q1244.71 1574.13 1248.72 1573.08 Q1252.77 1572.02 1256.74 1569.92 L1256.74 1576.97 Q1252.73 1578.67 1248.52 1579.56 Q1244.3 1580.45 1239.97 1580.45 Q1229.11 1580.45 1222.75 1574.13 Q1216.43 1567.81 1216.43 1557.04 Q1216.43 1545.9 1222.43 1539.37 Q1228.47 1532.81 1238.67 1532.81 Q1247.83 1532.81 1253.14 1538.73 Q1258.48 1544.6 1258.48 1554.73 M1251.03 1552.54 Q1250.95 1546.42 1247.59 1542.78 Q1244.26 1539.13 1238.75 1539.13 Q1232.52 1539.13 1228.75 1542.66 Q1225.02 1546.18 1224.46 1552.58 L1251.03 1552.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1314.99 1516.32 Q1309.56 1525.64 1306.93 1534.76 Q1304.3 1543.87 1304.3 1553.23 Q1304.3 1562.59 1306.93 1571.78 Q1309.61 1580.94 1314.99 1590.21 L1308.51 1590.21 Q1302.44 1580.69 1299.4 1571.5 Q1296.4 1562.3 1296.4 1553.23 Q1296.4 1544.2 1299.4 1535.04 Q1302.39 1525.88 1308.51 1516.32 L1314.99 1516.32 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1359.31 1540.79 L1359.31 1516.24 L1366.76 1516.24 L1366.76 1579.28 L1359.31 1579.28 L1359.31 1572.47 Q1356.96 1576.52 1353.35 1578.51 Q1349.79 1580.45 1344.77 1580.45 Q1336.54 1580.45 1331.36 1573.89 Q1326.21 1567.33 1326.21 1556.63 Q1326.21 1545.94 1331.36 1539.37 Q1336.54 1532.81 1344.77 1532.81 Q1349.79 1532.81 1353.35 1534.8 Q1356.96 1536.74 1359.31 1540.79 M1333.91 1556.63 Q1333.91 1564.85 1337.27 1569.55 Q1340.68 1574.21 1346.59 1574.21 Q1352.5 1574.21 1355.91 1569.55 Q1359.31 1564.85 1359.31 1556.63 Q1359.31 1548.41 1355.91 1543.75 Q1352.5 1539.05 1346.59 1539.05 Q1340.68 1539.05 1337.27 1543.75 Q1333.91 1548.41 1333.91 1556.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1402.74 1556.47 Q1393.7 1556.47 1390.22 1558.54 Q1386.73 1560.6 1386.73 1565.58 Q1386.73 1569.55 1389.33 1571.9 Q1391.96 1574.21 1396.46 1574.21 Q1402.65 1574.21 1406.38 1569.84 Q1410.15 1565.42 1410.15 1558.13 L1410.15 1556.47 L1402.74 1556.47 M1417.6 1553.39 L1417.6 1579.28 L1410.15 1579.28 L1410.15 1572.39 Q1407.6 1576.52 1403.79 1578.51 Q1399.98 1580.45 1394.47 1580.45 Q1387.5 1580.45 1383.37 1576.56 Q1379.28 1572.63 1379.28 1566.07 Q1379.28 1558.41 1384.38 1554.52 Q1389.53 1550.64 1399.7 1550.64 L1410.15 1550.64 L1410.15 1549.91 Q1410.15 1544.76 1406.75 1541.97 Q1403.38 1539.13 1397.27 1539.13 Q1393.38 1539.13 1389.69 1540.06 Q1386.01 1540.99 1382.6 1542.86 L1382.6 1535.97 Q1386.69 1534.39 1390.54 1533.62 Q1394.39 1532.81 1398.04 1532.81 Q1407.88 1532.81 1412.74 1537.92 Q1417.6 1543.02 1417.6 1553.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1451.83 1583.49 Q1448.67 1591.59 1445.68 1594.06 Q1442.68 1596.53 1437.65 1596.53 L1431.7 1596.53 L1431.7 1590.29 L1436.07 1590.29 Q1439.15 1590.29 1440.85 1588.84 Q1442.56 1587.38 1444.62 1581.95 L1445.96 1578.55 L1427.61 1533.91 L1435.51 1533.91 L1449.69 1569.39 L1463.86 1533.91 L1471.76 1533.91 L1451.83 1583.49 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1510.98 1535.24 L1510.98 1542.29 Q1507.82 1540.67 1504.41 1539.86 Q1501.01 1539.05 1497.36 1539.05 Q1491.81 1539.05 1489.02 1540.75 Q1486.27 1542.45 1486.27 1545.86 Q1486.27 1548.45 1488.25 1549.95 Q1490.24 1551.41 1496.23 1552.74 L1498.78 1553.31 Q1506.72 1555.01 1510.04 1558.13 Q1513.41 1561.21 1513.41 1566.76 Q1513.41 1573.08 1508.38 1576.76 Q1503.4 1580.45 1494.65 1580.45 Q1491 1580.45 1487.03 1579.72 Q1483.11 1579.03 1478.73 1577.61 L1478.73 1569.92 Q1482.86 1572.07 1486.87 1573.16 Q1490.88 1574.21 1494.81 1574.21 Q1500.08 1574.21 1502.91 1572.43 Q1505.75 1570.61 1505.75 1567.33 Q1505.75 1564.29 1503.68 1562.67 Q1501.66 1561.05 1494.73 1559.55 L1492.14 1558.94 Q1485.21 1557.48 1482.13 1554.48 Q1479.05 1551.45 1479.05 1546.18 Q1479.05 1539.78 1483.59 1536.3 Q1488.13 1532.81 1496.47 1532.81 Q1500.61 1532.81 1504.25 1533.42 Q1507.9 1534.03 1510.98 1535.24 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1524.1 1516.32 L1530.58 1516.32 Q1536.66 1525.88 1539.66 1535.04 Q1542.69 1544.2 1542.69 1553.23 Q1542.69 1562.3 1539.66 1571.5 Q1536.66 1580.69 1530.58 1590.21 L1524.1 1590.21 Q1529.49 1580.94 1532.12 1571.78 Q1534.79 1562.59 1534.79 1553.23 Q1534.79 1543.87 1532.12 1534.76 Q1529.49 1525.64 1524.1 1516.32 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  267.629,1392.94 2352.76,1392.94 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  267.629,1123.8 2352.76,1123.8 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  267.629,854.662 2352.76,854.662 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  267.629,585.522 2352.76,585.522 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  267.629,316.383 2352.76,316.383 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  267.629,47.2441 2352.76,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  267.629,1392.94 267.629,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  267.629,1392.94 286.527,1392.94 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  267.629,1123.8 286.527,1123.8 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  267.629,854.662 286.527,854.662 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  267.629,585.522 286.527,585.522 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  267.629,316.383 286.527,316.383 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  267.629,47.2441 286.527,47.2441 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip620)\" d=\"M145.866 1371.64 Q140.449 1371.64 137.706 1376.99 Q134.998 1382.3 134.998 1392.99 Q134.998 1403.65 137.706 1409 Q140.449 1414.31 145.866 1414.31 Q151.317 1414.31 154.026 1409 Q156.769 1403.65 156.769 1392.99 Q156.769 1382.3 154.026 1376.99 Q151.317 1371.64 145.866 1371.64 M145.866 1366.08 Q154.581 1366.08 159.164 1372.99 Q163.782 1379.87 163.782 1392.99 Q163.782 1406.08 159.164 1412.99 Q154.581 1419.87 145.866 1419.87 Q137.151 1419.87 132.533 1412.99 Q127.949 1406.08 127.949 1392.99 Q127.949 1379.87 132.533 1372.99 Q137.151 1366.08 145.866 1366.08 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M176.109 1410.04 L183.435 1410.04 L183.435 1418.86 L176.109 1418.86 L176.109 1410.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M213.713 1371.64 Q208.296 1371.64 205.553 1376.99 Q202.845 1382.3 202.845 1392.99 Q202.845 1403.65 205.553 1409 Q208.296 1414.31 213.713 1414.31 Q219.164 1414.31 221.872 1409 Q224.615 1403.65 224.615 1392.99 Q224.615 1382.3 221.872 1376.99 Q219.164 1371.64 213.713 1371.64 M213.713 1366.08 Q222.428 1366.08 227.011 1372.99 Q231.629 1379.87 231.629 1392.99 Q231.629 1406.08 227.011 1412.99 Q222.428 1419.87 213.713 1419.87 Q204.998 1419.87 200.379 1412.99 Q195.796 1406.08 195.796 1392.99 Q195.796 1379.87 200.379 1372.99 Q204.998 1366.08 213.713 1366.08 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M148.262 1102.5 Q142.845 1102.5 140.102 1107.85 Q137.394 1113.16 137.394 1123.85 Q137.394 1134.51 140.102 1139.86 Q142.845 1145.17 148.262 1145.17 Q153.713 1145.17 156.421 1139.86 Q159.164 1134.51 159.164 1123.85 Q159.164 1113.16 156.421 1107.85 Q153.713 1102.5 148.262 1102.5 M148.262 1096.94 Q156.977 1096.94 161.56 1103.85 Q166.178 1110.73 166.178 1123.85 Q166.178 1136.94 161.56 1143.85 Q156.977 1150.73 148.262 1150.73 Q139.546 1150.73 134.928 1143.85 Q130.345 1136.94 130.345 1123.85 Q130.345 1110.73 134.928 1103.85 Q139.546 1096.94 148.262 1096.94 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M178.505 1140.9 L185.831 1140.9 L185.831 1149.72 L178.505 1149.72 L178.505 1140.9 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M207.15 1143.82 L231.629 1143.82 L231.629 1149.72 L198.713 1149.72 L198.713 1143.82 Q202.706 1139.69 209.581 1132.74 Q216.491 1125.76 218.261 1123.75 Q221.629 1119.96 222.949 1117.36 Q224.303 1114.72 224.303 1112.19 Q224.303 1108.05 221.386 1105.45 Q218.504 1102.85 213.852 1102.85 Q210.553 1102.85 206.873 1103.99 Q203.227 1105.14 199.06 1107.46 L199.06 1100.38 Q203.296 1098.68 206.977 1097.81 Q210.657 1096.94 213.713 1096.94 Q221.768 1096.94 226.56 1100.97 Q231.352 1105 231.352 1111.73 Q231.352 1114.93 230.136 1117.81 Q228.956 1120.66 225.796 1124.55 Q224.928 1125.55 220.275 1130.38 Q215.622 1135.17 207.15 1143.82 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M145.137 833.36 Q139.72 833.36 136.977 838.707 Q134.269 844.019 134.269 854.714 Q134.269 865.373 136.977 870.72 Q139.72 876.033 145.137 876.033 Q150.588 876.033 153.296 870.72 Q156.039 865.373 156.039 854.714 Q156.039 844.019 153.296 838.707 Q150.588 833.36 145.137 833.36 M145.137 827.804 Q153.852 827.804 158.435 834.714 Q163.053 841.589 163.053 854.714 Q163.053 867.804 158.435 874.714 Q153.852 881.588 145.137 881.588 Q136.421 881.588 131.803 874.714 Q127.22 867.804 127.22 854.714 Q127.22 841.589 131.803 834.714 Q136.421 827.804 145.137 827.804 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M175.38 871.762 L182.706 871.762 L182.706 880.582 L175.38 880.582 L175.38 871.762 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M217.254 834.853 L199.546 862.526 L217.254 862.526 L217.254 834.853 M215.414 828.742 L224.234 828.742 L224.234 862.526 L231.629 862.526 L231.629 868.359 L224.234 868.359 L224.234 880.582 L217.254 880.582 L217.254 868.359 L193.852 868.359 L193.852 861.589 L215.414 828.742 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M145.623 564.22 Q140.206 564.22 137.463 569.568 Q134.755 574.88 134.755 585.574 Q134.755 596.234 137.463 601.581 Q140.206 606.894 145.623 606.894 Q151.074 606.894 153.783 601.581 Q156.526 596.234 156.526 585.574 Q156.526 574.88 153.783 569.568 Q151.074 564.22 145.623 564.22 M145.623 558.665 Q154.338 558.665 158.921 565.575 Q163.539 572.45 163.539 585.574 Q163.539 598.665 158.921 605.574 Q154.338 612.449 145.623 612.449 Q136.908 612.449 132.29 605.574 Q127.706 598.665 127.706 585.574 Q127.706 572.45 132.29 565.575 Q136.908 558.665 145.623 558.665 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M175.866 602.623 L183.192 602.623 L183.192 611.442 L175.866 611.442 L175.866 602.623 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M214.338 582.727 Q209.616 582.727 206.838 585.956 Q204.095 589.186 204.095 594.811 Q204.095 600.401 206.838 603.665 Q209.616 606.894 214.338 606.894 Q219.06 606.894 221.803 603.665 Q224.581 600.401 224.581 594.811 Q224.581 589.186 221.803 585.956 Q219.06 582.727 214.338 582.727 M228.261 560.748 L228.261 567.137 Q225.622 565.887 222.914 565.227 Q220.24 564.568 217.602 564.568 Q210.657 564.568 206.977 569.255 Q203.331 573.943 202.81 583.422 Q204.859 580.401 207.949 578.804 Q211.039 577.172 214.754 577.172 Q222.567 577.172 227.081 581.929 Q231.629 586.651 231.629 594.811 Q231.629 602.797 226.907 607.623 Q222.185 612.449 214.338 612.449 Q205.345 612.449 200.588 605.574 Q195.831 598.665 195.831 585.574 Q195.831 573.283 201.664 565.991 Q207.498 558.665 217.324 558.665 Q219.963 558.665 222.636 559.186 Q225.345 559.707 228.261 560.748 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M146.005 295.081 Q140.588 295.081 137.845 300.428 Q135.137 305.741 135.137 316.435 Q135.137 327.095 137.845 332.442 Q140.588 337.755 146.005 337.755 Q151.456 337.755 154.164 332.442 Q156.907 327.095 156.907 316.435 Q156.907 305.741 154.164 300.428 Q151.456 295.081 146.005 295.081 M146.005 289.526 Q154.72 289.526 159.303 296.435 Q163.921 303.31 163.921 316.435 Q163.921 329.526 159.303 336.435 Q154.72 343.31 146.005 343.31 Q137.29 343.31 132.672 336.435 Q128.088 329.526 128.088 316.435 Q128.088 303.31 132.672 296.435 Q137.29 289.526 146.005 289.526 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M176.248 333.484 L183.574 333.484 L183.574 342.303 L176.248 342.303 L176.248 333.484 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M213.852 317.685 Q208.852 317.685 205.97 320.359 Q203.123 323.033 203.123 327.72 Q203.123 332.407 205.97 335.081 Q208.852 337.755 213.852 337.755 Q218.852 337.755 221.734 335.081 Q224.615 332.373 224.615 327.72 Q224.615 323.033 221.734 320.359 Q218.886 317.685 213.852 317.685 M206.838 314.699 Q202.324 313.588 199.789 310.498 Q197.289 307.408 197.289 302.963 Q197.289 296.748 201.699 293.137 Q206.143 289.526 213.852 289.526 Q221.595 289.526 226.004 293.137 Q230.414 296.748 230.414 302.963 Q230.414 307.408 227.879 310.498 Q225.379 313.588 220.9 314.699 Q225.97 315.88 228.782 319.317 Q231.629 322.755 231.629 327.72 Q231.629 335.255 227.011 339.282 Q222.428 343.31 213.852 343.31 Q205.275 343.31 200.657 339.282 Q196.074 335.255 196.074 327.72 Q196.074 322.755 198.921 319.317 Q201.768 315.88 206.838 314.699 M204.268 303.623 Q204.268 307.651 206.768 309.908 Q209.303 312.165 213.852 312.165 Q218.366 312.165 220.9 309.908 Q223.47 307.651 223.47 303.623 Q223.47 299.595 220.9 297.338 Q218.366 295.081 213.852 295.081 Q209.303 295.081 206.768 297.338 Q204.268 299.595 204.268 303.623 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M132.081 67.2613 L143.54 67.2613 L143.54 27.7129 L131.074 30.2129 L131.074 23.8241 L143.47 21.3241 L150.484 21.3241 L150.484 67.2613 L161.942 67.2613 L161.942 73.1641 L132.081 73.1641 L132.081 67.2613 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M176.109 64.3447 L183.435 64.3447 L183.435 73.1641 L176.109 73.1641 L176.109 64.3447 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M213.713 25.9421 Q208.296 25.9421 205.553 31.2893 Q202.845 36.6018 202.845 47.2962 Q202.845 57.9558 205.553 63.303 Q208.296 68.6155 213.713 68.6155 Q219.164 68.6155 221.872 63.303 Q224.615 57.9558 224.615 47.2962 Q224.615 36.6018 221.872 31.2893 Q219.164 25.9421 213.713 25.9421 M213.713 20.3866 Q222.428 20.3866 227.011 27.2963 Q231.629 34.1712 231.629 47.2962 Q231.629 60.3864 227.011 67.2961 Q222.428 74.171 213.713 74.171 Q204.998 74.171 200.379 67.2961 Q195.796 60.3864 195.796 47.2962 Q195.796 34.1712 200.379 27.2963 Q204.998 20.3866 213.713 20.3866 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M34.6736 720.821 Q33.9444 722.077 33.6204 723.576 Q33.2558 725.034 33.2558 726.816 Q33.2558 733.136 37.3877 736.539 Q41.4791 739.901 49.1758 739.901 L73.0762 739.901 L73.0762 747.395 L27.706 747.395 L27.706 739.901 L34.7546 739.901 Q30.6227 737.551 28.6377 733.784 Q26.6123 730.017 26.6123 724.629 Q26.6123 723.859 26.7338 722.928 Q26.8148 721.996 27.0174 720.862 L34.6736 720.821 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M41.3981 709.965 L41.3981 692.789 L46.0161 692.789 L46.0161 717.013 L41.5601 717.013 Q40.3043 715.636 38.0358 713.084 Q25.6806 699.149 21.8727 699.149 Q19.1991 699.149 17.5788 701.255 Q15.9179 703.362 15.9179 706.805 Q15.9179 708.911 16.6471 711.383 Q17.3357 713.854 18.7535 716.77 L13.7709 716.77 Q12.6367 713.651 12.0695 710.977 Q11.5024 708.263 11.5024 705.954 Q11.5024 700.081 14.176 696.556 Q16.8496 693.032 21.2246 693.032 Q26.8553 693.032 38.3599 706.44 Q40.3043 708.709 41.3981 709.965 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip622)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  279.213,47.2444 290.797,48.1809 302.381,48.2291 313.965,48.8589 325.55,49.3247 337.134,49.0365 348.718,49.7774 360.302,50.1187 371.886,51.0755 383.47,51.4752 \n",
       "  395.054,52.2971 406.638,52.6714 418.222,53.144 429.806,53.2301 441.39,52.3461 452.974,52.1204 464.558,51.6909 476.142,52.226 487.726,54.2189 499.31,55.9313 \n",
       "  510.894,57.9421 522.478,60.1213 534.062,60.9989 545.646,60.3306 557.23,60.1418 568.814,63.2564 580.398,66.4588 591.982,65.7671 603.566,64.2214 615.15,60.6145 \n",
       "  626.734,57.7703 638.318,56.2748 649.903,55.3271 661.487,55.1812 673.071,54.2061 684.655,54.1142 696.239,52.6106 707.823,53.0469 719.407,53.5026 730.991,54.0552 \n",
       "  742.575,54.2206 754.159,54.1257 765.743,54.6323 777.327,54.4386 788.911,54.5423 800.495,54.2289 812.079,54.7086 823.663,54.5573 835.247,55.1226 846.831,55.5675 \n",
       "  858.415,55.019 869.999,55.2341 881.583,55.3098 893.167,55.7301 904.751,55.3636 916.335,56.8599 927.919,56.7683 939.503,57.3248 951.087,59.175 962.672,61.2027 \n",
       "  974.256,63.0914 985.84,63.1781 997.424,64.3576 1009.01,64.1537 1020.59,64.7848 1032.18,63.5749 1043.76,62.4344 1055.34,61.8855 1066.93,61.5461 1078.51,61.8439 \n",
       "  1090.1,64.2399 1101.68,67.6439 1113.26,65.7301 1124.85,67.6353 1136.43,62.9626 1148.02,60.418 1159.6,60.2134 1171.18,59.3428 1182.77,59.5323 1194.35,59.5358 \n",
       "  1205.94,59.4977 1217.52,60.7425 1229.1,61.7965 1240.69,60.6388 1252.27,59.1091 1263.86,58.6405 1275.44,57.3696 1287.02,57.7269 1298.61,56.9091 1310.19,56.7509 \n",
       "  1321.78,58.1953 1333.36,58.6457 1344.94,57.9426 1356.53,57.6151 1368.11,56.8112 1379.7,55.5063 1391.28,55.4201 1402.86,55.8272 1414.45,56.6839 1426.03,57.3049 \n",
       "  1437.62,58.6378 1449.2,58.9767 1460.79,59.8466 1472.37,61.4922 1483.95,64.4451 1495.54,67.3566 1507.12,73.5815 1518.71,78.0457 1530.29,81.9616 1541.87,88.642 \n",
       "  1553.46,91.4009 1565.04,92.1469 1576.63,92.6876 1588.21,90.5072 1599.79,89.5206 1611.38,85.0532 1622.96,83.8083 1634.55,81.7826 1646.13,80.7831 1657.71,82.2965 \n",
       "  1669.3,89.9566 1680.88,83.3074 1692.47,70.9699 1704.05,69.5829 1715.63,67.234 1727.22,66.54 1738.8,64.018 1750.39,63.7237 1761.97,62.8045 1773.55,61.4249 \n",
       "  1785.14,60.5516 1796.72,60.4774 1808.31,60.0092 1819.89,58.9985 1831.47,58.2546 1843.06,59.4023 1854.64,60.2876 1866.23,60.8403 1877.81,64.0434 1889.39,65.6006 \n",
       "  1900.98,67.5511 1912.56,66.2982 1924.15,62.9944 1935.73,67.1689 1947.31,65.6351 1958.9,69.461 1970.48,65.1139 1982.07,62.647 1993.65,63.4982 2005.23,63.9796 \n",
       "  2016.82,67.1367 2028.4,66.2915 2039.99,67.7336 2051.57,68.0802 2063.15,70.3589 2074.74,72.1485 2086.32,76.3969 2097.91,78.1068 2109.49,75.1787 2121.08,75.7897 \n",
       "  2132.66,77.6011 2144.24,80.767 2155.83,83.6761 2167.41,86.8656 2179,85.1477 2190.58,79.9406 2202.16,77.2276 2213.75,75.697 2225.33,73.6729 2236.92,72.0421 \n",
       "  2248.5,70.9909 2260.08,69.969 2271.67,69.7513 2283.25,72.2126 2294.84,74.2174 2306.42,74.0481 2318,74.3999 2329.59,76.8835 2341.17,90.9294 2352.76,108.674 \n",
       "  \n",
       "  \"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Plot the r2 throughout the timesteps\n",
    "\n",
    "begin\n",
    "    r2 = zeros(Float32, nstep)\n",
    "    for i in 1:nstep\n",
    "        r2[i] = Statistics.cor(A[:,1,i], scalar1_GEOS_Array[:,i])^2\n",
    "    end\n",
    "    plot(Int(2880/nstep)*10/2880:Int(2880/nstep)*10/2880:10, r2, width=2, label=false, xlabel=\"time (days)\", ylabel=\"r²\", xlabelfontsize=14, ylabelfontsize=14,\n",
    "        xtickfontsize=12, ytickfontsize=12, xlims=(0,10), ylims=(0.0,1.0))\n",
    "end\n",
    "savefig(\"r2_8x16t.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.9929091138891484, 0.9542317626641522, 0.961742977718)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Calculating correlations in different time steps\n",
    "\n",
    "Statistics.cor(A[:,1,1], 1e-7*input_NN_integrate[:,1,1,1])^2, Statistics.cor(A[:,1,960], 1e-7*input_NN_integrate[:,1,1,960])^2, Statistics.cor(A[:,1,1920], 1e-7*input_NN_integrate[:,1,1,1920])^2, Statistics.cor(A[:,1,2879], 1e-7*input_NN_integrate[:,1,1,2879])^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip930\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip930)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip931\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip930)\" d=\"\n",
       "M347.848 1343.4 L2329.13 1343.4 L2329.13 173.014 L347.848 173.014  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip932\">\n",
       "    <rect x=\"347\" y=\"173\" width=\"1982\" height=\"1171\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  646.218,1343.4 646.218,173.014 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1200.04,1343.4 1200.04,173.014 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1753.85,1343.4 1753.85,173.014 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2307.67,1343.4 2307.67,173.014 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  347.848,1343.4 2329.13,1343.4 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  646.218,1343.4 646.218,1324.5 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1200.04,1343.4 1200.04,1324.5 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1753.85,1343.4 1753.85,1324.5 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2307.67,1343.4 2307.67,1324.5 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip930)\" d=\"M539.436 1408.75 L591.369 1408.75 L591.369 1415.63 L539.436 1415.63 L539.436 1408.75 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M610.449 1431.31 L623.816 1431.31 L623.816 1385.17 L609.274 1388.09 L609.274 1380.63 L623.735 1377.72 L631.918 1377.72 L631.918 1431.31 L645.286 1431.31 L645.286 1438.2 L610.449 1438.2 L610.449 1431.31 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M668.863 1431.31 L697.421 1431.31 L697.421 1438.2 L659.019 1438.2 L659.019 1431.31 Q663.677 1426.49 671.698 1418.39 Q679.759 1410.25 681.825 1407.9 Q685.755 1403.48 687.294 1400.44 Q688.874 1397.36 688.874 1394.41 Q688.874 1389.59 685.471 1386.55 Q682.109 1383.51 676.681 1383.51 Q672.832 1383.51 668.538 1384.85 Q664.285 1386.18 659.424 1388.9 L659.424 1380.63 Q664.366 1378.65 668.66 1377.64 Q672.954 1376.62 676.519 1376.62 Q685.917 1376.62 691.507 1381.32 Q697.097 1386.02 697.097 1393.88 Q697.097 1397.61 695.68 1400.97 Q694.302 1404.29 690.616 1408.83 Q689.603 1410 684.175 1415.63 Q678.747 1421.22 668.863 1431.31 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M732.097 1383.11 Q725.778 1383.11 722.578 1389.34 Q719.418 1395.54 719.418 1408.02 Q719.418 1420.45 722.578 1426.69 Q725.778 1432.89 732.097 1432.89 Q738.457 1432.89 741.617 1426.69 Q744.817 1420.45 744.817 1408.02 Q744.817 1395.54 741.617 1389.34 Q738.457 1383.11 732.097 1383.11 M732.097 1376.62 Q742.265 1376.62 747.612 1384.69 Q753 1392.71 753 1408.02 Q753 1423.29 747.612 1431.35 Q742.265 1439.37 732.097 1439.37 Q721.929 1439.37 716.542 1431.35 Q711.194 1423.29 711.194 1408.02 Q711.194 1392.71 716.542 1384.69 Q721.929 1376.62 732.097 1376.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1093.25 1408.75 L1145.19 1408.75 L1145.19 1415.63 L1093.25 1415.63 L1093.25 1408.75 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1164.27 1431.31 L1177.63 1431.31 L1177.63 1385.17 L1163.09 1388.09 L1163.09 1380.63 L1177.55 1377.72 L1185.74 1377.72 L1185.74 1431.31 L1199.1 1431.31 L1199.1 1438.2 L1164.27 1438.2 L1164.27 1431.31 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1233.13 1383.11 Q1226.81 1383.11 1223.61 1389.34 Q1220.45 1395.54 1220.45 1408.02 Q1220.45 1420.45 1223.61 1426.69 Q1226.81 1432.89 1233.13 1432.89 Q1239.49 1432.89 1242.65 1426.69 Q1245.85 1420.45 1245.85 1408.02 Q1245.85 1395.54 1242.65 1389.34 Q1239.49 1383.11 1233.13 1383.11 M1233.13 1376.62 Q1243.3 1376.62 1248.65 1384.69 Q1254.04 1392.71 1254.04 1408.02 Q1254.04 1423.29 1248.65 1431.35 Q1243.3 1439.37 1233.13 1439.37 Q1222.96 1439.37 1217.58 1431.35 Q1212.23 1423.29 1212.23 1408.02 Q1212.23 1392.71 1217.58 1384.69 Q1222.96 1376.62 1233.13 1376.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1285.92 1383.11 Q1279.6 1383.11 1276.4 1389.34 Q1273.24 1395.54 1273.24 1408.02 Q1273.24 1420.45 1276.4 1426.69 Q1279.6 1432.89 1285.92 1432.89 Q1292.28 1432.89 1295.44 1426.69 Q1298.64 1420.45 1298.64 1408.02 Q1298.64 1395.54 1295.44 1389.34 Q1292.28 1383.11 1285.92 1383.11 M1285.92 1376.62 Q1296.08 1376.62 1301.43 1384.69 Q1306.82 1392.71 1306.82 1408.02 Q1306.82 1423.29 1301.43 1431.35 Q1296.08 1439.37 1285.92 1439.37 Q1275.75 1439.37 1270.36 1431.35 Q1265.01 1423.29 1265.01 1408.02 Q1265.01 1392.71 1270.36 1384.69 Q1275.75 1376.62 1285.92 1376.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1673.46 1408.75 L1725.4 1408.75 L1725.4 1415.63 L1673.46 1415.63 L1673.46 1408.75 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1760.56 1409.48 Q1754.73 1409.48 1751.36 1412.6 Q1748.04 1415.72 1748.04 1421.18 Q1748.04 1426.65 1751.36 1429.77 Q1754.73 1432.89 1760.56 1432.89 Q1766.39 1432.89 1769.75 1429.77 Q1773.12 1426.61 1773.12 1421.18 Q1773.12 1415.72 1769.75 1412.6 Q1766.43 1409.48 1760.56 1409.48 M1752.38 1405.99 Q1747.11 1404.7 1744.15 1401.09 Q1741.24 1397.49 1741.24 1392.3 Q1741.24 1385.05 1746.38 1380.84 Q1751.57 1376.62 1760.56 1376.62 Q1769.59 1376.62 1774.74 1380.84 Q1779.88 1385.05 1779.88 1392.3 Q1779.88 1397.49 1776.92 1401.09 Q1774.01 1404.7 1768.78 1405.99 Q1774.7 1407.37 1777.98 1411.38 Q1781.3 1415.39 1781.3 1421.18 Q1781.3 1429.97 1775.91 1434.67 Q1770.56 1439.37 1760.56 1439.37 Q1750.55 1439.37 1745.17 1434.67 Q1739.82 1429.97 1739.82 1421.18 Q1739.82 1415.39 1743.14 1411.38 Q1746.46 1407.37 1752.38 1405.99 M1749.38 1393.07 Q1749.38 1397.77 1752.3 1400.4 Q1755.25 1403.04 1760.56 1403.04 Q1765.83 1403.04 1768.78 1400.4 Q1771.78 1397.77 1771.78 1393.07 Q1771.78 1388.37 1768.78 1385.74 Q1765.83 1383.11 1760.56 1383.11 Q1755.25 1383.11 1752.3 1385.74 Q1749.38 1388.37 1749.38 1393.07 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1813.34 1383.11 Q1807.02 1383.11 1803.82 1389.34 Q1800.66 1395.54 1800.66 1408.02 Q1800.66 1420.45 1803.82 1426.69 Q1807.02 1432.89 1813.34 1432.89 Q1819.7 1432.89 1822.86 1426.69 Q1826.06 1420.45 1826.06 1408.02 Q1826.06 1395.54 1822.86 1389.34 Q1819.7 1383.11 1813.34 1383.11 M1813.34 1376.62 Q1823.51 1376.62 1828.86 1384.69 Q1834.25 1392.71 1834.25 1408.02 Q1834.25 1423.29 1828.86 1431.35 Q1823.51 1439.37 1813.34 1439.37 Q1803.17 1439.37 1797.79 1431.35 Q1792.44 1423.29 1792.44 1408.02 Q1792.44 1392.71 1797.79 1384.69 Q1803.17 1376.62 1813.34 1376.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M2227.28 1408.75 L2279.22 1408.75 L2279.22 1415.63 L2227.28 1415.63 L2227.28 1408.75 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M2315.39 1404.7 Q2309.88 1404.7 2306.64 1408.46 Q2303.44 1412.23 2303.44 1418.79 Q2303.44 1425.32 2306.64 1429.12 Q2309.88 1432.89 2315.39 1432.89 Q2320.9 1432.89 2324.1 1429.12 Q2327.34 1425.32 2327.34 1418.79 Q2327.34 1412.23 2324.1 1408.46 Q2320.9 1404.7 2315.39 1404.7 M2331.63 1379.05 L2331.63 1386.51 Q2328.56 1385.05 2325.4 1384.28 Q2322.28 1383.51 2319.2 1383.51 Q2311.1 1383.51 2306.8 1388.98 Q2302.55 1394.45 2301.94 1405.51 Q2304.33 1401.98 2307.94 1400.12 Q2311.54 1398.22 2315.88 1398.22 Q2324.99 1398.22 2330.26 1403.77 Q2335.56 1409.27 2335.56 1418.79 Q2335.56 1428.11 2330.05 1433.74 Q2324.55 1439.37 2315.39 1439.37 Q2304.9 1439.37 2299.35 1431.35 Q2293.8 1423.29 2293.8 1408.02 Q2293.8 1393.68 2300.6 1385.17 Q2307.41 1376.62 2318.87 1376.62 Q2321.95 1376.62 2325.07 1377.23 Q2328.23 1377.84 2331.63 1379.05 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M2367.16 1383.11 Q2360.84 1383.11 2357.64 1389.34 Q2354.48 1395.54 2354.48 1408.02 Q2354.48 1420.45 2357.64 1426.69 Q2360.84 1432.89 2367.16 1432.89 Q2373.52 1432.89 2376.68 1426.69 Q2379.88 1420.45 2379.88 1408.02 Q2379.88 1395.54 2376.68 1389.34 Q2373.52 1383.11 2367.16 1383.11 M2367.16 1376.62 Q2377.33 1376.62 2382.68 1384.69 Q2388.06 1392.71 2388.06 1408.02 Q2388.06 1423.29 2382.68 1431.35 Q2377.33 1439.37 2367.16 1439.37 Q2356.99 1439.37 2351.61 1431.35 Q2346.26 1423.29 2346.26 1408.02 Q2346.26 1392.71 2351.61 1384.69 Q2356.99 1376.62 2367.16 1376.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M997.816 1485.67 L1008.34 1485.67 L1008.34 1554.58 L1046.2 1554.58 L1046.2 1563.43 L997.816 1563.43 L997.816 1485.67 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1077.55 1511.82 Q1069.85 1511.82 1065.37 1517.86 Q1060.89 1523.85 1060.89 1534.32 Q1060.89 1544.78 1065.32 1550.83 Q1069.79 1556.82 1077.55 1556.82 Q1085.21 1556.82 1089.69 1550.77 Q1094.17 1544.73 1094.17 1534.32 Q1094.17 1523.95 1089.69 1517.91 Q1085.21 1511.82 1077.55 1511.82 M1077.55 1503.69 Q1090.05 1503.69 1097.19 1511.82 Q1104.33 1519.94 1104.33 1534.32 Q1104.33 1548.64 1097.19 1556.82 Q1090.05 1564.94 1077.55 1564.94 Q1065 1564.94 1057.87 1556.82 Q1050.78 1548.64 1050.78 1534.32 Q1050.78 1519.94 1057.87 1511.82 Q1065 1503.69 1077.55 1503.69 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1168.7 1528.22 L1168.7 1563.43 L1159.12 1563.43 L1159.12 1528.53 Q1159.12 1520.25 1155.89 1516.14 Q1152.66 1512.02 1146.2 1512.02 Q1138.44 1512.02 1133.96 1516.97 Q1129.48 1521.92 1129.48 1530.46 L1129.48 1563.43 L1119.85 1563.43 L1119.85 1505.1 L1129.48 1505.1 L1129.48 1514.16 Q1132.92 1508.9 1137.55 1506.29 Q1142.24 1503.69 1148.34 1503.69 Q1158.39 1503.69 1163.54 1509.94 Q1168.7 1516.14 1168.7 1528.22 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1226.2 1533.59 Q1226.2 1523.17 1221.88 1517.44 Q1217.61 1511.71 1209.85 1511.71 Q1202.14 1511.71 1197.81 1517.44 Q1193.54 1523.17 1193.54 1533.59 Q1193.54 1543.95 1197.81 1549.68 Q1202.14 1555.41 1209.85 1555.41 Q1217.61 1555.41 1221.88 1549.68 Q1226.2 1543.95 1226.2 1533.59 M1235.78 1556.19 Q1235.78 1571.09 1229.17 1578.33 Q1222.55 1585.62 1208.91 1585.62 Q1203.86 1585.62 1199.38 1584.84 Q1194.9 1584.11 1190.68 1582.54 L1190.68 1573.22 Q1194.9 1575.51 1199.01 1576.61 Q1203.13 1577.7 1207.4 1577.7 Q1216.83 1577.7 1221.51 1572.75 Q1226.2 1567.86 1226.2 1557.91 L1226.2 1553.17 Q1223.23 1558.33 1218.6 1560.88 Q1213.96 1563.43 1207.5 1563.43 Q1196.77 1563.43 1190.21 1555.25 Q1183.65 1547.08 1183.65 1533.59 Q1183.65 1520.04 1190.21 1511.87 Q1196.77 1503.69 1207.5 1503.69 Q1213.96 1503.69 1218.6 1506.24 Q1223.23 1508.79 1226.2 1513.95 L1226.2 1505.1 L1235.78 1505.1 L1235.78 1556.19 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1255.52 1505.1 L1265.11 1505.1 L1265.11 1563.43 L1255.52 1563.43 L1255.52 1505.1 M1255.52 1482.39 L1265.11 1482.39 L1265.11 1494.52 L1255.52 1494.52 L1255.52 1482.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1294.64 1488.53 L1294.64 1505.1 L1314.38 1505.1 L1314.38 1512.54 L1294.64 1512.54 L1294.64 1544.21 Q1294.64 1551.35 1296.56 1553.38 Q1298.54 1555.41 1304.53 1555.41 L1314.38 1555.41 L1314.38 1563.43 L1304.53 1563.43 Q1293.44 1563.43 1289.22 1559.32 Q1285 1555.15 1285 1544.21 L1285 1512.54 L1277.97 1512.54 L1277.97 1505.1 L1285 1505.1 L1285 1488.53 L1294.64 1488.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1325.99 1540.41 L1325.99 1505.1 L1335.57 1505.1 L1335.57 1540.04 Q1335.57 1548.33 1338.8 1552.49 Q1342.03 1556.61 1348.49 1556.61 Q1356.25 1556.61 1360.73 1551.66 Q1365.26 1546.71 1365.26 1538.17 L1365.26 1505.1 L1374.85 1505.1 L1374.85 1563.43 L1365.26 1563.43 L1365.26 1554.47 Q1361.77 1559.78 1357.14 1562.39 Q1352.55 1564.94 1346.46 1564.94 Q1336.41 1564.94 1331.2 1558.69 Q1325.99 1552.44 1325.99 1540.41 M1350.11 1503.69 L1350.11 1503.69 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1432.97 1513.95 L1432.97 1482.39 L1442.55 1482.39 L1442.55 1563.43 L1432.97 1563.43 L1432.97 1554.68 Q1429.95 1559.89 1425.31 1562.44 Q1420.73 1564.94 1414.27 1564.94 Q1403.7 1564.94 1397.03 1556.5 Q1390.42 1548.07 1390.42 1534.32 Q1390.42 1520.57 1397.03 1512.13 Q1403.7 1503.69 1414.27 1503.69 Q1420.73 1503.69 1425.31 1506.24 Q1429.95 1508.74 1432.97 1513.95 M1400.31 1534.32 Q1400.31 1544.89 1404.64 1550.93 Q1409.01 1556.92 1416.62 1556.92 Q1424.22 1556.92 1428.59 1550.93 Q1432.97 1544.89 1432.97 1534.32 Q1432.97 1523.74 1428.59 1517.75 Q1424.22 1511.71 1416.62 1511.71 Q1409.01 1511.71 1404.64 1517.75 Q1400.31 1523.74 1400.31 1534.32 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1512.19 1531.87 L1512.19 1536.55 L1468.13 1536.55 Q1468.75 1546.45 1474.06 1551.66 Q1479.43 1556.82 1488.96 1556.82 Q1494.48 1556.82 1499.64 1555.46 Q1504.84 1554.11 1509.95 1551.4 L1509.95 1560.46 Q1504.79 1562.65 1499.38 1563.79 Q1493.96 1564.94 1488.39 1564.94 Q1474.43 1564.94 1466.25 1556.82 Q1458.13 1548.69 1458.13 1534.84 Q1458.13 1520.51 1465.83 1512.13 Q1473.59 1503.69 1486.72 1503.69 Q1498.49 1503.69 1505.31 1511.29 Q1512.19 1518.85 1512.19 1531.87 M1502.6 1529.05 Q1502.5 1521.19 1498.18 1516.5 Q1493.91 1511.82 1486.82 1511.82 Q1478.8 1511.82 1473.96 1516.35 Q1469.17 1520.88 1468.44 1529.11 L1502.6 1529.05 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1584.84 1482.49 Q1577.86 1494.47 1574.48 1506.19 Q1571.09 1517.91 1571.09 1529.94 Q1571.09 1541.97 1574.48 1553.79 Q1577.92 1565.57 1584.84 1577.49 L1576.51 1577.49 Q1568.7 1565.25 1564.79 1553.43 Q1560.94 1541.61 1560.94 1529.94 Q1560.94 1518.33 1564.79 1506.56 Q1568.65 1494.78 1576.51 1482.49 L1584.84 1482.49 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1620.05 1490.98 Q1615.89 1490.98 1613.02 1493.9 Q1610.16 1496.76 1610.16 1500.93 Q1610.16 1505.04 1613.02 1507.91 Q1615.89 1510.72 1620.05 1510.72 Q1624.22 1510.72 1627.08 1507.91 Q1629.95 1505.04 1629.95 1500.93 Q1629.95 1496.82 1627.03 1493.9 Q1624.17 1490.98 1620.05 1490.98 M1620.05 1484.26 Q1623.39 1484.26 1626.46 1485.57 Q1629.53 1486.82 1631.77 1489.21 Q1634.17 1491.56 1635.36 1494.52 Q1636.56 1497.49 1636.56 1500.93 Q1636.56 1507.81 1631.72 1512.6 Q1626.93 1517.34 1619.95 1517.34 Q1612.92 1517.34 1608.23 1512.65 Q1603.54 1507.96 1603.54 1500.93 Q1603.54 1493.95 1608.33 1489.11 Q1613.13 1484.26 1620.05 1484.26 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1655.26 1482.49 L1663.59 1482.49 Q1671.41 1494.78 1675.26 1506.56 Q1679.17 1518.33 1679.17 1529.94 Q1679.17 1541.61 1675.26 1553.43 Q1671.41 1565.25 1663.59 1577.49 L1655.26 1577.49 Q1662.19 1565.57 1665.57 1553.79 Q1669.01 1541.97 1669.01 1529.94 Q1669.01 1517.91 1665.57 1506.19 Q1662.19 1494.47 1655.26 1482.49 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip932)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  347.848,1343.4 2329.13,1343.4 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  347.848,1148.33 2329.13,1148.33 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  347.848,953.27 2329.13,953.27 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  347.848,758.206 2329.13,758.206 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  347.848,563.142 2329.13,563.142 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  347.848,368.078 2329.13,368.078 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  347.848,173.014 2329.13,173.014 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  347.848,1343.4 347.848,173.014 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  347.848,1343.4 366.746,1343.4 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  347.848,1148.33 366.746,1148.33 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  347.848,953.27 366.746,953.27 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  347.848,758.206 366.746,758.206 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  347.848,563.142 366.746,563.142 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  347.848,368.078 366.746,368.078 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  347.848,173.014 366.746,173.014 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip930)\" d=\"M290.946 1318.55 Q284.626 1318.55 281.426 1324.78 Q278.266 1330.98 278.266 1343.46 Q278.266 1355.89 281.426 1362.13 Q284.626 1368.33 290.946 1368.33 Q297.306 1368.33 300.465 1362.13 Q303.665 1355.89 303.665 1343.46 Q303.665 1330.98 300.465 1324.78 Q297.306 1318.55 290.946 1318.55 M290.946 1312.06 Q301.113 1312.06 306.461 1320.13 Q311.848 1328.15 311.848 1343.46 Q311.848 1358.73 306.461 1366.79 Q301.113 1374.81 290.946 1374.81 Q280.778 1374.81 275.39 1366.79 Q270.043 1358.73 270.043 1343.46 Q270.043 1328.15 275.39 1320.13 Q280.778 1312.06 290.946 1312.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M227.711 1171.69 L256.27 1171.69 L256.27 1178.57 L217.867 1178.57 L217.867 1171.69 Q222.526 1166.87 230.547 1158.76 Q238.608 1150.62 240.674 1148.27 Q244.603 1143.86 246.143 1140.82 Q247.722 1137.74 247.722 1134.78 Q247.722 1129.96 244.32 1126.92 Q240.957 1123.89 235.529 1123.89 Q231.681 1123.89 227.387 1125.22 Q223.133 1126.56 218.272 1129.27 L218.272 1121.01 Q223.215 1119.03 227.508 1118.01 Q231.802 1117 235.367 1117 Q244.765 1117 250.356 1121.7 Q255.946 1126.4 255.946 1134.26 Q255.946 1137.98 254.528 1141.35 Q253.151 1144.67 249.464 1149.2 Q248.452 1150.38 243.023 1156.01 Q237.595 1161.6 227.711 1171.69 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M290.946 1123.48 Q284.626 1123.48 281.426 1129.72 Q278.266 1135.92 278.266 1148.39 Q278.266 1160.83 281.426 1167.07 Q284.626 1173.27 290.946 1173.27 Q297.306 1173.27 300.465 1167.07 Q303.665 1160.83 303.665 1148.39 Q303.665 1135.92 300.465 1129.72 Q297.306 1123.48 290.946 1123.48 M290.946 1117 Q301.113 1117 306.461 1125.06 Q311.848 1133.08 311.848 1148.39 Q311.848 1163.67 306.461 1171.73 Q301.113 1179.75 290.946 1179.75 Q280.778 1179.75 275.39 1171.73 Q270.043 1163.67 270.043 1148.39 Q270.043 1133.08 275.39 1125.06 Q280.778 1117 290.946 1117 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M243.145 930.16 L222.485 962.445 L243.145 962.445 L243.145 930.16 M240.998 923.03 L251.287 923.03 L251.287 962.445 L259.916 962.445 L259.916 969.251 L251.287 969.251 L251.287 983.51 L243.145 983.51 L243.145 969.251 L215.842 969.251 L215.842 961.351 L240.998 923.03 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M290.946 928.418 Q284.626 928.418 281.426 934.656 Q278.266 940.854 278.266 953.331 Q278.266 965.767 281.426 972.005 Q284.626 978.203 290.946 978.203 Q297.306 978.203 300.465 972.005 Q303.665 965.767 303.665 953.331 Q303.665 940.854 300.465 934.656 Q297.306 928.418 290.946 928.418 M290.946 921.936 Q301.113 921.936 306.461 929.997 Q311.848 938.018 311.848 953.331 Q311.848 968.603 306.461 976.664 Q301.113 984.685 290.946 984.685 Q280.778 984.685 275.39 976.664 Q270.043 968.603 270.043 953.331 Q270.043 938.018 275.39 929.997 Q280.778 921.936 290.946 921.936 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M239.175 754.945 Q233.666 754.945 230.425 758.712 Q227.225 762.48 227.225 769.042 Q227.225 775.564 230.425 779.372 Q233.666 783.139 239.175 783.139 Q244.684 783.139 247.885 779.372 Q251.125 775.564 251.125 769.042 Q251.125 762.48 247.885 758.712 Q244.684 754.945 239.175 754.945 M255.419 729.303 L255.419 736.756 Q252.341 735.298 249.181 734.528 Q246.062 733.759 242.983 733.759 Q234.881 733.759 230.587 739.227 Q226.334 744.696 225.726 755.755 Q228.116 752.231 231.721 750.367 Q235.327 748.464 239.661 748.464 Q248.776 748.464 254.042 754.013 Q259.349 759.523 259.349 769.042 Q259.349 778.359 253.839 783.99 Q248.33 789.621 239.175 789.621 Q228.683 789.621 223.133 781.6 Q217.584 773.539 217.584 758.267 Q217.584 743.927 224.389 735.42 Q231.195 726.872 242.659 726.872 Q245.738 726.872 248.857 727.48 Q252.016 728.088 255.419 729.303 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M290.946 733.354 Q284.626 733.354 281.426 739.592 Q278.266 745.79 278.266 758.267 Q278.266 770.703 281.426 776.941 Q284.626 783.139 290.946 783.139 Q297.306 783.139 300.465 776.941 Q303.665 770.703 303.665 758.267 Q303.665 745.79 300.465 739.592 Q297.306 733.354 290.946 733.354 M290.946 726.872 Q301.113 726.872 306.461 734.934 Q311.848 742.954 311.848 758.267 Q311.848 773.539 306.461 781.6 Q301.113 789.621 290.946 789.621 Q280.778 789.621 275.39 781.6 Q270.043 773.539 270.043 758.267 Q270.043 742.954 275.39 734.934 Q280.778 726.872 290.946 726.872 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M238.162 564.661 Q232.329 564.661 228.967 567.78 Q225.645 570.9 225.645 576.368 Q225.645 581.837 228.967 584.956 Q232.329 588.075 238.162 588.075 Q243.996 588.075 247.358 584.956 Q250.72 581.796 250.72 576.368 Q250.72 570.9 247.358 567.78 Q244.036 564.661 238.162 564.661 M229.98 561.177 Q224.713 559.881 221.756 556.276 Q218.84 552.67 218.84 547.485 Q218.84 540.234 223.984 536.021 Q229.169 531.808 238.162 531.808 Q247.196 531.808 252.341 536.021 Q257.485 540.234 257.485 547.485 Q257.485 552.67 254.528 556.276 Q251.611 559.881 246.386 561.177 Q252.3 562.555 255.581 566.565 Q258.903 570.575 258.903 576.368 Q258.903 585.159 253.515 589.858 Q248.168 594.557 238.162 594.557 Q228.157 594.557 222.769 589.858 Q217.422 585.159 217.422 576.368 Q217.422 570.575 220.743 566.565 Q224.065 562.555 229.98 561.177 M226.982 548.255 Q226.982 552.954 229.898 555.587 Q232.856 558.22 238.162 558.22 Q243.429 558.22 246.386 555.587 Q249.383 552.954 249.383 548.255 Q249.383 543.556 246.386 540.923 Q243.429 538.29 238.162 538.29 Q232.856 538.29 229.898 540.923 Q226.982 543.556 226.982 548.255 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M290.946 538.29 Q284.626 538.29 281.426 544.528 Q278.266 550.726 278.266 563.203 Q278.266 575.639 281.426 581.877 Q284.626 588.075 290.946 588.075 Q297.306 588.075 300.465 581.877 Q303.665 575.639 303.665 563.203 Q303.665 550.726 300.465 544.528 Q297.306 538.29 290.946 538.29 M290.946 531.808 Q301.113 531.808 306.461 539.87 Q311.848 547.89 311.848 563.203 Q311.848 578.475 306.461 586.536 Q301.113 594.557 290.946 594.557 Q280.778 594.557 275.39 586.536 Q270.043 578.475 270.043 563.203 Q270.043 547.89 275.39 539.87 Q280.778 531.808 290.946 531.808 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M169.297 391.432 L182.665 391.432 L182.665 345.292 L168.122 348.208 L168.122 340.755 L182.584 337.838 L190.767 337.838 L190.767 391.432 L204.135 391.432 L204.135 398.318 L169.297 398.318 L169.297 391.432 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M238.162 343.226 Q231.843 343.226 228.643 349.464 Q225.483 355.662 225.483 368.139 Q225.483 380.575 228.643 386.814 Q231.843 393.011 238.162 393.011 Q244.522 393.011 247.682 386.814 Q250.882 380.575 250.882 368.139 Q250.882 355.662 247.682 349.464 Q244.522 343.226 238.162 343.226 M238.162 336.744 Q248.33 336.744 253.677 344.806 Q259.065 352.826 259.065 368.139 Q259.065 383.411 253.677 391.472 Q248.33 399.493 238.162 399.493 Q227.995 399.493 222.607 391.472 Q217.26 383.411 217.26 368.139 Q217.26 352.826 222.607 344.806 Q227.995 336.744 238.162 336.744 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M290.946 343.226 Q284.626 343.226 281.426 349.464 Q278.266 355.662 278.266 368.139 Q278.266 380.575 281.426 386.814 Q284.626 393.011 290.946 393.011 Q297.306 393.011 300.465 386.814 Q303.665 380.575 303.665 368.139 Q303.665 355.662 300.465 349.464 Q297.306 343.226 290.946 343.226 M290.946 336.744 Q301.113 336.744 306.461 344.806 Q311.848 352.826 311.848 368.139 Q311.848 383.411 306.461 391.472 Q301.113 399.493 290.946 399.493 Q280.778 399.493 275.39 391.472 Q270.043 383.411 270.043 368.139 Q270.043 352.826 275.39 344.806 Q280.778 336.744 290.946 336.744 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M169.297 196.368 L182.665 196.368 L182.665 150.228 L168.122 153.144 L168.122 145.691 L182.584 142.774 L190.767 142.774 L190.767 196.368 L204.135 196.368 L204.135 203.254 L169.297 203.254 L169.297 196.368 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M227.711 196.368 L256.27 196.368 L256.27 203.254 L217.867 203.254 L217.867 196.368 Q222.526 191.547 230.547 183.445 Q238.608 175.303 240.674 172.953 Q244.603 168.538 246.143 165.5 Q247.722 162.421 247.722 159.464 Q247.722 154.643 244.32 151.605 Q240.957 148.567 235.529 148.567 Q231.681 148.567 227.387 149.904 Q223.133 151.241 218.272 153.955 L218.272 145.691 Q223.215 143.706 227.508 142.693 Q231.802 141.68 235.367 141.68 Q244.765 141.68 250.356 146.379 Q255.946 151.079 255.946 158.937 Q255.946 162.664 254.528 166.026 Q253.151 169.348 249.464 173.885 Q248.452 175.06 243.023 180.691 Q237.595 186.281 227.711 196.368 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M290.946 148.162 Q284.626 148.162 281.426 154.4 Q278.266 160.598 278.266 173.075 Q278.266 185.511 281.426 191.75 Q284.626 197.947 290.946 197.947 Q297.306 197.947 300.465 191.75 Q303.665 185.511 303.665 173.075 Q303.665 160.598 300.465 154.4 Q297.306 148.162 290.946 148.162 M290.946 141.68 Q301.113 141.68 306.461 149.742 Q311.848 157.763 311.848 173.075 Q311.848 188.347 306.461 196.408 Q301.113 204.429 290.946 204.429 Q280.778 204.429 275.39 196.408 Q270.043 188.347 270.043 173.075 Q270.043 157.763 275.39 149.742 Q280.778 141.68 290.946 141.68 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M31.0342 1223.31 L31.0342 1207.63 L83.9506 1187.79 L31.0342 1167.84 L31.0342 1152.16 L108.794 1152.16 L108.794 1162.42 L40.5133 1162.42 L93.8464 1182.47 L93.8464 1193.05 L40.5133 1213.1 L108.794 1213.1 L108.794 1223.31 L31.0342 1223.31 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M50.4612 1131.69 L50.4612 1122.11 L108.794 1122.11 L108.794 1131.69 L50.4612 1131.69 M27.753 1131.69 L27.753 1122.11 L39.8883 1122.11 L39.8883 1131.69 L27.753 1131.69 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M50.4612 1053.57 L78.8464 1074.66 L108.794 1052.48 L108.794 1063.78 L85.8777 1080.76 L108.794 1097.74 L108.794 1109.04 L78.2735 1086.38 L50.4612 1107.11 L50.4612 1095.81 L71.2423 1080.34 L50.4612 1064.87 L50.4612 1053.57 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M50.4612 1038.93 L50.4612 1029.35 L108.794 1029.35 L108.794 1038.93 L50.4612 1038.93 M27.753 1038.93 L27.753 1029.35 L39.8883 1029.35 L39.8883 1038.93 L27.753 1038.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M73.5861 960.809 L108.794 960.809 L108.794 970.392 L73.8986 970.392 Q65.6174 970.392 61.5028 973.621 Q57.3882 976.851 57.3882 983.309 Q57.3882 991.069 62.3361 995.548 Q67.284 1000.03 75.8256 1000.03 L108.794 1000.03 L108.794 1009.66 L50.4612 1009.66 L50.4612 1000.03 L59.5236 1000.03 Q54.2632 996.59 51.6591 991.955 Q49.0549 987.267 49.0549 981.174 Q49.0549 971.122 55.3049 965.965 Q61.5028 960.809 73.5861 960.809 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M78.9506 903.309 Q68.534 903.309 62.8049 907.632 Q57.0757 911.903 57.0757 919.663 Q57.0757 927.372 62.8049 931.695 Q68.534 935.965 78.9506 935.965 Q89.3151 935.965 95.0443 931.695 Q100.773 927.372 100.773 919.663 Q100.773 911.903 95.0443 907.632 Q89.3151 903.309 78.9506 903.309 M101.555 893.726 Q116.45 893.726 123.69 900.341 Q130.982 906.955 130.982 920.601 Q130.982 925.653 130.2 930.132 Q129.471 934.611 127.909 938.83 L118.586 938.83 Q120.877 934.611 121.971 930.497 Q123.065 926.382 123.065 922.111 Q123.065 912.684 118.117 907.997 Q113.221 903.309 103.273 903.309 L98.5338 903.309 Q103.69 906.278 106.242 910.913 Q108.794 915.549 108.794 922.007 Q108.794 932.736 100.617 939.299 Q92.4401 945.861 78.9506 945.861 Q65.409 945.861 57.232 939.299 Q49.0549 932.736 49.0549 922.007 Q49.0549 915.549 51.607 910.913 Q54.1591 906.278 59.3153 903.309 L50.4612 903.309 L50.4612 893.726 L101.555 893.726 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M59.4195 806.279 Q58.482 807.893 58.0653 809.82 Q57.5966 811.695 57.5966 813.987 Q57.5966 822.112 62.909 826.487 Q68.1694 830.81 78.0652 830.81 L108.794 830.81 L108.794 840.445 L50.4612 840.445 L50.4612 830.81 L59.5236 830.81 Q54.2112 827.789 51.6591 822.945 Q49.0549 818.101 49.0549 811.174 Q49.0549 810.185 49.2112 808.987 Q49.3154 807.789 49.5758 806.331 L59.4195 806.279 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M79.4714 769.716 Q79.4714 781.331 82.1277 785.81 Q84.7839 790.289 91.1901 790.289 Q96.2943 790.289 99.3151 786.956 Q102.284 783.57 102.284 777.789 Q102.284 769.821 96.6589 765.029 Q90.9818 760.185 81.6069 760.185 L79.4714 760.185 L79.4714 769.716 M75.5131 750.602 L108.794 750.602 L108.794 760.185 L99.9401 760.185 Q105.253 763.466 107.805 768.362 Q110.305 773.258 110.305 780.341 Q110.305 789.3 105.305 794.612 Q100.253 799.872 91.8151 799.872 Q81.9714 799.872 76.9715 793.31 Q71.9715 786.695 71.9715 773.623 L71.9715 760.185 L71.034 760.185 Q64.4194 760.185 60.8257 764.56 Q57.1799 768.883 57.1799 776.748 Q57.1799 781.748 58.3778 786.487 Q59.5757 791.227 61.9715 795.602 L53.1174 795.602 Q51.0862 790.341 50.0966 785.393 Q49.0549 780.445 49.0549 775.758 Q49.0549 763.102 55.6174 756.852 Q62.1799 750.602 75.5131 750.602 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M33.8988 721.383 L50.4612 721.383 L50.4612 701.644 L57.9091 701.644 L57.9091 721.383 L89.5756 721.383 Q96.7109 721.383 98.7422 719.456 Q100.773 717.477 100.773 711.487 L100.773 701.644 L108.794 701.644 L108.794 711.487 Q108.794 722.581 104.68 726.8 Q100.513 731.019 89.5756 731.019 L57.9091 731.019 L57.9091 738.05 L50.4612 738.05 L50.4612 731.019 L33.8988 731.019 L33.8988 721.383 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M50.4612 689.04 L50.4612 679.456 L108.794 679.456 L108.794 689.04 L50.4612 689.04 M27.753 689.04 L27.753 679.456 L39.8883 679.456 L39.8883 689.04 L27.753 689.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M57.1799 636.8 Q57.1799 644.509 63.2215 648.988 Q69.2111 653.467 79.6798 653.467 Q90.1485 653.467 96.1901 649.04 Q102.18 644.561 102.18 636.8 Q102.18 629.144 96.138 624.665 Q90.0964 620.186 79.6798 620.186 Q69.3152 620.186 63.2736 624.665 Q57.1799 629.144 57.1799 636.8 M49.0549 636.8 Q49.0549 624.3 57.1799 617.165 Q65.3049 610.03 79.6798 610.03 Q94.0026 610.03 102.18 617.165 Q110.305 624.3 110.305 636.8 Q110.305 649.352 102.18 656.488 Q94.0026 663.571 79.6798 663.571 Q65.3049 663.571 57.1799 656.488 Q49.0549 649.352 49.0549 636.8 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M27.8571 537.218 Q39.8362 544.197 51.5549 547.582 Q63.2736 550.968 75.3048 550.968 Q87.336 550.968 99.1588 547.582 Q110.93 544.145 122.857 537.218 L122.857 545.551 Q110.617 553.363 98.7943 557.27 Q86.9714 561.124 75.3048 561.124 Q63.6903 561.124 51.9195 557.27 Q40.1487 553.415 27.8571 545.551 L27.8571 537.218 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M100.044 509.353 L130.982 509.353 L130.982 518.989 L50.4612 518.989 L50.4612 509.353 L59.3153 509.353 Q54.107 506.332 51.607 501.749 Q49.0549 497.114 49.0549 490.707 Q49.0549 480.082 57.4924 473.468 Q65.9299 466.801 79.6798 466.801 Q93.4297 466.801 101.867 473.468 Q110.305 480.082 110.305 490.707 Q110.305 497.114 107.805 501.749 Q105.253 506.332 100.044 509.353 M79.6798 476.749 Q69.1069 476.749 63.1174 481.124 Q57.0757 485.447 57.0757 493.051 Q57.0757 500.655 63.1174 505.03 Q69.1069 509.353 79.6798 509.353 Q90.2526 509.353 96.2943 505.03 Q102.284 500.655 102.284 493.051 Q102.284 485.447 96.2943 481.124 Q90.2526 476.749 79.6798 476.749 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M100.044 441.645 L130.982 441.645 L130.982 451.281 L50.4612 451.281 L50.4612 441.645 L59.3153 441.645 Q54.107 438.624 51.607 434.041 Q49.0549 429.406 49.0549 422.999 Q49.0549 412.375 57.4924 405.76 Q65.9299 399.093 79.6798 399.093 Q93.4297 399.093 101.867 405.76 Q110.305 412.375 110.305 422.999 Q110.305 429.406 107.805 434.041 Q105.253 438.624 100.044 441.645 M79.6798 409.041 Q69.1069 409.041 63.1174 413.416 Q57.0757 417.739 57.0757 425.343 Q57.0757 432.947 63.1174 437.322 Q69.1069 441.645 79.6798 441.645 Q90.2526 441.645 96.2943 437.322 Q102.284 432.947 102.284 425.343 Q102.284 417.739 96.2943 413.416 Q90.2526 409.041 79.6798 409.041 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M79.6798 341.333 Q69.1069 341.333 63.1174 345.708 Q57.0757 350.031 57.0757 357.635 Q57.0757 365.239 63.1174 369.614 Q69.1069 373.937 79.6798 373.937 Q90.2526 373.937 96.2943 369.614 Q102.284 365.239 102.284 357.635 Q102.284 350.031 96.2943 345.708 Q90.2526 341.333 79.6798 341.333 M59.3153 373.937 Q54.107 370.916 51.607 366.333 Q49.0549 361.698 49.0549 355.291 Q49.0549 344.667 57.4924 338.052 Q65.9299 331.385 79.6798 331.385 Q93.4297 331.385 101.867 338.052 Q110.305 344.667 110.305 355.291 Q110.305 361.698 107.805 366.333 Q105.253 370.916 100.044 373.937 L108.794 373.937 L108.794 383.573 L27.753 383.573 L27.753 373.937 L59.3153 373.937 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M27.8571 317.01 L27.8571 308.677 Q40.1487 300.865 51.9195 297.011 Q63.6903 293.104 75.3048 293.104 Q86.9714 293.104 98.7943 297.011 Q110.617 300.865 122.857 308.677 L122.857 317.01 Q110.93 310.083 99.1588 306.698 Q87.336 303.261 75.3048 303.261 Q63.2736 303.261 51.5549 306.698 Q39.8362 310.083 27.8571 317.01 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M992.399 20.4629 L992.399 38.8655 L1014.33 38.8655 L1014.33 47.141 L992.399 47.141 L992.399 82.3259 Q992.399 90.2541 994.54 92.5111 Q996.739 94.768 1003.39 94.768 L1014.33 94.768 L1014.33 103.68 L1003.39 103.68 Q991.068 103.68 986.381 99.1083 Q981.693 94.4787 981.693 82.3259 L981.693 47.141 L973.881 47.141 L973.881 38.8655 L981.693 38.8655 L981.693 20.4629 L992.399 20.4629 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1028.34 38.8655 L1038.98 38.8655 L1038.98 103.68 L1028.34 103.68 L1028.34 38.8655 M1028.34 13.6342 L1038.98 13.6342 L1038.98 27.1179 L1028.34 27.1179 L1028.34 13.6342 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1111.73 51.3076 Q1115.72 44.1317 1121.28 40.7174 Q1126.83 37.303 1134.35 37.303 Q1144.48 37.303 1149.98 44.4211 Q1155.48 51.4812 1155.48 64.5598 L1155.48 103.68 L1144.77 103.68 L1144.77 64.9071 Q1144.77 55.59 1141.47 51.0761 Q1138.17 46.5623 1131.4 46.5623 Q1123.13 46.5623 1118.32 52.0599 Q1113.52 57.5576 1113.52 67.0483 L1113.52 103.68 L1102.82 103.68 L1102.82 64.9071 Q1102.82 55.5321 1099.52 51.0761 Q1096.22 46.5623 1089.33 46.5623 Q1081.17 46.5623 1076.37 52.1178 Q1071.57 57.6154 1071.57 67.0483 L1071.57 103.68 L1060.86 103.68 L1060.86 38.8655 L1071.57 38.8655 L1071.57 48.9349 Q1075.21 42.9743 1080.3 40.1387 Q1085.4 37.303 1092.4 37.303 Q1099.46 37.303 1104.38 40.891 Q1109.35 44.4789 1111.73 51.3076 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1232.15 68.6107 L1232.15 73.819 L1183.2 73.819 Q1183.89 84.8144 1189.79 90.6014 Q1195.75 96.3305 1206.34 96.3305 Q1212.48 96.3305 1218.21 94.8259 Q1224 93.3213 1229.67 90.312 L1229.67 100.381 Q1223.94 102.812 1217.92 104.085 Q1211.9 105.358 1205.71 105.358 Q1190.2 105.358 1181.11 96.3305 Q1172.09 87.3028 1172.09 71.9093 Q1172.09 55.9951 1180.65 46.678 Q1189.27 37.303 1203.86 37.303 Q1216.94 37.303 1224.52 45.7521 Q1232.15 54.1432 1232.15 68.6107 M1221.51 65.4858 Q1221.39 56.7474 1216.59 51.5391 Q1211.84 46.3308 1203.97 46.3308 Q1195.06 46.3308 1189.68 51.3655 Q1184.35 56.4002 1183.54 65.5436 L1221.51 65.4858 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1252.35 88.981 L1264.56 88.981 L1264.56 103.68 L1252.35 103.68 L1252.35 88.981 M1252.35 42.3956 L1264.56 42.3956 L1264.56 57.0946 L1252.35 57.0946 L1252.35 42.3956 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1353.74 24.9767 Q1344.71 24.9767 1340.14 33.8887 Q1335.63 42.7428 1335.63 60.5668 Q1335.63 78.3329 1340.14 87.2449 Q1344.71 96.099 1353.74 96.099 Q1362.83 96.099 1367.34 87.2449 Q1371.91 78.3329 1371.91 60.5668 Q1371.91 42.7428 1367.34 33.8887 Q1362.83 24.9767 1353.74 24.9767 M1353.74 15.7175 Q1368.27 15.7175 1375.9 27.2337 Q1383.6 38.6919 1383.6 60.5668 Q1383.6 82.3838 1375.9 93.9 Q1368.27 105.358 1353.74 105.358 Q1339.21 105.358 1331.52 93.9 Q1323.88 82.3838 1323.88 60.5668 Q1323.88 38.6919 1331.52 27.2337 Q1339.21 15.7175 1353.74 15.7175 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1482.96 48.7034 L1482.96 13.6342 L1493.61 13.6342 L1493.61 103.68 L1482.96 103.68 L1482.96 93.9578 Q1479.61 99.7448 1474.46 102.58 Q1469.36 105.358 1462.19 105.358 Q1450.44 105.358 1443.03 95.9833 Q1435.68 86.6083 1435.68 71.3306 Q1435.68 56.0529 1443.03 46.678 Q1450.44 37.303 1462.19 37.303 Q1469.36 37.303 1474.46 40.1387 Q1479.61 42.9164 1482.96 48.7034 M1446.68 71.3306 Q1446.68 83.0783 1451.48 89.7912 Q1456.34 96.4462 1464.79 96.4462 Q1473.24 96.4462 1478.1 89.7912 Q1482.96 83.0783 1482.96 71.3306 Q1482.96 59.583 1478.1 52.928 Q1473.24 46.215 1464.79 46.215 Q1456.34 46.215 1451.48 52.928 Q1446.68 59.583 1446.68 71.3306 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1545 71.0992 Q1532.1 71.0992 1527.12 74.0505 Q1522.14 77.0019 1522.14 84.1199 Q1522.14 89.7912 1525.85 93.1476 Q1529.61 96.4462 1536.03 96.4462 Q1544.88 96.4462 1550.21 90.1963 Q1555.59 83.8884 1555.59 73.4718 L1555.59 71.0992 L1545 71.0992 M1566.24 66.701 L1566.24 103.68 L1555.59 103.68 L1555.59 93.8421 Q1551.94 99.7448 1546.51 102.58 Q1541.07 105.358 1533.19 105.358 Q1523.24 105.358 1517.34 99.8027 Q1511.49 94.1893 1511.49 84.8144 Q1511.49 73.8769 1518.79 68.3214 Q1526.13 62.7659 1540.66 62.7659 L1555.59 62.7659 L1555.59 61.7242 Q1555.59 54.3747 1550.73 50.3817 Q1545.93 46.3308 1537.19 46.3308 Q1531.63 46.3308 1526.37 47.6618 Q1521.1 48.9928 1516.24 51.6548 L1516.24 41.8169 Q1522.08 39.56 1527.58 38.4604 Q1533.08 37.303 1538.29 37.303 Q1552.35 37.303 1559.29 44.5947 Q1566.24 51.8863 1566.24 66.701 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1615.14 109.698 Q1610.63 121.272 1606.34 124.803 Q1602.06 128.333 1594.88 128.333 L1586.38 128.333 L1586.38 119.421 L1592.63 119.421 Q1597.03 119.421 1599.46 117.337 Q1601.89 115.254 1604.84 107.499 L1606.75 102.638 L1580.53 38.8655 L1591.82 38.8655 L1612.07 89.5597 L1632.33 38.8655 L1643.61 38.8655 L1615.14 109.698 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1699.63 40.7752 L1699.63 50.8446 Q1695.12 48.5298 1690.25 47.3724 Q1685.39 46.215 1680.18 46.215 Q1672.26 46.215 1668.26 48.6456 Q1664.33 51.0761 1664.33 55.9372 Q1664.33 59.6409 1667.16 61.7821 Q1670 63.8654 1678.56 65.7751 L1682.21 66.5853 Q1693.55 69.0158 1698.3 73.4718 Q1703.1 77.87 1703.1 85.7982 Q1703.1 94.8259 1695.93 100.092 Q1688.81 105.358 1676.31 105.358 Q1671.1 105.358 1665.43 104.317 Q1659.81 103.333 1653.56 101.307 L1653.56 90.312 Q1659.47 93.3791 1665.2 94.9416 Q1670.93 96.4462 1676.54 96.4462 Q1684.06 96.4462 1688.11 93.9 Q1692.16 91.2958 1692.16 86.6083 Q1692.16 82.2681 1689.21 79.9533 Q1686.32 77.6385 1676.42 75.4973 L1672.72 74.6292 Q1662.82 72.5459 1658.43 68.2635 Q1654.03 63.9233 1654.03 56.4002 Q1654.03 47.2567 1660.51 42.2799 Q1666.99 37.303 1678.91 37.303 Q1684.81 37.303 1690.02 38.1711 Q1695.23 39.0391 1699.63 40.7752 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip932)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:16; stroke-opacity:1; fill:none\" points=\"\n",
       "  403.922,1343.4 473.15,1343.4 542.377,1343.4 611.604,1343.4 680.832,1343.4 750.059,1343.4 819.286,1343.4 888.514,1343.39 957.741,1292.66 1026.97,524.846 \n",
       "  1096.2,382.075 1165.42,388.928 1234.65,387.189 1303.88,390.67 1373.1,394.726 1442.33,376.789 1511.56,409.357 1580.79,337.03 1650.01,759.039 1719.24,1333.85 \n",
       "  1788.47,1343.4 1857.7,1343.4 1926.92,1343.4 1996.15,1343.4 2065.38,1343.4 2134.61,1343.4 2203.83,1343.4 2273.06,1343.4 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:16; stroke-opacity:1; fill:none\" points=\"\n",
       "  403.922,1343.4 473.15,1343.4 542.377,1343.4 611.604,1343.4 680.832,1343.4 750.059,1343.4 819.286,1343.4 888.514,1343.39 957.741,1292.66 1026.97,524.846 \n",
       "  1096.2,382.075 1165.42,388.928 1234.65,387.189 1303.88,390.67 1373.1,394.726 1442.33,376.789 1511.56,409.357 1580.79,337.03 1650.01,759.039 1719.24,1333.85 \n",
       "  1788.47,1343.4 1857.7,1343.4 1926.92,1343.4 1996.15,1343.4 2065.38,1343.4 2134.61,1343.4 2203.83,1343.4 2273.06,1343.4 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip930)\" d=\"\n",
       "M1659.75 484.187 L2263.09 484.187 L2263.09 212.027 L1659.75 212.027  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1659.75,484.187 2263.09,484.187 2263.09,212.027 1659.75,212.027 1659.75,484.187 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:28; stroke-opacity:1; fill:none\" points=\"\n",
       "  1681.76,302.747 1813.85,302.747 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip930)\" d=\"M1835.86 272.507 L1844.05 272.507 L1844.05 326.1 L1873.5 326.1 L1873.5 332.987 L1835.86 332.987 L1835.86 272.507 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1919.11 308.438 L1919.11 312.084 L1884.84 312.084 Q1885.33 319.781 1889.46 323.832 Q1893.63 327.842 1901.04 327.842 Q1905.34 327.842 1909.35 326.789 Q1913.4 325.736 1917.37 323.629 L1917.37 330.678 Q1913.36 332.379 1909.15 333.271 Q1904.93 334.162 1900.6 334.162 Q1889.74 334.162 1883.38 327.842 Q1877.06 321.523 1877.06 310.747 Q1877.06 299.608 1883.06 293.086 Q1889.09 286.523 1899.3 286.523 Q1908.46 286.523 1913.76 292.437 Q1919.11 298.311 1919.11 308.438 M1911.66 306.251 Q1911.58 300.134 1908.21 296.488 Q1904.89 292.842 1899.38 292.842 Q1893.14 292.842 1889.38 296.367 Q1885.65 299.891 1885.08 306.292 L1911.66 306.251 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1951.96 310.18 Q1942.93 310.18 1939.45 312.246 Q1935.96 314.312 1935.96 319.295 Q1935.96 323.265 1938.56 325.614 Q1941.19 327.923 1945.68 327.923 Q1951.88 327.923 1955.61 323.548 Q1959.38 319.133 1959.38 311.841 L1959.38 310.18 L1951.96 310.18 M1966.83 307.102 L1966.83 332.987 L1959.38 332.987 L1959.38 326.1 Q1956.82 330.232 1953.02 332.217 Q1949.21 334.162 1943.7 334.162 Q1936.73 334.162 1932.6 330.273 Q1928.51 326.343 1928.51 319.781 Q1928.51 312.125 1933.61 308.236 Q1938.76 304.347 1948.93 304.347 L1959.38 304.347 L1959.38 303.618 Q1959.38 298.473 1955.97 295.678 Q1952.61 292.842 1946.49 292.842 Q1942.61 292.842 1938.92 293.774 Q1935.23 294.706 1931.83 296.569 L1931.83 289.683 Q1935.92 288.103 1939.77 287.333 Q1943.62 286.523 1947.26 286.523 Q1957.11 286.523 1961.97 291.627 Q1966.83 296.731 1966.83 307.102 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M2008.47 294.584 Q2007.22 293.855 2005.72 293.531 Q2004.26 293.167 2002.48 293.167 Q1996.16 293.167 1992.76 297.298 Q1989.39 301.39 1989.39 309.087 L1989.39 332.987 L1981.9 332.987 L1981.9 287.617 L1989.39 287.617 L1989.39 294.665 Q1991.74 290.533 1995.51 288.549 Q1999.28 286.523 2004.67 286.523 Q2005.44 286.523 2006.37 286.645 Q2007.3 286.726 2008.43 286.928 L2008.47 294.584 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M2052.55 305.603 L2052.55 332.987 L2045.09 332.987 L2045.09 305.846 Q2045.09 299.405 2042.58 296.205 Q2040.07 293.005 2035.05 293.005 Q2029.01 293.005 2025.53 296.853 Q2022.04 300.701 2022.04 307.345 L2022.04 332.987 L2014.55 332.987 L2014.55 287.617 L2022.04 287.617 L2022.04 294.665 Q2024.72 290.574 2028.32 288.549 Q2031.97 286.523 2036.71 286.523 Q2044.53 286.523 2048.54 291.384 Q2052.55 296.205 2052.55 305.603 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M2106.22 308.438 L2106.22 312.084 L2071.95 312.084 Q2072.44 319.781 2076.57 323.832 Q2080.74 327.842 2088.16 327.842 Q2092.45 327.842 2096.46 326.789 Q2100.51 325.736 2104.48 323.629 L2104.48 330.678 Q2100.47 332.379 2096.26 333.271 Q2092.04 334.162 2087.71 334.162 Q2076.85 334.162 2070.49 327.842 Q2064.17 321.523 2064.17 310.747 Q2064.17 299.608 2070.17 293.086 Q2076.2 286.523 2086.41 286.523 Q2095.57 286.523 2100.87 292.437 Q2106.22 298.311 2106.22 308.438 M2098.77 306.251 Q2098.69 300.134 2095.33 296.488 Q2092 292.842 2086.49 292.842 Q2080.26 292.842 2076.49 296.367 Q2072.76 299.891 2072.19 306.292 L2098.77 306.251 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M2148.31 294.503 L2148.31 269.955 L2155.76 269.955 L2155.76 332.987 L2148.31 332.987 L2148.31 326.181 Q2145.96 330.232 2142.36 332.217 Q2138.79 334.162 2133.77 334.162 Q2125.54 334.162 2120.36 327.599 Q2115.22 321.037 2115.22 310.342 Q2115.22 299.648 2120.36 293.086 Q2125.54 286.523 2133.77 286.523 Q2138.79 286.523 2142.36 288.508 Q2145.96 290.452 2148.31 294.503 M2122.91 310.342 Q2122.91 318.566 2126.27 323.265 Q2129.68 327.923 2135.59 327.923 Q2141.51 327.923 2144.91 323.265 Q2148.31 318.566 2148.31 310.342 Q2148.31 302.119 2144.91 297.461 Q2141.51 292.761 2135.59 292.761 Q2129.68 292.761 2126.27 297.461 Q2122.91 302.119 2122.91 310.342 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip930)\" style=\"stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:28; stroke-opacity:1; fill:none\" points=\"\n",
       "  1681.76,393.467 1813.85,393.467 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip930)\" d=\"M1864.55 395.351 Q1867.18 396.242 1869.65 399.158 Q1872.16 402.075 1874.67 407.179 L1882.98 423.707 L1874.19 423.707 L1866.45 408.192 Q1863.45 402.116 1860.62 400.131 Q1857.82 398.146 1852.96 398.146 L1844.05 398.146 L1844.05 423.707 L1835.86 423.707 L1835.86 363.227 L1854.34 363.227 Q1864.71 363.227 1869.81 367.561 Q1874.92 371.896 1874.92 380.646 Q1874.92 386.358 1872.24 390.125 Q1869.61 393.892 1864.55 395.351 M1844.05 369.951 L1844.05 391.421 L1854.34 391.421 Q1860.25 391.421 1863.25 388.707 Q1866.29 385.953 1866.29 380.646 Q1866.29 375.339 1863.25 372.666 Q1860.25 369.951 1854.34 369.951 L1844.05 369.951 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1928.27 399.158 L1928.27 402.804 L1894 402.804 Q1894.48 410.501 1898.61 414.552 Q1902.79 418.562 1910.2 418.562 Q1914.49 418.562 1918.5 417.509 Q1922.55 416.456 1926.52 414.349 L1926.52 421.398 Q1922.51 423.099 1918.3 423.991 Q1914.09 424.882 1909.75 424.882 Q1898.9 424.882 1892.54 418.562 Q1886.22 412.243 1886.22 401.467 Q1886.22 390.328 1892.21 383.806 Q1898.25 377.243 1908.46 377.243 Q1917.61 377.243 1922.92 383.157 Q1928.27 389.031 1928.27 399.158 M1920.81 396.971 Q1920.73 390.854 1917.37 387.208 Q1914.05 383.562 1908.54 383.562 Q1902.3 383.562 1898.53 387.087 Q1894.81 390.611 1894.24 397.012 L1920.81 396.971 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M1963.47 360.675 L1963.47 366.873 L1956.34 366.873 Q1952.33 366.873 1950.75 368.493 Q1949.21 370.113 1949.21 374.326 L1949.21 378.337 L1961.48 378.337 L1961.48 384.13 L1949.21 384.13 L1949.21 423.707 L1941.71 423.707 L1941.71 384.13 L1934.59 384.13 L1934.59 378.337 L1941.71 378.337 L1941.71 375.177 Q1941.71 367.602 1945.24 364.159 Q1948.76 360.675 1956.42 360.675 L1963.47 360.675 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M2008.51 399.158 L2008.51 402.804 L1974.24 402.804 Q1974.73 410.501 1978.86 414.552 Q1983.03 418.562 1990.45 418.562 Q1994.74 418.562 1998.75 417.509 Q2002.8 416.456 2006.77 414.349 L2006.77 421.398 Q2002.76 423.099 1998.55 423.991 Q1994.34 424.882 1990 424.882 Q1979.15 424.882 1972.79 418.562 Q1966.47 412.243 1966.47 401.467 Q1966.47 390.328 1972.46 383.806 Q1978.5 377.243 1988.71 377.243 Q1997.86 377.243 2003.17 383.157 Q2008.51 389.031 2008.51 399.158 M2001.06 396.971 Q2000.98 390.854 1997.62 387.208 Q1994.3 383.562 1988.79 383.562 Q1982.55 383.562 1978.78 387.087 Q1975.05 390.611 1974.49 397.012 L2001.06 396.971 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M2047.04 385.304 Q2045.78 384.575 2044.28 384.251 Q2042.83 383.887 2041.04 383.887 Q2034.72 383.887 2031.32 388.018 Q2027.96 392.11 2027.96 399.807 L2027.96 423.707 L2020.46 423.707 L2020.46 378.337 L2027.96 378.337 L2027.96 385.385 Q2030.31 381.253 2034.08 379.269 Q2037.84 377.243 2043.23 377.243 Q2044 377.243 2044.93 377.365 Q2045.86 377.446 2047 377.648 L2047.04 385.304 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M2091.84 399.158 L2091.84 402.804 L2057.57 402.804 Q2058.06 410.501 2062.19 414.552 Q2066.36 418.562 2073.77 418.562 Q2078.07 418.562 2082.08 417.509 Q2086.13 416.456 2090.1 414.349 L2090.1 421.398 Q2086.09 423.099 2081.88 423.991 Q2077.66 424.882 2073.33 424.882 Q2062.47 424.882 2056.11 418.562 Q2049.79 412.243 2049.79 401.467 Q2049.79 390.328 2055.79 383.806 Q2061.82 377.243 2072.03 377.243 Q2081.19 377.243 2086.49 383.157 Q2091.84 389.031 2091.84 399.158 M2084.39 396.971 Q2084.31 390.854 2080.94 387.208 Q2077.62 383.562 2072.11 383.562 Q2065.88 383.562 2062.11 387.087 Q2058.38 390.611 2057.81 397.012 L2084.39 396.971 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M2141.79 396.323 L2141.79 423.707 L2134.34 423.707 L2134.34 396.566 Q2134.34 390.125 2131.82 386.925 Q2129.31 383.725 2124.29 383.725 Q2118.25 383.725 2114.77 387.573 Q2111.29 391.421 2111.29 398.065 L2111.29 423.707 L2103.79 423.707 L2103.79 378.337 L2111.29 378.337 L2111.29 385.385 Q2113.96 381.294 2117.56 379.269 Q2121.21 377.243 2125.95 377.243 Q2133.77 377.243 2137.78 382.104 Q2141.79 386.925 2141.79 396.323 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M2189.31 380.079 L2189.31 387.046 Q2186.15 385.304 2182.95 384.454 Q2179.79 383.562 2176.55 383.562 Q2169.29 383.562 2165.28 388.181 Q2161.27 392.758 2161.27 401.062 Q2161.27 409.367 2165.28 413.985 Q2169.29 418.562 2176.55 418.562 Q2179.79 418.562 2182.95 417.712 Q2186.15 416.82 2189.31 415.079 L2189.31 421.965 Q2186.19 423.423 2182.82 424.153 Q2179.5 424.882 2175.74 424.882 Q2165.49 424.882 2159.45 418.441 Q2153.42 412 2153.42 401.062 Q2153.42 389.963 2159.49 383.603 Q2165.61 377.243 2176.22 377.243 Q2179.67 377.243 2182.95 377.972 Q2186.23 378.661 2189.31 380.079 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M2241.08 399.158 L2241.08 402.804 L2206.81 402.804 Q2207.29 410.501 2211.42 414.552 Q2215.6 418.562 2223.01 418.562 Q2227.3 418.562 2231.31 417.509 Q2235.36 416.456 2239.33 414.349 L2239.33 421.398 Q2235.32 423.099 2231.11 423.991 Q2226.9 424.882 2222.56 424.882 Q2211.71 424.882 2205.35 418.562 Q2199.03 412.243 2199.03 401.467 Q2199.03 390.328 2205.02 383.806 Q2211.06 377.243 2221.27 377.243 Q2230.42 377.243 2235.73 383.157 Q2241.08 389.031 2241.08 399.158 M2233.62 396.971 Q2233.54 390.854 2230.18 387.208 Q2226.86 383.562 2221.35 383.562 Q2215.11 383.562 2211.34 387.087 Q2207.62 390.611 2207.05 397.012 L2233.62 396.971 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example of 1-D visualization with 8dx, 16dt case (t = 0 day). \n",
    "## You need to change the dimension if the resolution changes.\n",
    "\n",
    "begin\n",
    "    plot(-130.0+0.3125*4:0.3125*8:-60-0.3125*4, A[:,1,1]*1e9, label = \"Learned\", title = string(\"time: \", string(0), \" days\"), \n",
    "        xlabel=\"Longitude (°)\", ylabel=\"Mixing ratio (ppb)\", xlabelfontsize=18, ylabelfontsize=18, \n",
    "        xtickfontsize=14, ytickfontsize=14, titlefontsize=20, legendfontsize=14, width=4, ylims=(0.0, 1.2e2), legend = true,\n",
    "         margin=2.5Plots.mm)\n",
    "    plot!(-130.0+0.3125*4:0.3125*8:-60-0.3125*4, 1e2*input_NN_integrate[:,1,1,1], label = \"Reference\", width=4)\n",
    "end\n",
    "\n",
    "#savefig(\"8x16t_0day.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/projects/ctessum/manhop2/advect_NN_corrected/8x16t_10day.png\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example of 1-D visualization with 8dx, 16dt case (t = 10 days). \n",
    "## You need to change the dimension if the resolution changes.\n",
    "\n",
    "begin\n",
    "    plot(-130.0+0.3125*4:0.3125*8:-60-0.3125*4, 1e9*A[:,1,179], label = \"Learned\", title = string(\"time: \", string(10), \" days\"), \n",
    "        xtickfontsize=14, ytickfontsize=14, titlefontsize=20, width=4, ylims=(0.0, 1.2e2), legend = false, margin=2.5Plots.mm)\n",
    "    plot!(-130.0+0.3125*4:0.3125*8:-60-0.3125*4, 1e2*input_NN_integrate[:,1,1,179], label = \"Reference\", width=4)\n",
    "end\n",
    "\n",
    "#savefig(\"8x16t_10day.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/projects/ctessum/manhop2/advect_NN_corrected/1x1t_time_series.png\""
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example of 1-D visualization with 1dx, 1dt case (4 panels within 10 days - 0, 3.3, 6.6, and 10 days). \n",
    "## You need to change the dimension if the resolution changes.\n",
    "\n",
    "begin\n",
    "    xtf=12\n",
    "    ytf=12\n",
    "    p1=plot(-130.0+0.3125/2:0.3125*1:-60, 1e9*A[:,1,1], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=3,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.4e2), legend = false)\n",
    "    plot!(-130.0+0.3125/2:0.3125*1:-60, 1e2*input_NN_integrate[:,1,1,1], label = \"Reference\", width=4)\n",
    "    p2=plot(-130.0+0.3125/2:0.3125*1:-60, 1e9*A[:,1,960], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=3,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.4e2), legend = false)\n",
    "    plot!(-130.0+0.3125/2:0.3125*1:-60, 1e2*input_NN_integrate[:,1,1,960], label = \"Reference\", width=4)\n",
    "    p3=plot(-130.0+0.3125/2:0.3125*1:-60, 1e9*A[:,1,1920], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=3,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.4e2), legend = false)\n",
    "    plot!(-130.0+0.3125/2:0.3125*1:-60, 1e2*input_NN_integrate[:,1,1,1920], label = \"Reference\", width=4)\n",
    "    p4=plot(-130.0+0.3125/2:0.3125*1:-60, 1e9*A[:,1,2879], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=3,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.4e2), legend = false)\n",
    "    plot!(-130.0+0.3125/2:0.3125*1:-60, 1e2*input_NN_integrate[:,1,1,2879], label = \"Reference\", width=4)\n",
    "    \n",
    "    plot(p1,p2,p3,p4, layout = (1,4), size=(1200, 200), bottommargin=5Plots.mm, rightmargin=2.5Plots.mm)\n",
    "end\n",
    "\n",
    "#savefig(\"1x1t_time_series.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "BoundsError: attempt to access 14×1×1×44 Array{Float32, 4} at index [1:14, 1, 1, 60]",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 14×1×1×44 Array{Float32, 4} at index [1:14, 1, 1, 60]",
      "",
      "Stacktrace:",
      " [1] throw_boundserror(A::Array{Float32, 4}, I::Tuple{Base.Slice{Base.OneTo{Int64}}, Int64, Int64, Int64})",
      "   @ Base ./abstractarray.jl:703",
      " [2] checkbounds",
      "   @ ./abstractarray.jl:668 [inlined]",
      " [3] _getindex",
      "   @ ./multidimensional.jl:874 [inlined]",
      " [4] getindex(::Array{Float32, 4}, ::Function, ::Int64, ::Int64, ::Int64)",
      "   @ Base ./abstractarray.jl:1241",
      " [5] top-level scope",
      "   @ In[16]:9",
      " [6] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [7] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "## Example of 1-D visualization with 8dx, 16dt case (4 panels within 10 days - 0, 3.3, 6.6, and 10 days). \n",
    "## You need to change the dimension if the resolution changes.\n",
    "\n",
    "begin\n",
    "    xtf=12\n",
    "    ytf=12\n",
    "    p1=plot(-130.0+0.3125*4:0.3125*8:-60, 1e9*A_8x16x[:,1,1], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=4,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.1e2), legend = false)\n",
    "    plot!(-130.0+0.3125*4:0.3125*8:-60, 1e2*input_NN_integrate[:,1,1,1], label = \"Reference\", width=4)\n",
    "    p2=plot(-130.0+0.3125*4:0.3125*8:-60, 1e9*A_8x16x[:,1,60], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=4,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.1e2), legend = false)\n",
    "    plot!(-130.0+0.3125*4:0.3125*8:-60, 1e2*input_NN_integrate[:,1,1,60], label = \"Reference\", width=4)\n",
    "    p3=plot(-130.0+0.3125*4:0.3125*8:-60, 1e9*A_8x16x[:,1,120], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=4,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.1e2), legend = false)\n",
    "    plot!(-130.0+0.3125*4:0.3125*8:-60, 1e2*input_NN_integrate[:,1,1,120], label = \"Reference\", width=4)\n",
    "    p4=plot(-130.0+0.3125*4:0.3125*8:-60, 1e9*A_8x16x[:,1,179], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=4,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.1e2), legend = false)\n",
    "    plot!(-130.0+0.3125*4:0.3125*8:-60, 1e2*input_NN_integrate[:,1,1,179], label = \"Reference\", width=4)\n",
    "    \n",
    "    plot(p1,p2,p3,p4, layout = (1,4), size=(1200, 200), bottommargin=5Plots.mm, rightmargin=2.5Plots.mm)\n",
    "end\n",
    "\n",
    "savefig(\"8x16t_time_series.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/projects/ctessum/manhop2/advect_NN_corrected/16x64t_time_series.pdf\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example of 1-D visualization with 16dx, 64dt case (4 panels within 10 days - 0, 3.3, 6.6, and 10 days). \n",
    "## You need to change the dimension if the resolution changes.\n",
    "\n",
    "begin\n",
    "    xtf=12\n",
    "    ytf=12\n",
    "    p1=plot(-130.0+0.3125*8:0.3125*16:-60, 1e9*A_16x64x[:,1,1], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=4,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.1e2), legend = false)\n",
    "    plot!(-130.0+0.3125*8:0.3125*16:-60, 1e2*input_NN_integrate[:,1,1,1], label = \"Reference\", width=4)\n",
    "    p2=plot(-130.0+0.3125*8:0.3125*16:-60, 1e9*A_16x64x[:,1,15], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=4,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.1e2), legend = false)\n",
    "    plot!(-130.0+0.3125*8:0.3125*16:-60, 1e2*input_NN_integrate[:,1,1,15], label = \"Reference\", width=4)\n",
    "    p3=plot(-130.0+0.3125*8:0.3125*16:-60, 1e9*A_16x64x[:,1,30], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=4,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.1e2), legend = false)\n",
    "    plot!(-130.0+0.3125*8:0.3125*16:-60, 1e2*input_NN_integrate[:,1,1,30], label = \"Reference\", width=4)\n",
    "    p4=plot(-130.0+0.3125*8:0.3125*16:-60, 1e9*A_16x64x[:,1,44], label = \"Learned\", xticks=[-120, -100, -80, -60], yticks=4,\n",
    "        xtickfontsize=xtf, ytickfontsize=ytf, width=4, xlims=(-130, -60), ylims=(0.0, 1.1e2), legend = false)\n",
    "    plot!(-130.0+0.3125*8:0.3125*16:-60, 1e2*input_NN_integrate[:,1,1,44], label = \"Reference\", width=4)\n",
    "    \n",
    "    plot(p1,p2,p3,p4, layout = (1,4), size=(1200, 200), bottommargin=5Plots.mm, rightmargin=2.5Plots.mm)\n",
    "end\n",
    "\n",
    "savefig(\"16x64t_time_series.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
